{"model":"gpt-4.1","question":"Scenario: As a senior software engineer, build a high-performance Python web application integrating real-time streaming, GraphQL, async ORM, JWT security, and caching using the latest third-party libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. HTTP & WebSocket server with the latest Litestar, utilizing its new streaming response API for real-time event delivery.\n2. GraphQL API with the latest Strawberry-GraphQL, defining queries, mutations, and subscription streams for live data updates.\n3. Async database models and CRUD operations using the latest Piccolo ORM\u2019s Python 3.11+ async support and built-in migration tooling.\n4. JWT authentication via the latest python-jose, supporting secure login, token rotation, and injection of user context into requests.\n5. Redis-backed caching with the latest aioredis to cache GraphQL query results and automatically invalidate on data changes.\n6. Trigger real-time client notifications over WebSockets when database events occur, leveraging Litestar\u2019s WebSocket routing.\n\nDeliverables:\n\u2022 A numbered list of the final functional requirements.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":57,"modules":["litestar","strawberry-graphql","piccolo","python-jose","aioredis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1","question":"Scenario: You are tasked with building a modular Python 3.11+ microservice that delivers real-time chat with semantic search and scheduled maintenance, leveraging only the latest libraries released within the past 12 months.\n\nFunctional Requirements:\n1. Implement HTTP REST endpoints for posting and retrieving chat messages using Litestar (latest release).  \n2. Establish a WebSocket chat channel that broadcasts new messages to all connected clients via the official Litestar WebSocket plugin (latest).  \n3. Persist each message to MongoDB with Beanie ODM (latest) and simultaneously index its vector embedding into Qdrant using qdrant-client (latest).  \n4. Expose a GraphQL query endpoint that performs semantic search over stored messages via strawberry-graphql (latest).  \n5. Schedule a daily background job to purge messages older than 30 days using Dramatiq (latest).\n\nDeliverables:\n- A requirements.txt (or pyproject.toml) listing all dependencies with exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments marking where and how each external library is used.","question_index":76,"modules":["numpy","litestar","litestar-websocket","beanie","motor","pydantic","qdrant-client","strawberry-graphql","dramatiq","uvicorn"],"modulenotfound":["litestar-websocket"],"modulenotfound_count":1,"module_count":10}
{"model":"gpt-4.1","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python web service that combines asynchronous REST & GraphQL APIs, real-time notifications, persistent storage, caching, and AI-driven text summarization.\n\nFunctional Requirements:\n1. Implement async HTTP endpoints with FastAPI (latest as of 2025-05-06) using Python 3.11 structural pattern matching and Pydantic v2 dataclass models.\n2. Provide a GraphQL endpoint powered by Strawberry GraphQL v1.0+ with schema federation support.\n3. Persist and query data in PostgreSQL via the latest Tortoise-ORM with asyncio-optimized connection pooling.\n4. Push real-time notifications over WebSockets using python-socketio latest async server mode.\n5. Summarize user-submitted text on demand using the latest llama-cpp-python library with optional GPU offload.\n6. Cache frequent database queries in Redis using aioredis latest cluster-mode support.\n\nDeliverables:\n- A requirements list specifying exact versions of all third-party libraries.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":74,"modules":["fastapi","pydantic","strawberry-graphql","tortoise-orm","socketio","aioredis","llama-cpp-python","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1","question":"Scenario: Build a real-time collaborative note-taking service that supports user auth, live updates, AI summaries, PDF export, and caching using the latest Python web stack.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST API for user signup\/login with FastAPI (latest) using OAuth2 password flow.\n2. Persist users and notes in PostgreSQL via SQLModel (latest async features).\n3. Create a WebSocket endpoint broadcasting live note edits using the newest websockets library.\n4. Add an endpoint `\/summarize` that calls an external AI summarization API via httpx\u2019s HTTP\/3 client (latest).\n5. Provide a `\/export\/{note_id}` endpoint that generates a PDF of the note using Playwright Python (latest).\n6. Cache active JWT tokens in Redis Streams using aioredis v2.x.\n7. Emit structured JSON logs via loguru (latest advanced configuration).\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- A complete, runnable Python 3.11+ codebase under 150 lines, with clear comments indicating where each external library is used.","question_index":9,"modules":["fastapi","sqlmodel","jose","passlib","loguru","aioredis","httpx","playwright"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1","question":"You are a senior software engineer tasked with building a minimal Python web service for text ingestion, semantic search, and AI-powered summarization using the latest libraries.\n\nFunctional Requirements:\n1. HTTP API: use the latest Starlite framework (as of 2025-05-06) to expose async endpoints.\n2. Text Storage: on POST \/submit, accept JSON {\u201ctext\u201d: \u2026}, store original text and metadata in Redis via the latest redis-om Python client with async HashModel.\n3. Embedding: upon submission, generate a vector embedding using the latest ChromaDB Python client\u2019s async API and upsert into its vector store.\n4. Semantic Search: implement GET \/search?query=\u2026 that computes a query embedding via ChromaDB and returns top 3 matching texts from Redis.\n5. Summarization: implement GET \/summarize\/{id} to retrieve stored text by ID and produce a concise summary using the latest llama-cpp-python streaming API.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries (as of 2025-05-06).  \n\u2022 A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments indicating where and how each external library is used.","question_index":45,"modules":["starlite","pydantic","redis_om","chromadb","llama_cpp"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":5}
{"model":"gpt-4.1","question":"Scenario: Senior software engineer tasked with developing a real-time GraphQL web service that validates incoming event data, persists it to PostgreSQL, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI web framework) as of 2025-05-06 to implement an async HTTP POST endpoint at `\/events`.\n2. Validate incoming JSON payloads with Pydantic v2 (latest) models, including at least one custom field validator.\n3. Persist validated events to PostgreSQL using the latest Prisma Python client (type-safe generated models and migrations).\n4. Expose a GraphQL API with the latest Strawberry GraphQL library, supporting queries for event history and subscriptions for real-time updates.\n5. Configure WebSocket support in Starlite to broadcast new events to all GraphQL subscription clients.\n\nDeliverables:\n- A brief recap of the functional requirements.\n- Complete, runnable Python 3.11+ code under 150 lines that uses the latest versions of each library as of 2025-05-06, with clear comments marking where each external library is imported and utilized.","question_index":22,"modules":["pydantic","prisma","starlite","strawberry"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4.1","question":"Scenario: Design a minimal real-time chat service combining HTTP endpoints, data validation, async ORM persistence, WebSocket broadcasting, and JWT security using the latest Python libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Starlite to expose POST \/register and GET \/messages endpoints, leveraging its advanced dependency injection system.\n2. Define and serialize request\/response models with the latest Pydantic v2, employing its new `model_serializer` API.\n3. Persist User and Message models to SQLite asynchronously with the latest Ormar ORM, utilizing its built-in async migration feature.\n4. Implement a `\/ws` WebSocket route using the latest websockets library for real-time message broadcasting to connected clients.\n5. Secure both HTTP routes and WebSocket connections with JWT authentication via the latest PyJWT release.\n\nDeliverables:\n- A concise mapping of each numbered requirement to the specific library and feature used.\n- Complete, runnable Python 3.11+ code under 150 lines total, with clear comments indicating where each external library is imported and applied.","question_index":21,"modules":["starlite","pydantic","ormar","databases","tortoise-orm","PyJWT","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1","question":"Scenario: You are building a Python microservice that accepts user-uploaded text files, asynchronously generates summaries with a local LLM, and streams live progress to clients via WebSocket.\n\nFunctional Requirements:\n1. Implement a FastAPI app (use the latest FastAPI as of 2025-05-06) with an `\/upload` POST endpoint that accepts plain-text file uploads.  \n2. Upon upload, enqueue a background job using arq (latest version) to process the file asynchronously.  \n3. In the arq worker, load a local LLM via llama-cpp-python (latest version) using its new GPU offloading API to generate a summary.  \n4. Configure python-socketio (latest version) as an ASGI middleware in FastAPI to broadcast progress and final summary events to connected WebSocket clients.\n\nDeliverables:\n- A bullet list of the chosen third-party libraries with their exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":46,"modules":["fastapi","arq","python-socketio","llama-cpp-python","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1","question":"Scenario: Build an end-to-end real-time customer feedback summarization service with live dashboard updates.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled JSON ingest endpoint using Litestar (latest as of 2025-05-06) with async support.\n2. Define a feedback model with Pydantic v2 and persist submissions into SQLite asynchronously via SQLModel (latest).\n3. On each new submission, call the OpenAI Python SDK (latest) asynchronously to generate a brief summary.\n4. Cache each summary in Redis via redis-om (latest) with a 1-hour TTL.\n5. Serve a live dashboard using Reflex (latest) that connects over WebSockets to display incoming summaries in real time.\n6. Use HTTPX (latest) with HTTP\/3 support for any external HTTP calls.\n\nDeliverables:\n\u2022 requirements.txt listing all dependencies pinned to the latest versions as of 2025-05-06  \n\u2022 A single Python 3.11+ script (under 150 lines) that implements the above, with clear comments indicating where and why each external library is used","question_index":61,"modules":["litestar","pydantic","sqlmodel","databases","redis-om","redis","openai","httpx","reflex"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1","question":"Scenario: Develop a Python 3.11+ web application that provides secure collaborative document management with GraphQL, real-time WebSocket updates, AI-powered summarization, and on-demand PDF export.\n\nFunctional Requirements:\n1. Implement a GraphQL API for document queries and mutations using the latest \u201cariadne\u201d library with subscription support.\n2. Enable OAuth2 login with passwordless WebAuthn using the latest \u201cauthlib\u201d and \u201cfido2\u201d libraries.\n3. Provide real-time document update notifications over WebSocket leveraging FastAPI\u2019s built-in WebSocket support.\n4. Schedule asynchronous AI summarization tasks using the latest \u201cllama-cpp-python\u201d library and \u201cAPScheduler\u201d for background job management.\n5. Generate PDF exports of documents on demand using the latest \u201cplaywright\u201d headless browser library in Python.\n6. Persist documents and user metadata in PostgreSQL via the latest \u201ctortoise-orm\u201d with native async and Pydantic v2 model support.\n\nDeliverables:\n- A brief requirements list naming each third-party library and its version.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used. Use the latest library versions available as of 2025-05-06.","question_index":88,"modules":["fastapi","ariadne","authlib","fido2","tortoise-orm","pydantic","APScheduler","llama-cpp-python","playwright"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1","question":"Scenario: You are designing a Python microservice for ingesting user text, storing embeddings, and providing real-time, rate-limited on-device summaries via WebSockets.\n\nFunctional Requirements:\n1. HTTP POST \/ingest  \n   - Validate payload (id: str, text: str) using Pydantic v2 root validators  \n   - Compute embeddings on-device with llama-cpp-python and store (id, embedding, text) in ChromaDB  \n2. HTTP GET \/query  \n   - Accept query text, validate with Pydantic v2  \n   - Compute embedding via llama-cpp-python, retrieve top-5 nearest documents from ChromaDB, return metadata  \n3. WebSocket \/summarize  \n   - Client sends document id  \n   - Fetch text from ChromaDB metadata  \n   - Stream back a summary using llama-cpp-python streaming API  \n   - Enforce token-per-second rate limiting via aiolimiter  \n4. Lifespan events  \n   - On startup, initialize ChromaDB client, Llama model, and aiolimiter  \n   - On shutdown, cleanly close any resources  \n\nDeliverables:\n- requirements.txt listing the latest available versions as of today of:\n  \u2022 starlite  \n  \u2022 pydantic v2  \n  \u2022 chromadb  \n  \u2022 llama-cpp-python  \n  \u2022 aiolimiter  \n- main.py: a single, runnable Python 3.11+ script (under 150 lines) implementing all above, with clear comments marking where each external library is used.","question_index":8,"modules":["starlite","pydantic","chromadb","llama_cpp","aiolimiter"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":5}
{"model":"gpt-4.1","question":"Scenario: Build a serverless Python application enabling real-time geolocated chat with sentiment analysis, dynamic map visualization, and PDF session reporting.\n\nFunctional Requirements:\n1. Real\u2010time WebSocket chat using the latest Litestar (released in the last 12 months) with asynchronous message persistence via a modern async ORM.\n2. Interactive geospatial mapping of incoming messages on the client side using the latest Python geospatial visualization library released in the last 12 months.\n3. On-the-fly sentiment analysis of each message using the latest Python sentiment analysis library released in the last 12 months.\n4. PDF generation of the entire chat session\u2014including message timeline, geolocation markers, and sentiment scores\u2014using the latest Python PDF library released in the last 12 months.\n5. Deployment configuration for AWS Lambda using the latest Serverless Framework plugin and AWS Lambda Powertools (all released in the last 12 months).\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":97,"modules":["litestar","sqlmodel","sentiment_analyzer_lightning","leafmap","fpdf2","aws_lambda_powertools"],"modulenotfound":["sentiment_analyzer_lightning"],"modulenotfound_count":1,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: You are a senior software engineer tasked with building a real-time event analytics dashboard that ingests events, stores them, generates AI-driven summaries, caches counts, and streams updates to authenticated clients with full observability.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to implement a POST \/events REST endpoint for event ingestion.\n2. Validate incoming JSON payloads with the latest Pydantic v2 models.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise-ORM.\n4. Cache aggregated event counts in Redis via the latest aioredis.\n5. Generate streaming summaries of event batches with the latest OpenAI Python client.\n6. Stream real-time updates to clients over WebSocket using FastAPI.\n7. Secure all HTTP and WebSocket endpoints with JWT authentication via the latest fastapi-jwt-auth.\n8. Instrument HTTP, database, cache, and AI client calls with the latest OpenTelemetry for distributed tracing.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest library names and versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines integrating all subdomains, with clear comments indicating where each external library is used.","question_index":4,"modules":["fastapi","fastapi_jwt_auth","pydantic","tortoise","aioredis","openai","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-tortoiseorm","opentelemetry-instrumentation-aioredis","opentelemetry-instrumentation-openai"],"modulenotfound":["opentelemetry-instrumentation-aioredis"],"modulenotfound_count":1,"module_count":10}
{"model":"gpt-4.1","question":"Scenario: You\u2019re building a high-performance async task management API using the latest libraries in web framework, async ORM, caching, background processing, and observability as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement CRUD REST endpoints for a Task model using the latest starlite.\n2. Persist tasks in Postgres asynchronously with the latest Tortoise-ORM.\n3. Enqueue a background job on task creation using the latest Dramatiq with Redis broker.\n4. Cache GET \/tasks\/{id} responses in Redis via the latest redis.asyncio.\n5. Instrument all HTTP and database calls with OpenTelemetry using the latest opentelemetry-instrumentation-starlite.\n\nDeliverables:\n- A requirements.txt listing each external dependency with explicit versions.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":34,"modules":["starlite","tortoise-orm","dramatiq","redis","opentelemetry-instrumentation-starlite","opentelemetry-instrumentation-tortoiseorm","python-dotenv"],"modulenotfound":["opentelemetry-instrumentation-starlite"],"modulenotfound_count":1,"module_count":7}
{"model":"gpt-4.1","question":"Scenario  \nYou are building an AI-powered document search microservice that ingests user uploads, indexes them with a GPU-accelerated vector store, caches recent queries, and exposes HTTP\/3 endpoints for real-time search.  \n\nFunctional Requirements  \n1. HTTP API (Starlite, latest)  \n   1.1 POST \/upload: receive plain-text documents, validate with Pydantic.  \n   1.2 GET \/query?q=\u2026: return top-k results for query.  \n2. Document Ingestion (llama-index, latest)  \n   2.1 Stream tokenized embeddings upon upload.  \n3. Vector Store (ChromaDB, latest)  \n   3.1 Use GPU-backed indexing for fast similarity search.  \n4. Caching Layer (redis-om, latest)  \n   4.1 Cache each query + results with TTL and Pydantic models.  \n\nDeliverables  \n\u2022 A requirements.txt listing the latest versions (as of 2025-05-06) of all third-party libraries, each released within the past 12 months.  \n\u2022 Complete, runnable Python 3.11+ code (\u2264150 lines) implementing all requirements.  \n\u2022 Clear inline comments indicating where and how each external library is used.","question_index":47,"modules":["starlite","pydantic","llama_index","chromadb","redis_om"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1","question":"Scenario: You are a senior engineer building a high-throughput job processing microservice with a modern async Python stack.  \n\nFunctional Requirements:  \n1. Expose a POST \/jobs endpoint using the latest Starlite (as of 2025-05-06) to accept JSON job submissions.  \n2. Validate incoming JSON payloads with the latest Pydantic v2 models.  \n3. Persist job records to PostgreSQL via the latest prisma-client-py async ORM.  \n4. Enqueue and process jobs in the background using the latest Taskiq queue.  \n5. Expose a GET \/jobs\/{id}\/status endpoint to retrieve current job status.  \n6. Ensure all I\/O is non-blocking and use Python 3.11+ async\/await throughout.  \n\nDeliverables:  \n\u2022 A requirements list specifying the exact latest versions of Starlite, Pydantic v2, prisma-client-py, and Taskiq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ source code under 150 lines.  \n\u2022 Clear inline comments marking where each external library is used.","question_index":99,"modules":["starlite","pydantic","prisma","taskiq"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4.1","question":"Scenario: Build a secure, real-time AI-powered chat web service.\n\nFunctional Requirements:\n1. Expose an async REST API using the latest FastAPI for user registration and login endpoints.  \n2. Implement JWT authentication with rotating refresh tokens via the latest Authlib.  \n3. Persist users and chat messages in SQLite using the latest SQLModel, leveraging its new async support.  \n4. Enable real-time chat notifications over WebSockets using the latest python-socketio and ASGI.  \n5. Integrate AI chat completions using the latest OpenAI Python client in an async background task.\n\nDeliverables:\n1. A requirements.txt listing exact, up-to-date package versions as of today\u2019s date.  \n2. A single Python 3.11+ file (<150 lines) containing complete, runnable code with clear comments marking where each external library is used.","question_index":91,"modules":["fastapi","socketio","sqlmodel","sqlalchemy","authlib","openai","aiosqlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1","question":"Scenario: You are a senior software engineer tasked with building an AI-powered collaborative document annotation API.\n\nFunctional Requirements:\n1. Secure user signup and login with JWT-based authentication using Starlite (latest) and Pydantic v2.\n2. REST endpoint to upload documents and persist metadata in Redis OM Python (latest).\n3. WebSocket route for real-time annotation broadcasting using Starlite\u2019s built-in WebSocket support.\n4. Background summarization of uploaded documents using llama-cpp-python (latest).\n\nDeliverables:\n- A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":1,"modules":["starlite","pydantic","passlib","pyjwt","redis_om","llama_cpp"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: You are a senior engineer tasked with building a secure, high-throughput microservice for customer order ingestion, persistence, background processing, and caching.\n\nFunctional Requirements:\n1. Use FastAPI (latest as of 2025-05-06) with HTTP\/3 support to implement POST \/orders accepting JSON order data.\n2. Define an asynchronous Order model with SQLModel (latest) and connect to an async database engine (SQLite or PostgreSQL).\n3. Enqueue and process new orders via Arq (latest) background worker.\n4. Implement OAuth2 with JWT issuance and protected endpoints using Authlib (latest).\n5. Cache GET \/orders\/{id} responses in Redis using aiocache (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments marking where each external library is used.","question_index":68,"modules":["fastapi","sqlmodel","sqlalchemy","arq","authlib","aiocache","aiohttp","uvicorn","hypercorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1","question":"Scenario: Build an end-to-end Python web service that accepts data submissions, runs ML inference asynchronously, and pushes results back in real time.\n\nFunctional Requirements:\n1. Implement a POST \/submit endpoint using the latest FastAPI (ASGI, HTTP\/2) to accept JSON payloads.\n2. Validate and persist submissions into PostgreSQL via the latest SQLModel with asynchronous sessions.\n3. Dispatch an asynchronous classification task via the latest Celery (Redis broker) when new data arrives.\n4. In the Celery task, perform inference using the latest Hugging Face Transformers pipeline (e.g., text-classification).\n5. Cache inference results in Redis using the latest aioredis with a 5-minute TTL.\n6. Expose a WebSocket \/ws endpoint (FastAPI) to stream cached or newly computed results to subscribed clients.\n7. Instrument all HTTP requests and Celery tasks with the latest OpenTelemetry SDK for tracing.\n\nDeliverables:\n\u2022 requirements.txt listing \u201cthe latest\u201d versions of FastAPI, SQLModel, Celery, Redis, aioredis, Transformers, and OpenTelemetry as of 2025-05-06.  \n\u2022 A single Python 3.11+ file under 150 lines of runnable code. Include clear comments where each external library is used.","question_index":59,"modules":["fastapi","sqlmodel","sqlalchemy","aioredis","celery","transformers","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-celery","opentelemetry-exporter-otlp"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1","question":"Scenario: You are tasked with building a real-time IoT analytics dashboard featuring anomaly detection, edge caching, vector search, and HTTP\/3 streaming.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of May 6, 2025) with HTTP\/3 and ASGI concurrency for API endpoints.\n2. Ingest IoT telemetry via real-time WebSocket streams using the latest websockets library.\n3. Perform transformer-based anomaly detection using the latest HuggingFace transformers and PyTorch 2.x.\n4. Store and query time-series embeddings in a vector database using the latest Pinecone Python client with HNSW indexing.\n5. Cache recent query results at the edge using the latest aioredis client with TTL invalidation.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the \u201clatest\u201d versions of all third-party libraries as of today.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":67,"modules":["fastapi","aioredis","torch","transformers","pinecone-client","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: Build a high-performance, AI-powered web dashboard for real-time analytics and user collaboration.\n\nFunctional Requirements:\n1. Web API: Use the latest FastAPI (as of 2025-05-06) with async endpoints and the new dependency-injection system.\n2. Real-Time Collaboration: Implement bidirectional WebSocket chat using python-socketio v5.x+ for live user updates.\n3. AI Integration: Leverage the latest llama-cpp-python (or equivalent) to compute embeddings on incoming messages.\n4. Database Persistence: Store users and message history asynchronously with SQLModel v0.7+ (Pydantic v2) on SQLite.\n5. Caching Layer: Cache AI embedding results in Redis via aioredis v3.x+ to minimize redundant computation.\n6. Security: Protect all endpoints with OAuth2 JWT authentication using Authlib v1.2+.\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all third-party libraries as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, with clear inline comments indicating where each external library is used.","question_index":58,"modules":["fastapi","sqlmodel","llama-cpp-python","aioredis","python-socketio","authlib"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"You are a senior software engineer tasked with building a modern image-processing web service end-to-end.\n\nFunctional Requirements:\n1. Implement a REST endpoint in Litestar for image uploads that converts incoming images to AVIF using the latest pillow-avif-plugin.\n2. Asynchronously persist image metadata (filename, upload timestamp, AVIF URL, dimensions) in MongoDB via the latest Beanie ODM.\n3. Index and cache metadata in Redis for fast lookup using the latest Redis-OM Python library.\n4. Expose a GraphQL API via the latest Litestar-GraphQL plugin to query images with filtering, pagination, and type validation.\n5. Broadcast real-time upload notifications over WebSocket connections using Litestar\u2019s built-in WebSocket support.\n\nDeliverables:\n- A concise list of the five functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines, using the latest Litestar, Litestar-GraphQL, Beanie, Redis-OM, and pillow-avif-plugin (as of May 6, 2025), with clear comments indicating where each external library is used.","question_index":89,"modules":["litestar","pydantic","litestar-graphql","strawberry-graphql","pillow-avif-plugin","beanie","motor","redis-om"],"modulenotfound":["litestar-graphql"],"modulenotfound_count":1,"module_count":8}
{"model":"gpt-4.1","question":"You are a Senior Software Engineer building a high-throughput, real-time web application that ingests user events, enriches them with AI, persists outcomes, and serves analytics via a modern dashboard.\n\nFunctional Requirements:\n1. Implement JWT-based user authentication using the latest async web framework with schema-first validation (e.g., HTTP\/3 support).\n2. Expose a WebSocket endpoint for real-time event ingestion using the latest WebSocket library with per-message compression.\n3. Perform fully async CRUD on PostgreSQL using the latest ORM with native asyncio support and advanced connection pooling.\n4. Offload heavy computations to background workers via the latest task queue library supporting asyncio and result backends.\n5. Integrate an AI text-classification pipeline using the latest AI inference library with streaming outputs.\n6. Instrument the service with distributed tracing using the latest observability library supporting OpenTelemetry protocols.\n\nDeliverables:\n\u2022 requirements.txt listing the latest available versions of all dependencies (as of May 6, 2025).  \n\u2022 A single Python 3.11+ file under 150 lines that is complete and runnable, with clear comments marking where each external library is used.","question_index":12,"modules":["fastapi","python-jose","websockets","sqlalchemy","asyncpg","dramatiq","redis","transformers","opentelemetry-instrumentation-fastapi","opentelemetry-sdk"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1","question":"Scenario: Develop a high-performance asynchronous web service for ingesting user datasets, persisting metadata, launching background processing jobs, and tracing all operations end-to-end.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/upload endpoint using Starlite (latest) with async file streaming and dependency injection.\n2. Define and migrate a PostgreSQL table via Piccolo ORM (latest), storing upload metadata and task IDs, leveraging its GraphQL auto-schema feature.\n3. Enqueue and process tasks with Asynq (latest) in Redis, using priority queues, retry policies, and task result callbacks.\n4. Instrument incoming HTTP requests and background tasks with OpenTelemetry (latest), enabling W3C trace-context propagation across components.\n5. Provide an HTTP GET \/status\/{task_id} endpoint that queries Piccolo ORM for real-time task status.\n\nDeliverables:\n\u2022 The numbered functional requirements list (as above)  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, using the latest versions of all libraries as of 2025-05-06, with clear comments indicating where each external library is used.","question_index":43,"modules":["starlite","piccolo","piccolo_api","asynq","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-starlite"],"modulenotfound":["opentelemetry-instrumentation-starlite"],"modulenotfound_count":1,"module_count":7}
{"model":"gpt-4.1","question":"As a senior software engineer, you are tasked with building a secure, real-time collaborative whiteboard microservice using cutting-edge Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3 API using the latest FastAPI (as of 2025-05-06) and uvicorn[standard] for low-latency streaming.\n2. Implement OAuth2 authorization with JWTs and refresh-token rotation using the latest Authlib.\n3. Persist whiteboard sessions and user metadata in PostgreSQL via the latest SQLModel with an async engine.\n4. Broadcast drawing events in real time over WebSockets using the latest websockets library with room-based pub\/sub.\n5. Vectorize stroke data into SVG on the fly using the latest Shapely or pyvips for advanced geometry operations.\n6. Upload periodic session snapshots to AWS S3 using the latest aioboto3 with asynchronous multipart uploads.\n\nDeliverables:\n- A requirements.txt listing the latest available versions of all external libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, fully runnable, with clear comments indicating where each third-party library is used.","question_index":25,"modules":["fastapi","sqlmodel","authlib","shapely","aioboto3","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: Build a real-time sentiment analysis dashboard that fetches trending Twitter topics, analyzes sentiment, stores results, and serves updates via WebSockets.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to create both REST and WebSocket endpoints, leveraging its built-in asyncio and WebSocket broadcast features.\n2. Use the latest HTTPX to asynchronously fetch trending topics from the Twitter API v2 over HTTP\/3.\n3. Use the latest Hugging Face transformers to perform sentiment analysis via a pretrained pipeline.\n4. Use the latest SQLModel to define async SQLite models and persist topics with sentiment scores.\n5. Use the latest Jinja2 to render a minimal HTML dashboard for initial page load before WebSocket updates.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the latest versions of FastAPI, HTTPX, transformers, SQLModel, and Jinja2 as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":36,"modules":["fastapi","sqlmodel","sqlalchemy","transformers","httpx","jinja2"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"Senior Software Engineer: You are developing an intelligent IoT monitoring dashboard that integrates real-time ingestion, streaming anomaly detection, WebSocket updates, dynamic UI rendering, and cloud infrastructure provisioning.\n\nFunctional Requirements:\n1. Build an ASGI web server using the latest ASGI framework (as of 2025-05-06) with HTTP\/2 and WebSocket endpoints for real-time data streaming.\n2. Connect asynchronously to an MQTT broker using the latest Python MQTT client library (as of 2025-05-06), subscribe to sensor topics, and relay messages to the server.\n3. Implement real-time anomaly detection on incoming sensor data using the latest Python anomaly detection library supporting incremental or deep learning.\n4. Serve a live, interactive front-end auto-generated from Python to JavaScript using the latest Python-to-JS UI framework.\n5. Provision and deploy all components via infrastructure-as-code using the latest Python IaC SDK on a major cloud provider.\n\nDeliverables:\n\u2022 Restate the numbered functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code integrating all five requirements, under 150 lines.  \n\u2022 Clear inline comments indicating where and how each external \u201clatest\u201d library (as of 2025-05-06) is imported and used.","question_index":49,"modules":["fastapi","uvicorn","gmqtt","river","nicegui","pulumi","pulumi_aws"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1","question":"Scenario: Build a real-time collaborative note-taking web application with authentication, persistence, caching, live updates, and scheduled cleanup.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (as of today) for note creation, retrieval, update, and deletion.\n2. Secure endpoints with JWT authentication using the latest PyJWT.\n3. Persist notes in a relational database via the latest SQLModel ORM, defining Note and User models.\n4. Enable real-time note synchronization between clients using WebSockets with the latest python-socketio.\n5. Cache frequently accessed note metadata in Redis using the latest redis-om to reduce DB load.\n6. Schedule automatic archiving of notes older than 30 days using the latest APScheduler.\n7. Document all endpoints with OpenAPI and include example request\/response schemas.\n\nDeliverables:\n- A numbered list of all third-party libraries (with versions) you\u2019ve chosen.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear comments indicating where each external library is used.","question_index":69,"modules":["uvicorn","fastapi","sqlmodel","pyjwt","redis-om","apscheduler","python-socketio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1","question":"Scenario: Build a high-performance asynchronous message processing service for real-time sentiment analysis, storage, notification, and search.\n\nFunctional Requirements:\n1. Implement an async HTTP API using the latest FastAPI (as of May 6, 2025) to accept user messages.\n2. Define and persist message models in PostgreSQL via the latest SQLModel ORM.\n3. Schedule and execute sentiment analysis in the background using the latest Dramatiq with RabbitMQ.\n4. After analysis, update the database and push real-time notifications to connected clients over WebSockets.\n5. Index messages and sentiment scores into Elasticsearch using the latest official Python client.\n6. Provide a search endpoint to query stored messages by keyword and sentiment.\n\nDeliverables:\n\u2022 A numbered list of the \u201clatest\u201d third-party libraries (with versions) used for each sub-domain.  \n\u2022 Complete, runnable Python 3.11+ application code under 150 lines, with clear comments indicating where each external library is used.","question_index":48,"modules":["fastapi","uvicorn","pydantic","sqlmodel","asyncpg","dramatiq","elasticsearch","textblob"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1","question":"Scenario: Build a microservice that accepts text input, asynchronously generates summaries, persists data, and provides real-time metrics.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI 3, HTTP\/3 support) to implement POST \/summaries accepting JSON { \"text\": \"\u2026\" }, enqueue a background job via Arq, and return a job ID.\n2. Persist requests and summaries in SQLite asynchronously with the latest SQLModel (Pydantic V2 model support).\n3. Offload summarization jobs to Redis using the latest Arq, invoking the latest LangChain to call OpenAI for a concise summary.\n4. Expose GET \/summaries\/{id} to retrieve stored summaries from the database.\n5. Instrument request counts and job durations with the latest prometheus_client, exposing metrics on GET \/metrics.\n\nDeliverables:\n- A bulleted list of chosen \u201clatest\u201d libraries with exact version numbers.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear inline comments indicating where each external library is used.\n- Self-contained implementation: after installing dependencies, code runs without further setup.","question_index":95,"modules":["starlite","prometheus_client","sqlmodel","arq","langchain_openai","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: Create a serverless real-time analytics API that ingests WebSocket events, performs on-the-fly NLP summarization, indexes embeddings, exposes a GraphQL endpoint, and deploys to AWS Lambda.\n\nFunctional Requirements:\n1. Use the latest Litestar (as of 2025-05-06) to build an HTTP\/2 GraphQL endpoint with schema-based typing.\n2. Implement a real-time WebSocket consumer using wsproto or litestar-websockets for event ingestion.\n3. Offload text summarization to an asynchronous background task leveraging the latest Transformers library.\n4. Generate embeddings with the newest OpenAI Embeddings API or sentence-transformers and store them in the latest ChromaDB vector store.\n5. Secure all endpoints with passwordless authentication via the latest fastapi-users or equivalent.\n6. Package and deploy the entire application to AWS Lambda using the latest Mangum adapter.\n\nDeliverables:\n- A numbered list of chosen \u201clatest\u201d library versions (as of 2025-05-06) mapping to each requirement.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":77,"modules":["litestar","fastapi-users","litestar-websockets","transformers","sentence-transformers","chromadb","strawberry","mangum"],"modulenotfound":["litestar-websockets"],"modulenotfound_count":1,"module_count":8}
{"model":"gpt-4.1","question":"Scenario: Develop a modern, high-performance AI-powered web service for real-time text processing.\n\nFunctional Requirements:\n1. Build a JSON REST API with Litestar using its latest dependency-injection and ASGI lifespan hooks.\n2. Integrate Tortoise ORM (async) to manage a PostgreSQL \u201cjobs\u201d table with migrations.\n3. Use llama-cpp-python to generate text embeddings or responses via a local LLaMA model.\n4. Expose a WebSocket endpoint with python-socketio (asyncio mode) for live client notifications.\n5. Cache recent embeddings in Redis using redis-py\u2019s asyncio client.\n6. Schedule background tasks (e.g., stale job cleanup) leveraging asyncio\u2019s latest task groups.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":60,"modules":["litestar","tortoise-orm","llama-cpp-python","python-socketio","redis","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: You are a senior software engineer building a real-time social media monitoring application that ingests tweets, analyzes sentiment, classifies images, displays live charts, and deploys as a serverless microservice using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Ingest tweets in real time from Twitter API v2 using the latest HTTPX async streaming client.\n2. Perform NLP sentiment analysis on tweet text with the latest Hugging Face Transformers pipeline.\n3. Classify embedded images using the latest Ultralytics YOLOv8 model.\n4. Render an interactive, live-updating dashboard using the latest Plotly Dash framework.\n5. Scaffold serverless deployment with the latest AWS Lambda Powertools for Python.\n\nDeliverables:\n- A requirements list of all external dependencies.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":98,"modules":["httpx","transformers","ultralytics","Pillow","dash","plotly","aws-lambda-powertools"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1","question":"Scenario: A knowledge management platform must allow users to upload PDFs, index their content, and perform real-time similarity searches via a high-performance API.\n\nFunctional Requirements:\n1. Implement a PDF upload REST endpoint using the latest unstructured-sdk to extract text.\n2. Use the latest ChromaDB Python client to generate and store embeddings for each document.\n3. Provide a search REST endpoint that queries ChromaDB for top-k similar documents given a text query.\n4. Create a real-time WebSocket over HTTP\/3 endpoint using the latest aioquic to stream incremental search results.\n5. Assemble the application as an asynchronous service using the latest Starlite web framework.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":6,"modules":["starlite","unstructured","chromadb","aioquic","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1","question":"Scenario: As a senior software engineer, build an asyncio-based Python microservice showcasing HTTP\/3 REST, GraphQL, async ORM, real-time WebSockets, and scheduled tasks using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST endpoint at `\/api\/items` using FastAPI (latest) and Hypercorn (latest) with HTTP\/3 support.\n2. Expose a GraphQL API at `\/graphql` using Strawberry (latest) with asynchronous resolvers fetching from the database.\n3. Integrate PostgreSQL via Tortoise ORM (latest), define an `Item` model, and apply programmatic migrations on startup.\n4. Provide WebSocket notifications at `\/ws\/updates` using python-socketio (latest), broadcasting item creation and deletion events.\n5. Schedule an hourly cleanup job with APScheduler (latest) in asyncio mode to remove `Item` records older than 7 days.\n\nDeliverables:\n- A `requirements.txt` listing the latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is imported and utilized.","question_index":66,"modules":["fastapi","tortoise-orm","strawberry-graphql","apscheduler","python-socketio","uvloop","hypercorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1","question":"Scenario: Develop a real-time AI-driven sentiment analysis web service that ingests text, processes it via an on-device LLM, persists results, and streams live analytics to clients.\n\nFunctional Requirements:\n1. Expose RESTful and WebSocket endpoints using the latest Litestar Python framework (released within the last 12 months).\n2. Perform local LLM inference for sentiment analysis with the latest llama-cpp-python library (released within the last 12 months).\n3. Asynchronously persist raw text and sentiment scores in MongoDB using the latest async MongoDB driver (released within the last 12 months) with code-first schema.\n4. Offload sentiment analysis to a distributed background worker using the latest Dramatiq (or equivalent) async task queue released in the last 12 months.\n5. Broadcast real-time sentiment updates to connected clients over WebSockets.\n6. Render an interactive dashboard in server-side templates with the latest Plotly Python library (released within the last 12 months).\n7. Instrument the entire application with distributed tracing via the latest OpenTelemetry Python SDK (released within the last 12 months).\n\nDeliverables:\n- A requirements list specifying each third-party library and its exact latest version as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines total.\n- Clear inline comments indicating where and how each external library is used.","question_index":5,"modules":["litestar","llama-cpp-python","motor","plotly","opentelemetry-api","dramatiq","redis","jinja2"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1","question":"You are a senior software engineer tasked with building a real-time analytics dashboard with AI-driven anomaly alerts using the latest Python libraries as of 2025-05-06.  \n\nFunctional Requirements:  \n1. Implement asynchronous REST API endpoints using the latest FastAPI.  \n2. Add WebSocket-based real-time alert streaming using the latest websockets.  \n3. Persist incoming metrics in PostgreSQL via async ORM operations using the latest SQLModel.  \n4. Cache expensive queries in Redis for high throughput using the latest aioredis.  \n5. Perform streaming anomaly detection on incoming metrics using the latest river library.  \n6. Enable file uploads of detailed reports to S3-compatible storage via the latest minio SDK.  \n\nDeliverables:  \n\u2022 A numbered list of required Python libraries (with versions as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":20,"modules":["fastapi","sqlmodel","aioredis","river","minio","websockets","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1","question":"Build a real-time analytics microservice that lets authenticated users ingest sales data, stores it in PostgreSQL, streams live updates to connected clients, and generates PDF reports with embedded charts.\n\n1. Implement OAuth2 JWT authentication using the latest fastapi-jwt-auth as of May 6, 2025.  \n2. Create async CRUD endpoints for sales records with SQLModel and asyncpg.  \n3. Broadcast new sales events over WebSocket using python-socketio (latest).  \n4. Generate interactive charts in HTML via Plotly (latest) and convert them to PDF using WeasyPrint (latest).  \n5. Expose generated PDFs via an endpoint, serving files asynchronously with aiofiles.  \n\nDeliverables:  \n- A numbered confirmation of the functional requirements above.  \n- A single Python 3.11+ file under 150 lines\u2014complete, runnable code\u2014with clear comments indicating where each external library is used. Use the latest versions of all third-party libraries as of today\u2019s date.","question_index":26,"modules":["fastapi","fastapi-jwt-auth","pydantic","sqlmodel","sqlalchemy","asyncpg","plotly","weasyprint","aiofiles","python-socketio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1","question":"Scenario: You are tasked with building a real-time collaborative task management web service for enterprise teams.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to expose HTTP endpoints and WebSocket routes, leveraging its new dependency injection and WebSocket lifetime management.\n2. Configure an async PostgreSQL database using the latest Tortoise-ORM with its auto-migration CLI and Pydantic model integration.\n3. Broadcast real-time task updates to connected clients via the latest python-socketio in async mode.\n4. Implement OAuth2 authentication with JWT access and refresh tokens using the latest Authlib, including token rotation.\n5. Cache sessions and publish update events using the latest aioredis async Redis client.\n6. Schedule and execute background jobs (e.g., daily summaries) with the latest APScheduler async support.\n7. Instrument request tracing and logging using the latest OpenTelemetry Python SDK, exporting to console.\n8. Auto-generate interactive API docs using FastAPI\u2019s built-in Swagger UI and Redoc.\n\nDeliverables:\n\u2022 A bullet list of selected libraries (with version numbers).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":15,"modules":["fastapi","pydantic","tortoise-orm","python-socketio","authlib","aioredis","apscheduler","opentelemetry-sdk"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1","question":"Scenario: You are building a high-performance Python backend that ingests user articles, generates AI-powered summaries, stores and caches results, and exposes them via REST, GraphQL, and real-time channels with background processing.\n\nFunctional Requirements:\n1. Accept article submissions over a REST API using FastAPI and validate payloads with Pydantic v2.  \n2. Offload summarization to a background worker with Dramatiq using Redis as a broker.  \n3. Summarize articles via the latest OpenAI Python SDK and cache summaries with aiocache.  \n4. Persist articles and summaries in SQLite using SQLModel\u2019s async ORM features.  \n5. Expose stored summaries through a GraphQL endpoint implemented with Strawberry GraphQL.  \n6. Broadcast new summary notifications to connected clients over WebSockets with python-socketio.  \n\nDeliverables:\n\u2022 List of the latest third-party libraries (as of today) and their install commands.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":63,"modules":["fastapi","pydantic","sqlmodel","aiosqlite","aiocache","openai","dramatiq","redis","python-socketio","strawberry-graphql","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"gpt-4.1","question":"Scenario: Develop an internal Python microservice where authenticated users can submit text to be summarized by an LLM and receive real-time notifications via WebSockets.\n\nFunctional Requirements:\n1. Implement an asynchronous REST API using the latest Starlite framework (2025-05-06) leveraging its dependency injection and lifespan event system.\n2. Integrate OAuth2 Authorization Code Flow for user authentication with Authlib\u2019s async client libraries released in the last 12 months.\n3. Persist users, submissions, and summaries in PostgreSQL using the latest Piccolo ORM with async schema migrations and connection pooling.\n4. Summarize submitted text by calling OpenAI\u2019s GPT-4 Turbo via the newest openai Python SDK and its function-calling feature.\n5. Broadcast summary completion events over WebSockets using a modern Python websockets library (with HTTP\/2 support) released in the past year.\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":82,"modules":["starlite","piccolo","authlib","openai","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1","question":"You are tasked with building a real-time analytics web dashboard that ingests streaming data, processes it, summarizes it with AI, secures user access, and visualizes results dynamically using the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Implement a REST API in Python 3.11+ with FastAPI (HTTP\/3 support via Hypercorn) to accept JSON event streams.\n2. Broadcast processed insights in real time over WebSocket using python-socketio.\n3. Buffer incoming events and generate streaming AI summaries using the latest LangChain and OpenAI Python SDK.\n4. Perform real-time aggregation and lazy analytics on events with Polars.\n5. Produce dynamic Plotly charts of key metrics and serve them as JSON for front-end rendering.\n6. Secure all endpoints and WebSocket connections with JWT-based auth via fastapi-users.\n\nDeliverables:\n\u2022 A numbered list of the above functional requirements.  \n\u2022 A single Python 3.11+ file (under 150 lines) that fulfills all requirements, complete and runnable. Include clear comments indicating where and how each external library is used.","question_index":84,"modules":["fastapi","polars","fastapi-users","sqlmodel","python-socketio","pydantic","plotly","langchain","openai"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1","question":"Scenario: You are a senior software engineer tasked with building a next-generation real-time collaborative data analytics dashboard. Use the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a FastAPI server with a WebSocket endpoint for live data updates using the latest fastapi[full].\n2. Expose a type-safe GraphQL API at `\/graphql` using Strawberry with async schema stitching.\n3. Integrate AI-driven insights by generating embeddings on incoming data with llama-cpp-python.\n4. Perform vector similarity search over embeddings via the Weaviate Python client\u2019s async API.\n5. Secure all endpoints with passwordless authentication using py_webauthn for WebAuthn flows.\n6. Containerize the application using docker-py to build images and dynamically generate a docker-compose config.\n7. Instrument the app with async Prometheus metrics using prometheus-client.\n\nDeliverables:\n- A list of required libraries (with versions) using the latest releases as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":72,"modules":["fastapi","strawberry-graphql","prometheus-client","llama-cpp-python","weaviate-client","py_webauthn","docker","uvicorn","yaml"],"modulenotfound":["yaml"],"modulenotfound_count":1,"module_count":9}
{"model":"gpt-4.1","question":"Scenario: Develop a real-time collaborative note-taking web application with AI-powered summarization.\n\nFunctional Requirements:\n1. Implement user authentication via OAuth2 password flow using the latest FastAPI features for secure login.  \n2. Expose a WebSocket endpoint for real-time CRDT-based document state synchronization using the newest FastAPI WebSocket enhancements.  \n3. Persist users and documents in PostgreSQL via the latest Piccolo ORM async API with JSONB indexing for rich querying.  \n4. Cache live presence and session metadata in Redis using aioredis with RedisJSON support for low-latency reads\/writes.  \n5. Summarize document edits on demand using the latest LangChain library and ChatOpenAI (GPT-4) integration.  \n6. Serve a minimal HTML frontend via Jinja2 templates that connects to the WebSocket and displays live updates.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of each dependency as of 2025-05-06.  \n\u2022 main.py: Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":52,"modules":["fastapi","uvicorn","piccolo","aioredis","langchain","dotenv","jinja2"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1","question":"Scenario: Senior Software Engineer, you are tasked with building a prototype real-time collaborative note-taking service using only the latest Python libraries released in the last 12 months.\n\nFunctional Requirements:\n1. Develop an asynchronous REST API for note CRUD operations using the latest Litestar framework.\n2. Persist notes in a SQL database via a newly released async ORM library.\n3. Cache frequent note queries in Redis with a recently published async Redis client.\n4. Enable real-time collaboration via WebSockets using a modern Python WebSocket library launched within the last year.\n5. Expose an AI-powered summarization endpoint using the latest OpenAI Python client.\n6. Render an interactive HTML dashboard showing active collaborators with a new Python-to-JS visualization tool.\n7. Instrument distributed tracing and structured logging using a cutting-edge observability library released in the past 12 months.\n\nDeliverables:\n- requirements.txt listing only libraries created in the last 12 months with exact versions.\n- A single Python 3.11+ file under 150 lines containing the complete, runnable application, with clear comments indicating where each external library is used.","question_index":41,"modules":["litestar","sqlmesh","redis-om","websockets3","openai","plotpy","opentelemetry-api","logfire"],"modulenotfound":["websockets3"],"modulenotfound_count":1,"module_count":8}
{"model":"gpt-4.1","question":"Scenario:\nCreate an AI-powered article summarization service that accepts user-submitted text, stores it, generates summaries asynchronously via an AI API, caches results, and streams them back to clients in real time.\n\nFunctional Requirements:\n1. Implement a REST API using FastAPI (latest) with endpoints POST \/articles to submit text and GET \/stream\/{article_id} for server-sent events.\n2. Persist submissions in a database using SQLModel (latest) with an async SQLite or PostgreSQL engine.\n3. Orchestrate a background job queue using Celery (latest) with Redis as broker to process newly submitted articles.\n4. Perform summarization using the OpenAI Python SDK (latest) and its chat completion endpoint.\n5. Cache generated summaries in Redis via aioredis (latest) with a configurable TTL.\n6. Stream summaries to connected clients in real time using server-sent events via sse-starlette (latest).\n\nDeliverables:\n\u2022 A list of all third-party libraries (with versions pinned to the latest as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":64,"modules":["fastapi","sse-starlette","sqlmodel","sqlalchemy","celery","aioredis","openai","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1","question":"Scenario: Build a lightweight feedback API that records user comments, analyzes sentiment in real time, stores results, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (as of 2025-05-06) to implement asynchronous REST endpoints for submitting feedback and querying stored analytics.\n2. Use the latest SQLModel to define async ORM models and persist incoming feedback in an SQLite database.\n3. Use the latest PyWebSocketX to broadcast each new sentiment result over an ASGI WebSocket endpoint.\n4. Use the latest VulcanMind inference library to perform on-the-fly sentiment analysis of submitted feedback.\n5. Implement structured logging and error handling throughout the request, inference, database, and WebSocket flows.\n\nDeliverables:\n- A requirements list with exact version pins for Starlite, SQLModel, PyWebSocketX, and VulcanMind (latest as of 2025-05-06).\n- Complete, runnable Python 3.11+ code (\u2264150 lines) that integrates all four libraries with clear comments marking where each is used.","question_index":83,"modules":["starlite","sqlmodel","pywebsocketx","vulcanmind"],"modulenotfound":["pywebsocketx","vulcanmind"],"modulenotfound_count":2,"module_count":4}
{"model":"gpt-4.1","question":"You are a senior software engineer building a high-performance, real-time analytics web service for IoT sensor data.  \n\nFunctional Requirements:  \n1. Expose HTTP REST ingest endpoints using the latest Nebula framework (released 2024-06) with its async dependency-injection.  \n2. Persist incoming readings to PostgreSQL via StarDB ORM (v1.0+, released 2024-08), leveraging async transactions and auto-migration.  \n3. Secure all endpoints with OAuth2 PKCE flows using AuthFusion (released 2024-12).  \n4. Broadcast processed data updates to dashboard clients via WaveSocket (released 2024-10) server-clustering.  \n5. Schedule hourly aggregation jobs using ChronoTask (released 2024-11) cron-style scheduler.  \n\nDeliverables:  \n\u2022 A requirements.txt listing the latest versions of all libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":92,"modules":["nebula-framework","stardb-orm","authfusion","wavesocket","chronotask","asyncpg"],"modulenotfound":["nebula-framework","stardb-orm","authfusion","wavesocket"],"modulenotfound_count":4,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: You are a senior software engineer building a high-throughput chat summarization microservice in Python 3.11+ using only the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Real-time WebSocket ingestion: implement a `\/ws` endpoint using fastapi-socketio (latest) with session storage in Redis via `redis.asyncio`.\n2. AI summarization: enqueue incoming messages in Arq (latest) and process them with the OpenAI Python SDK (latest) calling GPT-4 Turbo for summaries.\n3. Async data persistence: store raw messages and summaries in PostgreSQL using SQLModel + encode\/databases with the asyncpg driver.\n4. Dynamic admin dashboard: expose `\/admin` rendering Jinja2 templates enhanced with HTMX via fastapi-htmx (latest).\n5. Email alerts: send summary notifications asynchronously using FastAPI-Mail (latest).\n6. Containerization: provide a multi-stage Dockerfile based on `python:3.11-slim`.\n\nDeliverables:\n- A `requirements.txt` listing the exact versions of all third-party libraries (as of May 6, 2025).\n- A single `main.py` (under 150 lines) containing complete, runnable Python 3.11+ code with clear comments indicating where and how each external library is used.","question_index":23,"modules":["fastapi","fastapi_socketio","redis","arq","openai","sqlmodel","databases","fastapi_htmx","Jinja2","fastapi_mail"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1","question":"Scenario: Build a compact Python microservice that ingests text and images, leverages AI summarization, real-time streams results, and stores everything in a modern database.\n\nFunctional Requirements:\n1. Use the latest Litestar (async web framework released Dec 2023) to define:\n   a. POST \/articles to receive JSON {\u201ctitle\u201d:\u2026, \u201ccontent\u201d:\u2026}.\n   b. WebSocket endpoint \/ws\/summary for streaming summaries.\n2. Connect asynchronously to PostgreSQL via the latest Prisma Python client (released Jul 2023), defining models for Article and Image.\n3. Summarize incoming content using the latest llama-cpp-python (released Sep 2023), utilizing its streaming interface for token-by-token output.\n4. Implement \/upload to accept multipart\/form-data image files, generate HEIF thumbnails with the latest pillow-heif (released Oct 2023), and persist file paths.\n5. Ensure all endpoints use async\/await, proper error handling, and pydantic-based validation.\n\nDeliverables:\n\u2013 A requirements.txt listing the latest library versions as of 2025-05-06.\n\u2013 A single, runnable Python 3.11+ script under 150 lines, with clear comments indicating where each external library is used.","question_index":16,"modules":["litestar","prisma","llama-cpp-python","pillow-heif","Pillow","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"You are tasked with building a prototype web service for authenticated semantic search with real-time visual feedback.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) server with:\n   a. an OAuth2 login flow using PyOIDC (latest) against an external Identity Provider  \n   b. a POST \/query endpoint to accept natural-language queries  \n   c. a WebSocket \/ws endpoint to stream real-time results  \n2. Upon authentication, embed incoming queries using sentence-transformers (latest) and perform vector similarity search in ChromaDB (latest).  \n3. As results arrive, push them incrementally over the WebSocket to clients.  \n4. Generate a Plotly (latest) bar chart of top-k result scores and serve it as an HTML snippet.  \n5. Ensure all components run under Python 3.11+, integrate only \u201cthe latest\u201d library releases as of 2025-05-06, and keep implementation concise.  \n\nDeliverables:\n\u2022 A list of chosen libraries with exact versions (the \u201crequirements\u201d).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":42,"modules":["fastapi","sentence-transformers","chromadb","plotly","pyOIDC","starlette"],"modulenotfound":["pyOIDC"],"modulenotfound_count":1,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: You are building a high-throughput, real-time analytics web application for processing IoT sensor streams using the latest Python async frameworks and libraries.\n\nFunctional Requirements:\n1. Ingest JSON sensor events via a REST API built on FastAPI (use its latest HTTP\/3 support).\n2. Broadcast processed metrics to connected clients over WebSockets using python-socketio (asyncio).\n3. Offload CPU-intensive aggregation to background workers with Arq (latest Redis-backed queue).\n4. Persist time-series summaries in PostgreSQL via SQLModel (Pydantic v2-powered ORM).\n5. Cache hot query results in Redis using aioredis for sub-millisecond lookups.\n6. Expose tracing and Prometheus metrics using OpenTelemetry and prometheus-client.\n\nDeliverables:\n- A bullet list of the latest library dependencies as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments indicating where each external library is used.","question_index":40,"modules":["fastapi","sqlmodel","prometheus-client","python-socketio","aioredis","arq","opentelemetry-api","opentelemetry-instrumentation-fastapi"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1","question":"You are a senior software engineer tasked with building a Python web application that integrates user authentication, real-time notifications, AI-based recommendations, and payment processing.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (2025-05-06) and Python 3.11 features (e.g. match-case).\n2. Secure all endpoints with OAuth2 JWT authentication via the latest fastapi-users, including email verification flows.\n3. Provide real-time notifications over WebSockets using the latest python-socketio async server.\n4. Handle one-time charges using the latest Stripe Python SDK via the Payment Intents API.\n5. Generate AI-driven product recommendations by invoking gpt-4-turbo through the latest openai library.\n6. Emit structured, context-rich logs for each operation using the latest structlog.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06  \n\u2022 Complete, runnable Python 3.11+ code (under 150 lines) with clear comments marking where each external library is used","question_index":96,"modules":["structlog","fastapi","fastapi_users","sqlalchemy","databases","socketio","openai","stripe"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1","question":"Scenario: As a senior software engineer, design a high-performance microservice for real-time document collaboration with secure access, persistence, and cloud integration.  \n\nFunctional Requirements:  \n1. Implement an asynchronous JSON REST API using the latest Python web framework available as of 2025-05-06, supporting pagination and input validation.  \n2. Secure all endpoints with OAuth2 PKCE and JWT using the latest authentication library (released within the last 12 months).  \n3. Enable bidirectional real-time collaboration events over WebSocket using the latest WebSocket library (released within the last 12 months).  \n4. Persist document data in an async NoSQL database (e.g., MongoDB) using the newest driver supporting transactions and change streams.  \n5. Upload and serve document attachments to an S3-compatible object storage using the latest SDK with multipart upload support.  \n6. Cache frequently accessed documents in Redis with TTL and automatic invalidation using the newest Redis client library.  \n\nDeliverables:  \n- A requirements.txt listing \u201cthe latest\u201d versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":80,"modules":["fastapi","uvicorn","pydantic","authlib","motor","redis","aiobotocore","python-multipart","jwt"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1","question":"Scenario: Build a high-performance Python microservice that combines cutting-edge web, real-time, AI and observability features into a single ASGI application.\n\nFunctional Requirements:\n1. HTTP\/3 ASGI server with automatic TLS and QUIC support using the latest HTTP\/3 library released within the past 12 months.  \n2. Real-time WebSocket endpoint that streams JSON events with zero-copy backpressure control using the newest streaming library created in the last year.  \n3. Semantic search pipeline: accept text uploads, generate embeddings, and perform vector similarity queries via the latest vector-store client released within 12 months.  \n4. On-device LLM inference endpoint that consumes streamed text segments and returns summaries using the most recent quantized model library published this year.  \n5. Distributed tracing and metrics export using the newest OpenTelemetry OTLP exporter released within the past 12 months.\n\nDeliverables:\n- A requirements list (requirements.txt) specifying the exact, latest versions of each third-party library as of 2025-05-06.  \n- A single Python 3.11+ script under 150 lines that implements all features above, with clear comments marking where each external library is used.","question_index":51,"modules":["fastapi","uvicorn","fastapi-websocket-streamer","chromadb","sentence-transformers","llama-cpp-python","opentelemetry-api","opentelemetry-sdk","opentelemetry-exporter-otlp"],"modulenotfound":["fastapi-websocket-streamer"],"modulenotfound_count":1,"module_count":9}
{"model":"gpt-4.1","question":"Scenario  \nDevelop an end-to-end Python web service that provides passwordless WebAuthn authentication, stores and summarizes text inputs via OpenAI, broadcasts real-time summary events, and serves interactive charts.\n\nFunctional Requirements  \n1. Implement passwordless registration and login using the latest \u201cwebauthn\u201d library (FIDO2) with Litestar middleware.  \n2. Create REST endpoints under Litestar (latest version) for submitting text to summarize and retrieving stored summaries.  \n3. Use the latest SQLModel with an async PostgreSQL engine to persist input texts, summaries, timestamps, and user IDs.  \n4. Invoke the latest \u201copenai\u201d Python client to perform GPT-4 Turbo summarization with function-calling.  \n5. Serialize and broadcast new summaries over WebSockets using the latest \u201cmsgspec\u201d for high-performance JSON.  \n6. Serve an HTML page at \u201c\/charts\u201d that embeds an interactive Plotly chart of summary lengths over time.\n\nDeliverables  \n\u2022 A numbered list of all external libraries (with exact latest versions as of 2025-05-06) and their roles.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":13,"modules":["litestar","webauthn","sqlmodel","asyncpg","openai","msgspec","plotly"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1","question":"Scenario: As a senior software engineer, you must develop a real-time analytics dashboard backend that requires user authentication, data ingestion, live updates, background enrichment via an external API, and a web-based dashboard.\n\nFunctional Requirements:\n1. Implement user registration and login endpoints using JWT authentication with asymmetric keys via FastAPI-JWT-Auth (latest).\n2. Create a POST \/data endpoint to ingest JSON payloads and store them asynchronously in SQLite using SQLModel (latest).\n3. Broadcast each new record to connected WebSocket clients at \/ws using the latest websockets library, leveraging async broadcast.\n4. Schedule a background job every minute with arq (latest) to enrich stored records by streaming JSON from a third-party API via httpx (latest).\n5. Serve a simple HTML dashboard at GET \/dashboard that opens a WebSocket to \/ws and displays incoming records in real time.\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":75,"modules":["fastapi","fastapi_jwt_auth","sqlmodel","arq","httpx","async_broadcast"],"modulenotfound":["async_broadcast"],"modulenotfound_count":1,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: Design and implement a Python microservice that exposes HTTP endpoints, real-time WebSocket notifications, persistent storage, caching, scheduled tasks, and email alerts using the latest third-party libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use FastAPI (latest) with HTTP\/2 support to expose async REST endpoints for user registration and notification retrieval.\n2. Use SQLModel (latest) as an async ORM for PostgreSQL, defining typed models for users and notifications.\n3. Implement WebSocket broadcasting via FastAPI\u2019s WebSocket support to push new notifications to connected clients.\n4. Leverage Aioredis (latest) Redis Streams to cache session state and stream notification events.\n5. Schedule periodic database cleanup and cache eviction using APScheduler (latest) with its asyncio integration.\n6. Send email alerts for critical notifications using Aiosmtplib (latest) with STARTTLS.\n7. Perform external health-check API calls using HTTPX (latest) with async retries and timeouts.\n\nDeliverables:\n- A requirements list naming each library and its latest version as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, including clear comments that indicate where each external library is used.","question_index":44,"modules":["fastapi","sqlmodel","aioredis","apscheduler","aiosmtplib","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: You are tasked as a senior software engineer to build an end-to-end collaborative document editing service with real-time updates, async persistence, GraphQL subscriptions, and cloud file storage.\n\nFunctional Requirements:\n1. Use the latest Litestar (first released within the last 12 months) to create an HTTP\/2 web server with WebSocket endpoints for real-time edit streams.\n2. Implement a GraphQL API with Strawberry GraphQL (latest) that supports queries, mutations, and live subscriptions for document change events.\n3. Persist documents in a PostgreSQL database via a fully async ORM of your choice that was first released in the last 12 months and supports automated migrations.\n4. Generate AWS S3 presigned URLs for resumable file attachments using aiobotocore (latest) with async upload\/download.\n5. Schedule periodic cleanup of stale sessions using an async task scheduler library first released within the last 12 months.\n\nDeliverables:\n- A brief list of the chosen third-party libraries (with exact versions) that meet the \u201cfirst released within the last 12 months\u201d requirement.\n- Complete, runnable Python 3.11+ code (under 150 lines) fulfilling the above requirements, with clear comments indicating where and how each external library is used.","question_index":78,"modules":["litestar","strawberry-graphql","ormsg","aiobotocore","delayed-tasks"],"modulenotfound":["ormsg","delayed-tasks"],"modulenotfound_count":2,"module_count":5}
{"model":"gpt-4.1","question":"Scenario:\nYou are a senior software engineer tasked with building a real-time AI summarization microservice.\n\nFunctional Requirements:\n1. Implement an async RESTful POST endpoint `\/summarize` that accepts JSON `{ \"text\": \"<string>\" }` and returns a summary generated via `llama-cpp-python`.\n2. Implement a WebSocket endpoint `\/stream` that streams incremental summary tokens to the client as they are produced.\n3. Persist each original text and its summary in an async SQLite database using `SQLModel`.\n4. Cache recent summaries in Redis for 10 minutes with `aioredis` to avoid redundant computation.\n5. Expose Prometheus-compatible metrics on `\/metrics` (request counts, latency histograms, cache hit\/miss) via `prometheus-client`.\n6. Use a background task to warm up the Llama model at startup and to periodically clean up expired cache entries.\n\nDeliverables:\n- A restatement of the functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Code must use the latest versions of FastAPI, SQLModel, aioredis, llama-cpp-python, and prometheus-client as of 2025-05-06.\n- Include clear comments indicating where each external library is used.","question_index":54,"modules":["fastapi","sqlmodel","sqlalchemy","aioredis","llama-cpp-python","prometheus-client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"You are a senior software engineer tasked with building a high-throughput real-time sentiment analysis microservice.\n\nFunctional Requirements:\n1. Use FastAPI (latest) to expose a POST \/analyze endpoint that accepts JSON text payloads and stores request metadata in PostgreSQL via SQLModel (latest).\n2. Secure all endpoints with OAuth2 JWT authentication using Authlib (latest), leveraging its PKCE support.\n3. Perform real-time sentiment analysis on incoming text using the latest Hugging Face Transformers pipeline with GPU acceleration via PyTorch (latest) and cache results in Redis using aiocache (latest).\n4. Broadcast live sentiment scores to connected clients over WebSockets using python-socketio (latest) and run the server on Uvicorn with HTTP\/3 support.\n5. Asynchronously upload any user-submitted audio snippets to S3-compatible storage via aiobotocore (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all external libraries as of 2025-05-06  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":62,"modules":["torch","fastapi","authlib","sqlmodel","pydantic","socketio","aiocache","transformers","aiobotocore"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1","question":"Scenario: Build a high-performance Python web application combining HTTP\/3 REST, GraphQL, real-time WebSockets, async database operations, data analytics, AI summarization, and distributed tracing.\n\nFunctional Requirements:\n1. HTTP\/3 REST API: use FastAPI (latest as of 2025-05-06) with Uvicorn HTTP\/3 support to serve \/items endpoints.\n2. Async GraphQL API: use Strawberry (latest as of 2025-05-06) to expose a GraphQL schema with DataLoader for batch fetching.\n3. Async DB access: use SQLModel (latest as of 2025-05-06) with an async SQLAlchemy engine to perform PostgreSQL CRUD.\n4. Real-time WebSockets: use websockets library (latest as of 2025-05-06) to broadcast item updates at \/ws.\n5. Data analytics: use Polars (latest as of 2025-05-06) lazy API to compute summary statistics on items.\n6. AI summarization: use OpenAI Python SDK (latest as of 2025-05-06) to generate summaries of item descriptions.\n7. Observability: use OpenTelemetry Python SDK (latest as of 2025-05-06) to instrument all endpoints for distributed tracing.\n\nDeliverables:\n- requirements.txt listing exact latest versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments marking where each external library is used.","question_index":53,"modules":["fastapi","sqlmodel","uvicorn","strawberry-graphql","websockets","polars","openai","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-sqlalchemy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1","question":"Scenario: You are a senior software engineer building a real-time sentiment and OCR-enhanced geospatial chat dashboard for remote field teams.\n\nFunctional Requirements:\n1. Use FastAPI (latest version as of 2025-05-06) to implement an async REST API that serves a minimal HTML client.\n2. Implement bi-directional real-time chat via Socket.IO using python-socketio (latest) on the FastAPI server.\n3. Perform on-the-fly sentiment analysis of text messages using Hugging Face\u2019s transformers pipeline (latest) and broadcast sentiment scores.\n4. Accept image uploads from clients, extract embedded text with EasyOCR (latest), and inject the OCR result into the chat stream.\n5. Plot message geolocation metadata on a Folium (latest) map and serve it as an embeddable HTML snippet.\n\nDeliverables:\n- A requirements list enumerating the exact \u201clatest\u201d third-party libraries and versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":2,"modules":["fastapi","python-socketio","folium","transformers","torch","easyocr"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: Develop a Python microservice that supports AI-driven product imagery, real-time stock updates, secure user authentication, and checkout processing.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) app with JWT-based auth using Authlib (latest) and secure password hashing.  \n2. Define async ORM models for Users and Products with SQLModel (latest) on PostgreSQL, including migration setup.  \n3. On product creation, generate an AI image via Diffusers (latest) or Stability-SDK (latest) and expose it via a REST endpoint.  \n4. Provide a WebSocket endpoint for real-time inventory updates using Starlette\u2019s WebSocket support (latest).  \n5. Create a \/checkout POST endpoint integrating Stripe\u2019s Python SDK (latest) with webhook handling to confirm payments.\n\nDeliverables:\n\u2022 A bullet list of the \u201clatest\u201d libraries (with pip install specifiers as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":85,"modules":["stripe","fastapi","sqlmodel","starlette","authlib","passlib","diffusers","torch","alembic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1","question":"Scenario: You are developing a high-performance, real-time AI-powered web service for personalized content recommendations.\n\nFunctional Requirements:\n1. Implement a RESTful POST \/recommend endpoint using the latest Python web framework released within the last 12 months, accepting JSON input and returning JSON responses.\n2. Add a WebSocket \/ws endpoint for streaming live recommendation updates using the newest asyncio-based WebSocket library from the past year.\n3. Integrate semantic search over user profiles by connecting to a vector database via its newest Python client library released within the last 12 months.\n4. Schedule periodic model retraining in a background worker using the most recent Python task scheduler released in the last 12 months.\n5. Store and serve user-uploaded media files on cloud object storage using the latest cloud storage client library published within the past 12 months.\n\nDeliverables:\n\u2022 A requirements list (requirements.txt) specifying the exact library names and versions (all released within the last 12 months).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total, with clear comments marking where and why each external library is used.","question_index":71,"modules":["litestar","starlette-websockets","chromadb","apscheduler","boto3"],"modulenotfound":["starlette-websockets"],"modulenotfound_count":1,"module_count":5}
{"model":"gpt-4.1","question":"Scenario: Develop a high-performance Python microservice that ingests real-time social media posts via WebSockets, generates AI summaries, stores them in MongoDB Atlas, serves secured HTTP\/3 endpoints, and provides an interactive Streamlit dashboard for sentiment trends.\n\nFunctional Requirements:\n1. Initialize an asynchronous FastAPI app with HTTP\/3 (h2\/h3) and ASGI support.  \n2. Protect all HTTP endpoints with OAuth2 JWT using Authlib\u2019s latest OAuth library.  \n3. Consume a WebSocket stream of JSON posts from wss:\/\/stream.example.com asynchronously.  \n4. Summarize each post using OpenAI\u2019s latest Python SDK leveraging function-calling or embeddings.  \n5. Persist summaries and metadata in MongoDB Atlas via Motor (async driver).  \n6. Expose a secured REST endpoint `\/summaries` supporting pagination and filtering by sentiment.  \n7. Build a Streamlit dashboard that fetches `\/summaries` and visualizes sentiment trends over time with Plotly.\n\nDeliverables:\n- A requirements list naming the latest versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":28,"modules":["fastapi","uvicorn","hypercorn","authlib","python-jose","openai","motor","streamlit","plotly","httpx","pandas"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"gpt-4.1","question":"Scenario: Build a real-time HTTP\/3-powered AI chat service that streams user messages, generates LLM responses, persists embeddings in a vector DB, and exposes chat history via GraphQL.\n\nFunctional Requirements:\n1. HTTP\/3 streaming: use the latest aioquic (as of 2025-05-06) to handle bidirectional HTTP\/3 connections for chat.\n2. On-device LLM inference: integrate the latest llama-cpp-python for quantized model loading and response generation.\n3. Vector storage: use the latest chromadb Python client to embed each message (via llama-cpp-python\u2019s embed API) and store\/retrieve vectors.\n4. GraphQL API: implement an async GraphQL schema with the latest strawberry to query chat history from ChromaDB.\n5. Async orchestration: ensure end-to-end async I\/O under Python 3.11+, coordinating the QUIC server, LLM, and DB.\n6. Code constraints: keep the complete solution under 150 lines, include clear comments marking where each external library is used.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of aioquic, llama-cpp-python, chromadb, and strawberry (latest as of 2025-05-06)  \n\u2022 A single Python 3.11+ script (<150 lines) with runnable code and comments identifying each external library usage.","question_index":79,"modules":["aioquic","llama-cpp-python","chromadb","strawberry-graphql"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4.1","question":"Scenario: Design and implement a high-performance real-time chat backend with semantic search and background processing.\n\nFunctional Requirements:\n1. Asynchronous REST API: use FastAPI (latest as of 2025-05-06) to expose user and message endpoints.\n2. HTTP\/3 WebSockets: implement real-time chat over WebSockets with Hypercorn\u2019s HTTP\/3 support.\n3. Async ORM persistence: store users and messages in PostgreSQL via SQLModel (async).\n4. Vector semantic search: index and query messages in Qdrant using the latest qdrant-client.\n5. Background sentiment analysis: enqueue and run analysis jobs with ARQ (async Redis-backed queue).\n6. JWT authentication: secure all endpoints with python-jose for JWT issuance and validation.\n7. Redis caching: cache active WebSocket connections or session data via redis.asyncio.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06  \n\u2022 A single Python 3.11+ file under 150 lines that:\n  \u2013 Is fully runnable  \n  \u2013 Shows clear comments where each external library is used  \n  \u2013 Integrates all above requirements into a cohesive application","question_index":11,"modules":["fastapi","sqlmodel","qdrant-client","redis","arq","python-jose"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: A fintech startup requires a live transaction monitoring service that ingests, processes, stores, and notifies users of suspicious activity in real time.\n\nFunctional Requirements:\n1. Build an HTTP endpoint to receive and stream-respond to JSON transaction payloads using the latest async web framework released in the last 12 months.\n2. Validate and serialize incoming data with the latest data validation library, enforcing strict type constraints and custom validators.\n3. Persist transactions asynchronously in a relational database using the latest async ORM library compatible with Python 3.11+.\n4. Offload CPU-intensive anomaly detection to a background worker queue using the latest task orchestration library.\n5. Broadcast real-time alerts to connected clients via WebSockets using the latest real-time communication library.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of the latest libraries (as of May 6, 2025)  \n\u2022 A single Python 3.11+ file under 150 lines, complete and runnable, with clear comments indicating where each external library is used.","question_index":39,"modules":["litestar","pydantic","tortoise-orm","encode-databases","celery","socketify"],"modulenotfound":["encode-databases"],"modulenotfound_count":1,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: Build a real-time collaborative document editor service with AI-powered summarization, OAuth2 access control, WebSocket syncing, Redis caching, and NoSQL persistence.\n\nFunctional Requirements:\n1. Use the latest async Python web framework (as of 2025-05-06) to define RESTful CRUD endpoints for documents.\n2. Implement OAuth2 authorization with the latest Python OAuth library released within the last 12 months for secure user login and token issuance.\n3. Establish bi-directional real-time document synchronization over WebSockets using the newest real-time communication library (with built-in pub\/sub).\n4. Integrate the latest AI inference library published in the past year to generate live summaries of incremental document edits.\n5. Employ the latest Redis client library (released within the last year) to cache active sessions and coordinate pub\/sub channels for syncing.\n6. Persist document state and version history in a NoSQL database using the newest async database client released in the last 12 months.\n\nDeliverables:\n1. A numbered list of the chosen \u201clatest\u201d libraries (with a one-line description of their role).\n2. Complete, runnable Python 3.11+ code under 150 lines, including clear comments indicating where and why each external library is used.","question_index":86,"modules":["fastify","authlib","socketify","aredis","llama_cpp_python","motor"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: Develop a user registration microservice over HTTP\/3 that validates input, persists user data, and integrates with an external email verification API.\n\nFunctional Requirements:\n1. Expose a POST \/users endpoint using Starlite (latest) with HTTP\/3 support.  \n2. Define request and response schemas with Pydantic v2 (latest) for data validation.  \n3. Persist user records in PostgreSQL via the latest Prisma Python client, including model definitions and migrations.  \n4. Perform an asynchronous HTTP\/3 request to an external email verification service using httpx (latest), and incorporate its response.  \n5. Return a JSON payload containing the stored user data and email verification status.\n\nDeliverables:\n- A list of required third-party libraries with exact versions (as of today)  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":100,"modules":["starlite","pydantic","prisma","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4.1","question":"Scenario: As a senior software engineer, build a modular web application that authenticates users with WebAuthn passkeys, streams real-time stock data, generates AI trading signals, processes payments, and delivers interactive dashboards.\n\nFunctional Requirements:\n1. Implement user authentication via WebAuthn using the latest pywebauthn library (as of 2025-05-06) with hardware security key support.\n2. Ingest and stream real-time stock price data over WebSockets using FastAPI and the latest Starlette HTTP\/2 push features.\n3. Generate AI-driven trading signals leveraging the latest OpenAI Python SDK\u2019s new function-calling interface.\n4. Integrate secure payment processing with the latest Stripe Python SDK supporting Payment Element and webhook handling.\n5. Serve interactive dashboards using the latest Plotly Dash or Reflex library for live data visualization and client-side callbacks.\n6. Add structured logging and Prometheus metrics using structlog and prometheus-client.\n\nDeliverables:\n- A concise mapping of each functional requirement to its implementation approach.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used and using the latest libraries available as of 2025-05-06.","question_index":56,"modules":["fastapi","starlette","pywebauthn","openai","dash","plotly","stripe","structlog","prometheus-client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1","question":"You are a senior software engineer tasked with architecting a small, high-performance async web application using only the latest Python libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Litestar framework (with native HTTP\/3 support) to define async REST endpoints for user signup and profile retrieval.  \n2. Integrate the latest Piccolo-ORM to define a PostgreSQL \u201cusers\u201d table and perform async CRUD operations.  \n3. Implement real-time notifications via WebSocket using the latest python-socketio in asyncio mode.  \n4. Schedule a daily summary email job with APScheduler\u2019s new AsyncIOScheduler and send mail via an async SMTP library.  \n5. Fetch external data during signup from a third-party API using HTTPX\u2019s HTTP\/2 client.\n\nDeliverables:\n\u2022 A requirements.txt listing exact package names and latest versions.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":29,"modules":["litestar","piccolo","python-socketio","httpx","apscheduler","aiosmtplib","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1","question":"Scenario: You are a senior software engineer building a high-performance real-time IoT analytics backend.\n\nFunctional Requirements:\n1. Expose an asynchronous HTTP API using FastAPI with WebSocket support to ingest and broadcast live sensor readings.\n2. Persist incoming data in PostgreSQL via SQLModel\u2019s async ORM and validate payloads with Pydantic v2 models.\n3. Enforce per-device rate limiting using Redis Streams and aioredis, leveraging RedisJSON for overflow handling.\n4. Secure all endpoints with OAuth2 passwordless login via FastAPI-Users, sending one-time codes.\n5. Schedule background aggregation jobs with Celery (beat scheduler) to compute rolling metrics and push updates over WebSockets.\n6. Provide a paginated historical metrics REST endpoint, caching hot queries in Redis for low-latency responses.\n\nDeliverables:\n- A list of required external libraries pinned to the latest versions available as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is utilized.","question_index":27,"modules":["fastapi","sqlmodel","pydantic","fastapi-users","celery","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: Build a real-time collaborative document review service with semantic search and AI summarization.\n\nFunctional Requirements:\n1. HTTP API (HTTP\/3) using the latest starlite (v1.x) to manage documents and user sessions.  \n2. Real-time updates over WebSockets for live editing and presence using starlite\u2019s WebSocket support.  \n3. Semantic indexing and search: on each document save, compute embeddings and store\/retrieve via the latest qdrant-client.  \n4. AI summarization endpoint: call an LLM using the latest llm-tools library to generate concise summaries on demand.  \n5. Background task scheduling with dramatiq (latest) to clean up stale sessions and regenerate indexes.\n\nDeliverables:\n\u2022 Reiterate the numbered requirements.  \n\u2022 A complete, runnable Python 3.11+ code file under 150 lines.  \n\u2022 Use the latest starlite, qdrant-client, llm-tools, and dramatiq libraries available as of 2025-05-06.  \n\u2022 Include clear comments indicating where and why each external library is used.","question_index":55,"modules":["starlite","qdrant-client","llm-tools","dramatiq"],"modulenotfound":["llm-tools"],"modulenotfound_count":1,"module_count":4}
{"model":"gpt-4.1","question":"Scenario: You are building a high-performance, AI-driven text-generation microservice for a real-time web application.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/generate endpoint using FastAPI (latest 0.100+) for incoming requests.\n2. Validate and parse the request payload with Pydantic v2 (latest) leveraging its new ArgModel feature.\n3. Enqueue each generation request as an asynchronous job using ARQ (latest 0.7+) backed by Redis.\n4. In the ARQ worker, invoke vLLM (latest 0.7+) AsyncLLMClient to perform streaming text inference.\n5. Persist each request and its full response to PostgreSQL using SQLModel (latest 0.0.x) with SQLAlchemy 2.0 async engine.\n6. Expose a WebSocket \/ws\/stream endpoint in FastAPI to broadcast partial LLM outputs to clients in real time.\n\nDeliverables:\n\u2022 The above numbered requirements list.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":30,"modules":["fastapi","pydantic","sqlmodel","sqlalchemy","arq","vllm"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: You must build a Python microservice that fetches articles by URL, summarizes them with AI, stores results in Postgres, and provides REST, GraphQL, and WebSocket interfaces with background orchestration using only the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a POST \/submit REST endpoint using the latest Starlite framework (released within the last 12 months) over HTTP\/3 to accept JSON { \"url\": string }.  \n2. Fetch article content asynchronously using the latest httpx library and handle timeouts and redirects.  \n3. Summarize text with the latest HuggingFace Transformers v5 pipeline, leveraging GPU if available.  \n4. Persist the original URL and its summary in Postgres via Tortoise ORM v0.25 (async ORM released in the last 12 months).  \n5. Expose a \/graphql endpoint using Strawberry GraphQL (latest release) to query summaries by URL.  \n6. Provide a \/ws\/notifications WebSocket in Starlite to notify clients in real time when a summary is ready.  \n7. Orchestrate fetch-and-summarize tasks as background jobs using Prefect 2 (newly released workflow orchestrator).\n\nDeliverables:\n- A copy of the numbered functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Use only the latest versions of Starlite, httpx, Transformers v5, Tortoise ORM v0.25, Strawberry GraphQL, and Prefect 2 as of 2025-05-06.\n- Include clear comments indicating where and how each external library is used.","question_index":70,"modules":["starlite","strawberry-graphql","httpx","transformers","tortoise-orm","prefect"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: You are tasked with developing a next-generation chat analytics platform that seamlessly integrates HTTP\/3, JWT auth, real-time messaging, vector search, and on-device LLM inference.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) with HTTP\/3 support via Hypercorn to serve all endpoints.\n2. Implement JWT authentication with rotational refresh tokens using fastapi-users (latest).\n3. Provide a real-time WebSocket chat endpoint using fastapi-websocket-pubsub (latest) supporting pub\/sub channels.\n4. On each incoming chat message, generate vector embeddings using llama-cpp-python (latest) with an on-device streaming pipeline, and store them in ChromaDB (latest) with HNSW indexing.\n5. Expose a REST endpoint to query semantic similarity: accept a text query, compute embeddings, perform a ChromaDB search, and return the top-3 similar messages.\n6. Expose a streaming summarization endpoint that returns an on-the-fly summary of recent messages using llama-cpp-python\u2019s streaming completion interface.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements you\u2019ve implemented.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":93,"modules":["fastapi","fastapi-users","fastapi-websocket-pubsub","llama-cpp-python","chromadb"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1","question":"Scenario: As a senior software engineer, develop a Python 3.11+ microservice that manages real-time collaborative whiteboard sessions, persists data, exposes REST and GraphQL APIs, generates session snapshots, integrates external metrics, and secures endpoints with OAuth2.\n\nFunctional Requirements:\n1. Implement session and user management REST endpoints using FastAPI (latest) with async event hooks and Pydantic V2 models.  \n2. Persist all data in PostgreSQL via Tortoise ORM (latest) using model lifecycle events.  \n3. Expose a GraphQL API with Strawberry GraphQL (latest) dataclass resolvers for session and user queries\/mutations.  \n4. Broadcast real-time drawing events using python-socketio (latest) over WebSockets.  \n5. Generate on-demand PNG snapshots of whiteboard state with Pillow-SIMD (latest) leveraging multi-threaded rendering.  \n6. Send session metrics asynchronously to an external analytics service using httpx (latest) with HTTP\/2 support.  \n7. Secure all endpoints with OAuth2 (authorization code + PKCE) using Authlib (latest).  \n\nDeliverables:\n- A numbered list of the functional requirements above.  \n- Complete, runnable Python 3.11+ code (\u2264 150 lines) integrating all requirements.  \n- Inline comments indicating where each external library is used.  \n- Use the latest stable releases of all libraries as of today\u2019s date.","question_index":50,"modules":["fastapi","pydantic","tortoise-orm","strawberry-graphql","python-socketio","pillow-simd","httpx","authlib"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1","question":"Scenario: You are a senior software engineer tasked with building a real-time analytics microservice that combines REST, GraphQL, persistent storage, and background processing using the latest Python ecosystem tools as of 2025-05-06.\n\nFunctional Requirements:\n1. Expose HTTP and WebSocket endpoints using the latest Litestar ASGI framework with annotated routing and streaming support.\n2. Implement a GraphQL API using the latest Strawberry GraphQL code-first schema builder with asyncio support.\n3. Persist and query analytics data in PostgreSQL via the latest Piccolo-ORM\u2019s async-first models and automatic migrations.\n4. Offload long-running analytics jobs to a Redis-backed async queue using the latest Arq library.\n5. Send WebSocket notifications to clients when background jobs complete.\n\nDeliverables:\n\u2022 A requirements list (pip install commands) specifying the latest versions of Litestar, Strawberry, Piccolo-ORM, and Arq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":65,"modules":["piccolo","strawberry-graphql","arq","litestar","asyncpg","redis","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1","question":"Scenario: You are a senior software engineer tasked with developing an end-to-end Python web application that leverages the latest libraries to showcase advanced API development, ORM integration, security, background processing, and real-time features.\n\nFunctional Requirements:\n1. Asynchronous REST API: implement CRUD endpoints for a `Task` model using FastAPI (latest version as of 2025-05-06).\n2. Data Modeling and Persistence: define `Task` ORM models with SQLModel (latest version) and connect to a SQLite or PostgreSQL database asynchronously.\n3. Security: secure all API endpoints with OAuth2 password flow using Authlib (latest version) to obtain and validate JWT tokens.\n4. Background Processing: create an asynchronous task queue with Arq (latest version) to process tasks in the background.\n5. Real-Time Notifications: establish a WebSocket endpoint using the websockets library (latest version) to push real-time task status updates.\n\nDeliverables:\n- The numbered list of functional requirements (as above).\n- Complete, runnable Python 3.11+ code under 150 lines. Use the latest versions of FastAPI, SQLModel, Authlib, Arq, and websockets as of 2025-05-06. Include clear comments indicating where and how each external library is used.","question_index":24,"modules":["fastapi","sqlmodel","sqlalchemy","authlib","arq","websockets","starlette"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1","question":"Scenario: Build a real-time collaborative messaging microservice integrating graph storage, vector search, JWT authentication, and WebSockets using the latest Python libraries (as of May 6, 2025).\n\nFunctional Requirements:\n1. Authentication: secure all endpoints with JWTs using the latest TokenGuard library.\n2. Message Ingestion: implement POST \/messages to accept user messages; decode JWT, extract user ID.\n3. Graph Storage: store each message and its \u201cauthored_by\u201d relationship in a graph database via the latest GraphitORM.\n4. Vector Indexing & Search: generate text embeddings with the latest Morpher library, store them in its vector store, and expose GET \/search?q={text} to return top-5 similar messages.\n5. Real-Time Updates: broadcast new messages to connected clients over WebSockets using the latest Voila framework.\n\nDeliverables:\n1. A requirements list with each third-party library pinned to its latest version as of May 6, 2025.\n2. A single Python 3.11+ script (\u2264150 lines) implementing all endpoints, with clear comments marking where each external library is used.","question_index":35,"modules":["fastapi","uvicorn","TokenGuard","GraphitORM","Morpher","Voila"],"modulenotfound":["TokenGuard","GraphitORM","Morpher"],"modulenotfound_count":3,"module_count":6}
{"model":"gpt-4.1","question":"You are tasked with building a high-performance text summarization service with real-time updates, persistent storage, and caching for internal research teams.\n\nFunctional Requirements:\n1. Implement OAuth2 password flow with JWT tokens for user authentication using the latest python-jose.\n2. Expose a POST \/summarize endpoint that uses the latest transformers library to perform text summarization.\n3. Provide a WebSocket at \/ws\/progress to stream summarization progress in real time using FastAPI\u2019s latest WebSocket support.\n4. Persist user requests and summaries in a SQLite database via the latest SQLModel ORM.\n5. Cache summary results in Redis for 5 minutes using the latest aioredis library to accelerate repeat requests.\n\nDeliverables:\n\u2022 A requirements list specifying the latest versions of FastAPI, transformers, python-jose, SQLModel, aioredis (as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":10,"modules":["fastapi","transformers","jose","sqlmodel","pydantic","aioredis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"You are a senior software engineer.\n\nScenario: Build an end-to-end Python service for user profile CRUD with high-performance caching and real-time notifications.\n\nFunctional Requirements:\n1. Implement RESTful CRUD endpoints for user profiles using the latest FastAPI (as of 2025-05-06), leveraging its async lifespan context.\n2. Define an async PostgreSQL-backed User model with SQLModel (latest) using Relations and the new async engine.\n3. Cache GET \/users\/{id} responses in Redis with redis.asyncio (latest), utilizing cluster mode and automatic TTL invalidation on updates.\n4. Broadcast profile change events in real time over WebSocket using the websockets library (latest) with permessage-deflate compression.\n\nDeliverables:\n- A requirements.txt listing the latest versions of FastAPI, SQLModel, redis.asyncio and websockets as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":17,"modules":["fastapi","sqlmodel","redis","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4.1","question":"Scenario: You are a senior software engineer building a real-time sentiment-analysis web service for social media feeds.\n\nFunctional Requirements:\n1. Use the latest Python web framework (released within the last 12 months) to expose REST endpoints with dependency injection and OpenAPI support.\n2. Implement real-time websocket notifications for incoming sentiment updates using a recently released WebSocket library (last 12 months).\n3. Persist incoming posts and sentiment scores in a NoSQL database via its newest async client library (released within the last 12 months).\n4. Offload sentiment inference to a CPU-optimized ML library (first released within the last 12 months) with zero-copy or memory-efficient batch inference.\n5. Schedule periodic cleanup and summary-generation tasks using a lightweight scheduler library introduced within the last 12 months.\n6. Serve a minimal interactive dashboard at \u201c\/dashboard\u201d using a modern, Python-native plotting\/UI library created in the last 12 months.\n\nDeliverables:\n- A bullet list of all third-party libraries chosen, pinned to \u201clatest\u201d versions as of today\u2019s date.\n- A single, complete Python 3.11+ file (under 150 lines) that implements all requirements, with clear comments marking where each external library is used.","question_index":94,"modules":["litestar","motor","inference","minidoro","datapane-lite"],"modulenotfound":["minidoro","datapane-lite"],"modulenotfound_count":2,"module_count":5}
{"model":"gpt-4.1","question":"Scenario: As a senior software engineer, you must build a real-time analytics backend that ingests user events, persists them, and serves both REST and GraphQL clients with low latency and live updates.\n\nFunctional Requirements:\n1. Implement a POST \/events endpoint using the latest FastAPI (as of 2025-05-06), leveraging its new lifespan context and async router features.\n2. Validate incoming JSON payloads with Pydantic v2\u2019s new @model_validator decorator for strict schema enforcement.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise ORM, utilizing its recent bulk_create optimization.\n4. Expose a GraphQL query and mutation schema via the latest Strawberry GraphQL with code-first federation support.\n5. Broadcast new events to connected clients over WebSockets using the latest websockets library with per-message deflate compression.\n\nDeliverables:\n\u2022 A requirements.txt listing each third-party library pinned to its latest version as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments indicating where and how each external library is used.","question_index":19,"modules":["fastapi","pydantic","tortoise","strawberry","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1","question":"Scenario: You are a senior software engineer building a modern microblogging backend that supports authenticated posts, AI-generated images, real-time feed updates, and cloud storage integration.\n\nFunctional Requirements:\n1. Implement a REST API using a Python web framework released in the last 12 months (use the latest available as of 2025-05-06).\n2. Persist posts in a SQL database via an async ORM library created in the last 12 months, leveraging advanced async model features.\n3. Broadcast new posts to connected clients over WebSockets using a recently released high-performance async messaging library.\n4. Authenticate users with OAuth2 (Google) using the latest OAuth client library.\n5. Generate post images by calling Stable Diffusion through the most recent Python API client for AI image generation.\n6. Store and serve AI-generated images in AWS S3 using the newest AWS SDK for Python.\n7. Ensure all components run under Python 3.11+ and fit within 150 lines of code.\n\nDeliverables:\n\u2022 A numbered list of the specific libraries (with versions) you chose to satisfy each requirement.  \n\u2022 Complete, runnable Python 3.11+ code (<150 lines) combining all features.  \n\u2022 Clear inline comments marking where and why each external library is used.","question_index":7,"modules":["litestar","pixie-orm","wsx","diffuzers","aiobotocore"],"modulenotfound":["pixie-orm"],"modulenotfound_count":1,"module_count":5}
{"model":"gpt-4.1","question":"Scenario: You are a senior software engineer tasked with building a serverless real-time IoT analytics service that ingests sensor data, performs streaming aggregation, applies online anomaly detection, and streams results to clients.\n\nFunctional Requirements:\n1. Ingest JSON messages from an MQTT broker (\u201csensors\/+\/data\u201d) using the latest asyncio-based gmqtt library (as of 2025-05-06).  \n2. Stream-process incoming messages with streamz (latest version) to compute 1-minute tumbling-window averages for temperature and humidity.  \n3. Apply online anomaly detection on each windowed aggregation using River\u2019s latest HDBSCAN-based drift detector.  \n4. Persist raw messages and aggregated results into TimescaleDB via the asyncpg driver (latest version).  \n5. Expose a Python 3.11+ FastAPI application with a WebSocket endpoint for live metric and anomaly alert streaming.  \n6. Wrap the FastAPI app for AWS Lambda deployment using the latest mangum adapter.\n\nDeliverables:\n\u2022 A list of third-party dependencies pinned to \u201clatest\u201d versions as of 2025-05-06.  \n\u2022 A single, complete, runnable Python 3.11+ source file under 150 lines.  \n\u2022 Clear inline comments showing where each external library is used.  \n\u2022 The code must integrate all functional requirements and be deployable serverlessly on AWS Lambda.","question_index":38,"modules":["asyncpg","fastapi","gmqtt","mangum","river","streamz","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1","question":"Scenario:\nYou are tasked with developing a lightweight AI-powered semantic search microservice in Python 3.11+.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI framework with HTTP\/3 support) to expose a POST \/search endpoint.  \n2. Validate and parse incoming JSON with the latest Pydantic v2\u2019s strict model parsing and Python pattern-matching features.  \n3. Asynchronously fetch text embeddings from OpenAI\u2019s embedding API using the latest HTTPX client with HTTP\/3.  \n4. Store and query vectors in a GPU-accelerated ChromaDB instance via the latest ChromaDB Python client.  \n5. Return the top 5 semantically related document IDs and similarity scores as a JSON response.\n\nDeliverables:\n\u2022 A numbered list confirming the above functional requirements.  \n\u2022 A single, complete, runnable Python 3.11+ source file (\u2264150 lines) that implements the microservice, with clear comments indicating where each external library is used.","question_index":3,"modules":["starlite","pydantic","httpx","chromadb","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1","question":"Scenario: Build a real-time chat analysis service that ingests user messages, performs on-device LLM summarization and sentiment analysis, caches results, persists chat history, and broadcasts updates via WebSockets.\n\nFunctional Requirements:\n1. Expose a POST \/messages endpoint using FastAPI (latest) to receive JSON payloads with \u201cuser_id\u201d and \u201ctext\u201d.\n2. Perform sentiment analysis and summarization on each message using llama-cpp-python (latest) in an async background task.\n3. Cache sentiment results for identical texts in Redis via redis-py (latest) to avoid recomputation.\n4. Persist messages, sentiments, and summaries in PostgreSQL using SQLModel (latest version).\n5. Provide a WebSocket \/ws\/{user_id} endpoint (FastAPI) that broadcasts new summaries and sentiments to connected clients in real time.\n6. Secure all endpoints with OAuth2 password flow via Authlib (latest).\n\nDeliverables:\n\u2022 Restate the numbered requirements above.  \n\u2022 Provide complete, runnable Python 3.11+ code integrating the latest versions of FastAPI, llama-cpp-python, redis-py, SQLModel, and Authlib, under 150 lines total.  \n\u2022 Include clear comments indicating where and how each external library is used.","question_index":32,"modules":["fastapi","sqlmodel","psycopg2","redis","llama-cpp-python","authlib","starlette"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python microservice for real-time IoT sensor analytics and visualization.\n\nFunctional Requirements:\n1. REST API: Use the latest asynchronous Python web framework available as of 2025-05-06 to expose CRUD endpoints with advanced dependency injection and OpenAPI schema generation.\n2. WebSockets: Integrate the latest real-time streaming library as of 2025-05-06 supporting protocol-level backpressure to push live sensor updates to clients.\n3. ML Inference: Embed an on-the-fly analytics model using the latest Python JIT-accelerated inference library as of 2025-05-06, auto-batching requests on CPU\/GPU.\n4. Graph Storage: Persist sensor relationships in a distributed graph database via the latest async driver as of 2025-05-06 with reactive query pipelining.\n5. Dashboard UI: Serve an interactive web dashboard using the latest server-driven UI component library as of 2025-05-06 for real-time charting and controls.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":14,"modules":["torch","fastapi","fastapi_websocket_pubsub","async_redisgraph"],"modulenotfound":["async_redisgraph"],"modulenotfound_count":1,"module_count":4}
{"model":"gpt-4.1","question":"You are a Senior Software Engineer tasked with building a high-performance, real-time analytics microservice using the latest asynchronous Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled ASGI endpoint using the latest Litestar and aioquic (as of 2025-05-06) for bi-directional streaming.\n2. Implement real-time GraphQL subscriptions with the latest Strawberry GraphQL to push analytics updates.\n3. Define domain models and perform asynchronous CRUD operations on Postgres using the latest SQLModel.\n4. Integrate a Redis cache layer with the latest aioredis for hot-path data retrieval.\n5. Instrument request handling and background tasks with OpenTelemetry SDK for distributed tracing.\n\nDeliverables:\n- requirements.txt listing the latest library versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments indicating where each external library is used.","question_index":90,"modules":["litestar","aioquic","strawberry-graphql","sqlmodel","sqlalchemy","asyncpg","aioredis","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation","opentelemetry-instrumentation-asgi","uvloop"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"gpt-4.1","question":"You are building a real-time collaborative code-review web application that leverages the latest AI, async, storage and security frameworks.\n\nFunctional Requirements:\n1. Implement a FastAPI 0.95+ server with async endpoints and Jinja2 templates for the main UI.\n2. Enable real-time code editing and review comments via WebSockets using the latest websockets library as of 2025-05-06.\n3. On each code submission, stream analysis from OpenAI\u2019s GPT-4o API (or equivalent newest model) and display suggestions in real time.\n4. Persist code snapshots and review metadata in Pinecone\u2019s newest Python client (vector storage) for similarity search.\n5. Secure all HTTP and WS routes with JWT authentication using the latest PyJWT release.\n6. Add structured logging and a \/healthz endpoint using structlog for observability.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total.  \n\u2022 Clear comments indicating where and why each external library is used.","question_index":37,"modules":["jwt","structlog","fastapi","openai","pinecone-client","python-dotenv"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: Build a real-time Q&A web application that ingests documents, generates embeddings, serves semantic search, supports live WebSocket notifications, and exposes observability metrics.\n\nFunctional Requirements:\n1. Use the latest FastAPI (>=0.100) to implement asynchronous REST endpoints for document ingestion and search, plus a WebSocket endpoint leveraging its new lifecycle event hooks.\n2. Upon document ingestion, asynchronously generate embeddings with the latest llama-cpp-python (v0.2+) using 4-bit quantization and store vectors in Pinecone (latest) using hybrid search.\n3. Implement a `\/search` endpoint that queries Pinecone for nearest neighbors based on user input and returns ranked results.\n4. Schedule periodic re-indexing tasks with the latest arq (v1.10+), utilizing its async Redis job scheduler.\n5. Integrate OpenTelemetry Python SDK (v1.21+) for auto-instrumentation, exporting traces and metrics to Prometheus.\n\nDeliverables:\n- A requirements.txt listing the latest libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":33,"modules":["fastapi","uvicorn","llama_cpp_python","pinecone-client","arq","redis","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","opentelemetry-exporter-prometheus","opentelemetry-instrumentation-redis","pydantic","prometheus-client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"gpt-4.1","question":"Scenario: As a senior software engineer, you must build a modular Python web application that serves real-time, ML-enhanced classification results with persistent storage and observability.\n\nFunctional Requirements:\n1. API Layer: Use the latest ASGI web framework available as of 2025-05-06 that supports HTTP\/3 and automatic OpenAPI generation to expose a POST \/classify endpoint.  \n2. Database Layer: Connect asynchronously to PostgreSQL using the latest async driver with statement pooling to record incoming requests and classification results.  \n3. Real-time Pub\/Sub: Implement server-side WebSocket broadcasts of classification results using the latest message queue library offering at-least-once delivery semantics.  \n4. ML Inference: Integrate on-demand image classification via the latest lightweight, GPU-accelerated inference engine released in the past 12 months.  \n5. Observability: Instrument distributed tracing and metrics collection using the latest OpenTelemetry-compatible tracing library.\n\nDeliverables:\n- A list of the latest library dependencies as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":18,"modules":["litestar","asyncpg","aiokafka","onnxruntime-gpu","opentelemetry-sdk","opentelemetry-instrumentation-litestar","pillow","python-multipart","starlette","numpy"],"modulenotfound":["opentelemetry-instrumentation-litestar"],"modulenotfound_count":1,"module_count":10}
{"model":"gpt-4.1","question":"Scenario: As a senior software engineer, build a high-performance microblogging API using the latest Python libraries for REST, validation, async database operations, caching, background tasks, and real-time streaming.\n\nFunctional Requirements:\n1. Implement a user signup endpoint in FastAPI (>=0.100.0) using Pydantic v2 root_validator for advanced schema validation.  \n2. Create and list posts asynchronously in PostgreSQL via SQLModel\u2019s latest async engine.  \n3. Maintain a cache of the 10 most recent posts in Redis using redis-py (>=5.0) asyncio client, updating on post creation.  \n4. Offload image thumbnail generation to an async Dramatiq (>=2.0) background task triggered on new post with image.  \n5. Provide a WebSocket endpoint in FastAPI to broadcast newly created posts to connected clients in real time.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library (FastAPI, Pydantic v2, SQLModel, redis-py asyncio, Dramatiq) is used. Use the latest available versions of all libraries as of 2025-05-06.","question_index":87,"modules":["fastapi","pydantic","sqlmodel","redis","dramatiq","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1","question":"Scenario: You are building a real-time news aggregation microservice with live client updates.\n\nFunctional Requirements:\n1. Develop a REST API using Litestar (the latest version as of 2025-05-06) leveraging its OpenAPI v3.1 auto-generation feature.\n2. Asynchronously fetch and parse external RSS feeds over HTTP\/3 using HTTPX AsyncClient (the latest), then cache results in aiocache (the latest) with a TTL.\n3. Validate and serialize each article using Pydantic v2 (the latest) functional-style models to ensure data integrity.\n4. Expose a WebSocket endpoint using the websockets library (the latest) with permessage-deflate compression for real-time article broadcasting.\n5. Schedule periodic background tasks with Arq (the latest) to re-poll feeds every 5 minutes.\n\nDeliverables:\n- A list of the chosen libraries and their latest versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":31,"modules":["litestar","httpx","aiocache","pydantic","websockets","arq","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1","question":"Scenario: Build a real-time collaborative note-taking microservice integrating REST, database persistence, caching, GraphQL, WebSockets, and background tasks.\n\nFunctional Requirements:\n1. Expose REST endpoints using FastAPI (latest) for creating, updating and retrieving notes; leverage Pydantic v2\u2019s new validation decorators.\n2. Persist notes asynchronously in SQLite via SQLModel (latest) with SQLAlchemy 2.0-style typing and async engine.\n3. Broadcast note creation and updates over WebSockets with FastAPI\u2019s WebSocket support to subscribed clients in real time.\n4. Cache note retrieval results in Redis using aioredis (latest), implement TTL and cache invalidation on updates.\n5. Provide a GraphQL schema and endpoint using Strawberry (latest) to query and filter notes with federation-style resolvers.\n6. Schedule a daily summary email using FastAPI BackgroundTasks and aiosmtplib (latest) for asynchronous SMTP delivery.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- A single Python 3.11+ source file under 150 lines that implements all requirements, with clear comments marking where each external library is used.","question_index":73,"modules":["fastapi","pydantic","sqlmodel","sqlalchemy","aioredis","strawberry-graphql","aiosmtplib"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1","question":"Scenario: You are a senior software engineer building a proof-of-concept Python service that ingests user text, generates embeddings and classifications via an LLM, stores them, and streams nearest-neighbor results in real time.\n\nFunctional Requirements:\n1. Implement a JWT-secured POST \/ingest endpoint using the latest async Python web framework as of 2025-05-06 (e.g., Litestar).  \n2. In \/ingest, accept JSON `{ \"text\": \"...\" }`, call a state-of-the-art LLM (via the latest Transformers or LangChain client) to produce both an embedding vector and a classification label.  \n3. Persist the embedding vector in a vector database using the newest ChromaDB client.  \n4. Log the original text and its classification into a relational database through the latest async ORM (e.g., Prisma Python).  \n5. Expose a GET \/stream endpoint that streams nearest-neighbor matches for incoming embeddings over server-sent events using the newest SSE library (e.g., httpx-sse or framework-native support).  \n6. Target Python 3.11+, include type hints, and keep all code under 150 lines.\n\nDeliverables:\n\u2022 A `requirements.txt` (or equivalent) listing each library with exact versions (latest as of 2025-05-06).  \n\u2022 A single Python module (\u2264150 lines) that meets all requirements, with clear comments marking where and why each external library is used.","question_index":81,"modules":["litestar","prisma","chromadb","transformers","jose"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-mini","question":"Scenario: As a senior software engineer, build a high-performance Python web application integrating real-time streaming, GraphQL, async ORM, JWT security, and caching using the latest third-party libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. HTTP & WebSocket server with the latest Litestar, utilizing its new streaming response API for real-time event delivery.\n2. GraphQL API with the latest Strawberry-GraphQL, defining queries, mutations, and subscription streams for live data updates.\n3. Async database models and CRUD operations using the latest Piccolo ORM\u2019s Python 3.11+ async support and built-in migration tooling.\n4. JWT authentication via the latest python-jose, supporting secure login, token rotation, and injection of user context into requests.\n5. Redis-backed caching with the latest aioredis to cache GraphQL query results and automatically invalidate on data changes.\n6. Trigger real-time client notifications over WebSockets when database events occur, leveraging Litestar\u2019s WebSocket routing.\n\nDeliverables:\n\u2022 A numbered list of the final functional requirements.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":57,"modules":["litestar","strawberry-graphql","piccolo","python-jose","aioredis","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-mini","question":"Scenario: You are tasked with building a modular Python 3.11+ microservice that delivers real-time chat with semantic search and scheduled maintenance, leveraging only the latest libraries released within the past 12 months.\n\nFunctional Requirements:\n1. Implement HTTP REST endpoints for posting and retrieving chat messages using Litestar (latest release).  \n2. Establish a WebSocket chat channel that broadcasts new messages to all connected clients via the official Litestar WebSocket plugin (latest).  \n3. Persist each message to MongoDB with Beanie ODM (latest) and simultaneously index its vector embedding into Qdrant using qdrant-client (latest).  \n4. Expose a GraphQL query endpoint that performs semantic search over stored messages via strawberry-graphql (latest).  \n5. Schedule a daily background job to purge messages older than 30 days using Dramatiq (latest).\n\nDeliverables:\n- A requirements.txt (or pyproject.toml) listing all dependencies with exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments marking where and how each external library is used.","question_index":76,"modules":["beanie","litestar","litestar_ws_plugin","motor","qdrant-client","strawberry-graphql","dramatiq","redis","uvicorn"],"modulenotfound":["litestar_ws_plugin"],"modulenotfound_count":1,"module_count":9}
{"model":"gpt-4.1-mini","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python web service that combines asynchronous REST & GraphQL APIs, real-time notifications, persistent storage, caching, and AI-driven text summarization.\n\nFunctional Requirements:\n1. Implement async HTTP endpoints with FastAPI (latest as of 2025-05-06) using Python 3.11 structural pattern matching and Pydantic v2 dataclass models.\n2. Provide a GraphQL endpoint powered by Strawberry GraphQL v1.0+ with schema federation support.\n3. Persist and query data in PostgreSQL via the latest Tortoise-ORM with asyncio-optimized connection pooling.\n4. Push real-time notifications over WebSockets using python-socketio latest async server mode.\n5. Summarize user-submitted text on demand using the latest llama-cpp-python library with optional GPU offload.\n6. Cache frequent database queries in Redis using aioredis latest cluster-mode support.\n\nDeliverables:\n- A requirements list specifying exact versions of all third-party libraries.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":74,"modules":["fastapi","pydantic","tortoise-orm","strawberry-graphql","python-socketio","aioredis","llama-cpp-python","uvicorn","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1-mini","question":"Scenario: Build a real-time collaborative note-taking service that supports user auth, live updates, AI summaries, PDF export, and caching using the latest Python web stack.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST API for user signup\/login with FastAPI (latest) using OAuth2 password flow.\n2. Persist users and notes in PostgreSQL via SQLModel (latest async features).\n3. Create a WebSocket endpoint broadcasting live note edits using the newest websockets library.\n4. Add an endpoint `\/summarize` that calls an external AI summarization API via httpx\u2019s HTTP\/3 client (latest).\n5. Provide a `\/export\/{note_id}` endpoint that generates a PDF of the note using Playwright Python (latest).\n6. Cache active JWT tokens in Redis Streams using aioredis v2.x.\n7. Emit structured JSON logs via loguru (latest advanced configuration).\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- A complete, runnable Python 3.11+ codebase under 150 lines, with clear comments indicating where each external library is used.","question_index":9,"modules":["fastapi","jose","loguru","passlib","pydantic","sqlmodel","httpx","aioredis","playwright","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1-mini","question":"You are a senior software engineer tasked with building a minimal Python web service for text ingestion, semantic search, and AI-powered summarization using the latest libraries.\n\nFunctional Requirements:\n1. HTTP API: use the latest Starlite framework (as of 2025-05-06) to expose async endpoints.\n2. Text Storage: on POST \/submit, accept JSON {\u201ctext\u201d: \u2026}, store original text and metadata in Redis via the latest redis-om Python client with async HashModel.\n3. Embedding: upon submission, generate a vector embedding using the latest ChromaDB Python client\u2019s async API and upsert into its vector store.\n4. Semantic Search: implement GET \/search?query=\u2026 that computes a query embedding via ChromaDB and returns top 3 matching texts from Redis.\n5. Summarization: implement GET \/summarize\/{id} to retrieve stored text by ID and produce a concise summary using the latest llama-cpp-python streaming API.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries (as of 2025-05-06).  \n\u2022 A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments indicating where and how each external library is used.","question_index":45,"modules":["starlite","redis-om","chromadb","llama-cpp-python","pydantic","httpx","anyio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"Scenario: Senior software engineer tasked with developing a real-time GraphQL web service that validates incoming event data, persists it to PostgreSQL, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI web framework) as of 2025-05-06 to implement an async HTTP POST endpoint at `\/events`.\n2. Validate incoming JSON payloads with Pydantic v2 (latest) models, including at least one custom field validator.\n3. Persist validated events to PostgreSQL using the latest Prisma Python client (type-safe generated models and migrations).\n4. Expose a GraphQL API with the latest Strawberry GraphQL library, supporting queries for event history and subscriptions for real-time updates.\n5. Configure WebSocket support in Starlite to broadcast new events to all GraphQL subscription clients.\n\nDeliverables:\n- A brief recap of the functional requirements.\n- Complete, runnable Python 3.11+ code under 150 lines that uses the latest versions of each library as of 2025-05-06, with clear comments marking where each external library is imported and utilized.","question_index":22,"modules":["pydantic","prisma","starlite","strawberry-graphql","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-mini","question":"Scenario: Design a minimal real-time chat service combining HTTP endpoints, data validation, async ORM persistence, WebSocket broadcasting, and JWT security using the latest Python libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Starlite to expose POST \/register and GET \/messages endpoints, leveraging its advanced dependency injection system.\n2. Define and serialize request\/response models with the latest Pydantic v2, employing its new `model_serializer` API.\n3. Persist User and Message models to SQLite asynchronously with the latest Ormar ORM, utilizing its built-in async migration feature.\n4. Implement a `\/ws` WebSocket route using the latest websockets library for real-time message broadcasting to connected clients.\n5. Secure both HTTP routes and WebSocket connections with JWT authentication via the latest PyJWT release.\n\nDeliverables:\n- A concise mapping of each numbered requirement to the specific library and feature used.\n- Complete, runnable Python 3.11+ code under 150 lines total, with clear comments indicating where each external library is imported and applied.","question_index":21,"modules":["starlite","pydantic","databases","sqlalchemy","ormar","websockets","PyJWT","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-mini","question":"Scenario: You are building a Python microservice that accepts user-uploaded text files, asynchronously generates summaries with a local LLM, and streams live progress to clients via WebSocket.\n\nFunctional Requirements:\n1. Implement a FastAPI app (use the latest FastAPI as of 2025-05-06) with an `\/upload` POST endpoint that accepts plain-text file uploads.  \n2. Upon upload, enqueue a background job using arq (latest version) to process the file asynchronously.  \n3. In the arq worker, load a local LLM via llama-cpp-python (latest version) using its new GPU offloading API to generate a summary.  \n4. Configure python-socketio (latest version) as an ASGI middleware in FastAPI to broadcast progress and final summary events to connected WebSocket clients.\n\nDeliverables:\n- A bullet list of the chosen third-party libraries with their exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":46,"modules":["fastapi","arq","python-socketio","llama-cpp-python","uvicorn","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-mini","question":"Scenario: Build an end-to-end real-time customer feedback summarization service with live dashboard updates.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled JSON ingest endpoint using Litestar (latest as of 2025-05-06) with async support.\n2. Define a feedback model with Pydantic v2 and persist submissions into SQLite asynchronously via SQLModel (latest).\n3. On each new submission, call the OpenAI Python SDK (latest) asynchronously to generate a brief summary.\n4. Cache each summary in Redis via redis-om (latest) with a 1-hour TTL.\n5. Serve a live dashboard using Reflex (latest) that connects over WebSockets to display incoming summaries in real time.\n6. Use HTTPX (latest) with HTTP\/3 support for any external HTTP calls.\n\nDeliverables:\n\u2022 requirements.txt listing all dependencies pinned to the latest versions as of 2025-05-06  \n\u2022 A single Python 3.11+ script (under 150 lines) that implements the above, with clear comments indicating where and why each external library is used","question_index":61,"modules":["litestar","sqlmodel","pydantic","openai","httpx","redis-om","reflex","asyncpg","aioredis","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1-mini","question":"Scenario: Develop a Python 3.11+ web application that provides secure collaborative document management with GraphQL, real-time WebSocket updates, AI-powered summarization, and on-demand PDF export.\n\nFunctional Requirements:\n1. Implement a GraphQL API for document queries and mutations using the latest \u201cariadne\u201d library with subscription support.\n2. Enable OAuth2 login with passwordless WebAuthn using the latest \u201cauthlib\u201d and \u201cfido2\u201d libraries.\n3. Provide real-time document update notifications over WebSocket leveraging FastAPI\u2019s built-in WebSocket support.\n4. Schedule asynchronous AI summarization tasks using the latest \u201cllama-cpp-python\u201d library and \u201cAPScheduler\u201d for background job management.\n5. Generate PDF exports of documents on demand using the latest \u201cplaywright\u201d headless browser library in Python.\n6. Persist documents and user metadata in PostgreSQL via the latest \u201ctortoise-orm\u201d with native async and Pydantic v2 model support.\n\nDeliverables:\n- A brief requirements list naming each third-party library and its version.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used. Use the latest library versions available as of 2025-05-06.","question_index":88,"modules":["fastapi","ariadne","fido2","authlib","tortoise-orm","apscheduler","llama-cpp-python","playwright"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-mini","question":"Scenario: You are designing a Python microservice for ingesting user text, storing embeddings, and providing real-time, rate-limited on-device summaries via WebSockets.\n\nFunctional Requirements:\n1. HTTP POST \/ingest  \n   - Validate payload (id: str, text: str) using Pydantic v2 root validators  \n   - Compute embeddings on-device with llama-cpp-python and store (id, embedding, text) in ChromaDB  \n2. HTTP GET \/query  \n   - Accept query text, validate with Pydantic v2  \n   - Compute embedding via llama-cpp-python, retrieve top-5 nearest documents from ChromaDB, return metadata  \n3. WebSocket \/summarize  \n   - Client sends document id  \n   - Fetch text from ChromaDB metadata  \n   - Stream back a summary using llama-cpp-python streaming API  \n   - Enforce token-per-second rate limiting via aiolimiter  \n4. Lifespan events  \n   - On startup, initialize ChromaDB client, Llama model, and aiolimiter  \n   - On shutdown, cleanly close any resources  \n\nDeliverables:\n- requirements.txt listing the latest available versions as of today of:\n  \u2022 starlite  \n  \u2022 pydantic v2  \n  \u2022 chromadb  \n  \u2022 llama-cpp-python  \n  \u2022 aiolimiter  \n- main.py: a single, runnable Python 3.11+ script (under 150 lines) implementing all above, with clear comments marking where each external library is used.","question_index":8,"modules":["starlite","pydantic","chromadb","llama_cpp","aiolimiter"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":5}
{"model":"gpt-4.1-mini","question":"Scenario: Build a serverless Python application enabling real-time geolocated chat with sentiment analysis, dynamic map visualization, and PDF session reporting.\n\nFunctional Requirements:\n1. Real\u2010time WebSocket chat using the latest Litestar (released in the last 12 months) with asynchronous message persistence via a modern async ORM.\n2. Interactive geospatial mapping of incoming messages on the client side using the latest Python geospatial visualization library released in the last 12 months.\n3. On-the-fly sentiment analysis of each message using the latest Python sentiment analysis library released in the last 12 months.\n4. PDF generation of the entire chat session\u2014including message timeline, geolocation markers, and sentiment scores\u2014using the latest Python PDF library released in the last 12 months.\n5. Deployment configuration for AWS Lambda using the latest Serverless Framework plugin and AWS Lambda Powertools (all released in the last 12 months).\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":97,"modules":["litestar","piccolo-orm","flair","folium","borb","aws-lambda-powertools","uvicorn"],"modulenotfound":["piccolo-orm"],"modulenotfound_count":1,"module_count":7}
{"model":"gpt-4.1-mini","question":"Scenario: You are a senior software engineer tasked with building a real-time event analytics dashboard that ingests events, stores them, generates AI-driven summaries, caches counts, and streams updates to authenticated clients with full observability.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to implement a POST \/events REST endpoint for event ingestion.\n2. Validate incoming JSON payloads with the latest Pydantic v2 models.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise-ORM.\n4. Cache aggregated event counts in Redis via the latest aioredis.\n5. Generate streaming summaries of event batches with the latest OpenAI Python client.\n6. Stream real-time updates to clients over WebSocket using FastAPI.\n7. Secure all HTTP and WebSocket endpoints with JWT authentication via the latest fastapi-jwt-auth.\n8. Instrument HTTP, database, cache, and AI client calls with the latest OpenTelemetry for distributed tracing.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest library names and versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines integrating all subdomains, with clear comments indicating where each external library is used.","question_index":4,"modules":["fastapi","fastapi-jwt-auth","pydantic","tortoise-orm","aioredis","openai","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-asyncpg","opentelemetry-instrumentation-redis","opentelemetry-instrumentation-httpx","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":13}
{"model":"gpt-4.1-mini","question":"Scenario: You\u2019re building a high-performance async task management API using the latest libraries in web framework, async ORM, caching, background processing, and observability as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement CRUD REST endpoints for a Task model using the latest starlite.\n2. Persist tasks in Postgres asynchronously with the latest Tortoise-ORM.\n3. Enqueue a background job on task creation using the latest Dramatiq with Redis broker.\n4. Cache GET \/tasks\/{id} responses in Redis via the latest redis.asyncio.\n5. Instrument all HTTP and database calls with OpenTelemetry using the latest opentelemetry-instrumentation-starlite.\n\nDeliverables:\n- A requirements.txt listing each external dependency with explicit versions.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":34,"modules":["starlite","pydantic","tortoise-orm","dramatiq","redis","opentelemetry-sdk","opentelemetry-instrumentation-starlite","opentelemetry-instrumentation-tortoise-orm","asyncpg","uvicorn"],"modulenotfound":["opentelemetry-instrumentation-starlite","opentelemetry-instrumentation-tortoise-orm"],"modulenotfound_count":2,"module_count":10}
{"model":"gpt-4.1-mini","question":"Scenario  \nYou are building an AI-powered document search microservice that ingests user uploads, indexes them with a GPU-accelerated vector store, caches recent queries, and exposes HTTP\/3 endpoints for real-time search.  \n\nFunctional Requirements  \n1. HTTP API (Starlite, latest)  \n   1.1 POST \/upload: receive plain-text documents, validate with Pydantic.  \n   1.2 GET \/query?q=\u2026: return top-k results for query.  \n2. Document Ingestion (llama-index, latest)  \n   2.1 Stream tokenized embeddings upon upload.  \n3. Vector Store (ChromaDB, latest)  \n   3.1 Use GPU-backed indexing for fast similarity search.  \n4. Caching Layer (redis-om, latest)  \n   4.1 Cache each query + results with TTL and Pydantic models.  \n\nDeliverables  \n\u2022 A requirements.txt listing the latest versions (as of 2025-05-06) of all third-party libraries, each released within the past 12 months.  \n\u2022 Complete, runnable Python 3.11+ code (\u2264150 lines) implementing all requirements.  \n\u2022 Clear inline comments indicating where and how each external library is used.","question_index":47,"modules":["starlite","pydantic","llama-index","chromadb","redis-om","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-mini","question":"Scenario: You are a senior engineer building a high-throughput job processing microservice with a modern async Python stack.  \n\nFunctional Requirements:  \n1. Expose a POST \/jobs endpoint using the latest Starlite (as of 2025-05-06) to accept JSON job submissions.  \n2. Validate incoming JSON payloads with the latest Pydantic v2 models.  \n3. Persist job records to PostgreSQL via the latest prisma-client-py async ORM.  \n4. Enqueue and process jobs in the background using the latest Taskiq queue.  \n5. Expose a GET \/jobs\/{id}\/status endpoint to retrieve current job status.  \n6. Ensure all I\/O is non-blocking and use Python 3.11+ async\/await throughout.  \n\nDeliverables:  \n\u2022 A requirements list specifying the exact latest versions of Starlite, Pydantic v2, prisma-client-py, and Taskiq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ source code under 150 lines.  \n\u2022 Clear inline comments marking where each external library is used.","question_index":99,"modules":["starlite","pydantic","prisma","taskiq","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-mini","question":"Scenario: Build a secure, real-time AI-powered chat web service.\n\nFunctional Requirements:\n1. Expose an async REST API using the latest FastAPI for user registration and login endpoints.  \n2. Implement JWT authentication with rotating refresh tokens via the latest Authlib.  \n3. Persist users and chat messages in SQLite using the latest SQLModel, leveraging its new async support.  \n4. Enable real-time chat notifications over WebSockets using the latest python-socketio and ASGI.  \n5. Integrate AI chat completions using the latest OpenAI Python client in an async background task.\n\nDeliverables:\n1. A requirements.txt listing exact, up-to-date package versions as of today\u2019s date.  \n2. A single Python 3.11+ file (<150 lines) containing complete, runnable code with clear comments marking where each external library is used.","question_index":91,"modules":["fastapi","sqlmodel","passlib","jose","authlib","socketio","openai"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"Scenario: You are a senior software engineer tasked with building an AI-powered collaborative document annotation API.\n\nFunctional Requirements:\n1. Secure user signup and login with JWT-based authentication using Starlite (latest) and Pydantic v2.\n2. REST endpoint to upload documents and persist metadata in Redis OM Python (latest).\n3. WebSocket route for real-time annotation broadcasting using Starlite\u2019s built-in WebSocket support.\n4. Background summarization of uploaded documents using llama-cpp-python (latest).\n\nDeliverables:\n- A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":1,"modules":["starlite","pydantic","redis-om","llama-cpp-python","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-mini","question":"Scenario: You are a senior engineer tasked with building a secure, high-throughput microservice for customer order ingestion, persistence, background processing, and caching.\n\nFunctional Requirements:\n1. Use FastAPI (latest as of 2025-05-06) with HTTP\/3 support to implement POST \/orders accepting JSON order data.\n2. Define an asynchronous Order model with SQLModel (latest) and connect to an async database engine (SQLite or PostgreSQL).\n3. Enqueue and process new orders via Arq (latest) background worker.\n4. Implement OAuth2 with JWT issuance and protected endpoints using Authlib (latest).\n5. Cache GET \/orders\/{id} responses in Redis using aiocache (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments marking where each external library is used.","question_index":68,"modules":["fastapi","sqlmodel","databases","authlib","aiocache","arq","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"Scenario: Build an end-to-end Python web service that accepts data submissions, runs ML inference asynchronously, and pushes results back in real time.\n\nFunctional Requirements:\n1. Implement a POST \/submit endpoint using the latest FastAPI (ASGI, HTTP\/2) to accept JSON payloads.\n2. Validate and persist submissions into PostgreSQL via the latest SQLModel with asynchronous sessions.\n3. Dispatch an asynchronous classification task via the latest Celery (Redis broker) when new data arrives.\n4. In the Celery task, perform inference using the latest Hugging Face Transformers pipeline (e.g., text-classification).\n5. Cache inference results in Redis using the latest aioredis with a 5-minute TTL.\n6. Expose a WebSocket \/ws endpoint (FastAPI) to stream cached or newly computed results to subscribed clients.\n7. Instrument all HTTP requests and Celery tasks with the latest OpenTelemetry SDK for tracing.\n\nDeliverables:\n\u2022 requirements.txt listing \u201cthe latest\u201d versions of FastAPI, SQLModel, Celery, Redis, aioredis, Transformers, and OpenTelemetry as of 2025-05-06.  \n\u2022 A single Python 3.11+ file under 150 lines of runnable code. Include clear comments where each external library is used.","question_index":59,"modules":["fastapi","sqlmodel","SQLAlchemy","asyncpg","celery","redis","transformers","torch","aioredis","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-celery","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":14}
{"model":"gpt-4.1-mini","question":"Scenario: You are tasked with building a real-time IoT analytics dashboard featuring anomaly detection, edge caching, vector search, and HTTP\/3 streaming.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of May 6, 2025) with HTTP\/3 and ASGI concurrency for API endpoints.\n2. Ingest IoT telemetry via real-time WebSocket streams using the latest websockets library.\n3. Perform transformer-based anomaly detection using the latest HuggingFace transformers and PyTorch 2.x.\n4. Store and query time-series embeddings in a vector database using the latest Pinecone Python client with HNSW indexing.\n5. Cache recent query results at the edge using the latest aioredis client with TTL invalidation.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the \u201clatest\u201d versions of all third-party libraries as of today.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":67,"modules":["fastapi","torch","transformers","pinecone-client","aioredis","websockets","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"Scenario: Build a high-performance, AI-powered web dashboard for real-time analytics and user collaboration.\n\nFunctional Requirements:\n1. Web API: Use the latest FastAPI (as of 2025-05-06) with async endpoints and the new dependency-injection system.\n2. Real-Time Collaboration: Implement bidirectional WebSocket chat using python-socketio v5.x+ for live user updates.\n3. AI Integration: Leverage the latest llama-cpp-python (or equivalent) to compute embeddings on incoming messages.\n4. Database Persistence: Store users and message history asynchronously with SQLModel v0.7+ (Pydantic v2) on SQLite.\n5. Caching Layer: Cache AI embedding results in Redis via aioredis v3.x+ to minimize redundant computation.\n6. Security: Protect all endpoints with OAuth2 JWT authentication using Authlib v1.2+.\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all third-party libraries as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, with clear inline comments indicating where each external library is used.","question_index":58,"modules":["fastapi","sqlmodel","pydantic","authlib","jose","uvicorn","socketio","aioredis","llama_cpp","sqlalchemy"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":10}
{"model":"gpt-4.1-mini","question":"You are a senior software engineer tasked with building a modern image-processing web service end-to-end.\n\nFunctional Requirements:\n1. Implement a REST endpoint in Litestar for image uploads that converts incoming images to AVIF using the latest pillow-avif-plugin.\n2. Asynchronously persist image metadata (filename, upload timestamp, AVIF URL, dimensions) in MongoDB via the latest Beanie ODM.\n3. Index and cache metadata in Redis for fast lookup using the latest Redis-OM Python library.\n4. Expose a GraphQL API via the latest Litestar-GraphQL plugin to query images with filtering, pagination, and type validation.\n5. Broadcast real-time upload notifications over WebSocket connections using Litestar\u2019s built-in WebSocket support.\n\nDeliverables:\n- A concise list of the five functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines, using the latest Litestar, Litestar-GraphQL, Beanie, Redis-OM, and pillow-avif-plugin (as of May 6, 2025), with clear comments indicating where each external library is used.","question_index":89,"modules":["Pillow","litestar","strawberry-graphql","beanie","motor","redis-om","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"You are a Senior Software Engineer building a high-throughput, real-time web application that ingests user events, enriches them with AI, persists outcomes, and serves analytics via a modern dashboard.\n\nFunctional Requirements:\n1. Implement JWT-based user authentication using the latest async web framework with schema-first validation (e.g., HTTP\/3 support).\n2. Expose a WebSocket endpoint for real-time event ingestion using the latest WebSocket library with per-message compression.\n3. Perform fully async CRUD on PostgreSQL using the latest ORM with native asyncio support and advanced connection pooling.\n4. Offload heavy computations to background workers via the latest task queue library supporting asyncio and result backends.\n5. Integrate an AI text-classification pipeline using the latest AI inference library with streaming outputs.\n6. Instrument the service with distributed tracing using the latest observability library supporting OpenTelemetry protocols.\n\nDeliverables:\n\u2022 requirements.txt listing the latest available versions of all dependencies (as of May 6, 2025).  \n\u2022 A single Python 3.11+ file under 150 lines that is complete and runnable, with clear comments marking where each external library is used.","question_index":12,"modules":["fastapi","jwt","sqlalchemy","celery","transformers","opentelemetry-instrumentation-fastapi","opentelemetry-sdk","opentelemetry-api","opentelemetry-exporter-otlp","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1-mini","question":"Scenario: Develop a high-performance asynchronous web service for ingesting user datasets, persisting metadata, launching background processing jobs, and tracing all operations end-to-end.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/upload endpoint using Starlite (latest) with async file streaming and dependency injection.\n2. Define and migrate a PostgreSQL table via Piccolo ORM (latest), storing upload metadata and task IDs, leveraging its GraphQL auto-schema feature.\n3. Enqueue and process tasks with Asynq (latest) in Redis, using priority queues, retry policies, and task result callbacks.\n4. Instrument incoming HTTP requests and background tasks with OpenTelemetry (latest), enabling W3C trace-context propagation across components.\n5. Provide an HTTP GET \/status\/{task_id} endpoint that queries Piccolo ORM for real-time task status.\n\nDeliverables:\n\u2022 The numbered functional requirements list (as above)  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, using the latest versions of all libraries as of 2025-05-06, with clear comments indicating where each external library is used.","question_index":43,"modules":["starlite","piccolo","asynq","opentelemetry-sdk","opentelemetry-instrumentation-starlite","opentelemetry-instrumentation-asyncpg","uvicorn"],"modulenotfound":["opentelemetry-instrumentation-starlite"],"modulenotfound_count":1,"module_count":7}
{"model":"gpt-4.1-mini","question":"As a senior software engineer, you are tasked with building a secure, real-time collaborative whiteboard microservice using cutting-edge Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3 API using the latest FastAPI (as of 2025-05-06) and uvicorn[standard] for low-latency streaming.\n2. Implement OAuth2 authorization with JWTs and refresh-token rotation using the latest Authlib.\n3. Persist whiteboard sessions and user metadata in PostgreSQL via the latest SQLModel with an async engine.\n4. Broadcast drawing events in real time over WebSockets using the latest websockets library with room-based pub\/sub.\n5. Vectorize stroke data into SVG on the fly using the latest Shapely or pyvips for advanced geometry operations.\n6. Upload periodic session snapshots to AWS S3 using the latest aioboto3 with asynchronous multipart uploads.\n\nDeliverables:\n- A requirements.txt listing the latest available versions of all external libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, fully runnable, with clear comments indicating where each third-party library is used.","question_index":25,"modules":["fastapi","jose","authlib","sqlmodel","asyncpg","shapely","aioboto3","websockets","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1-mini","question":"Scenario: Build a real-time sentiment analysis dashboard that fetches trending Twitter topics, analyzes sentiment, stores results, and serves updates via WebSockets.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to create both REST and WebSocket endpoints, leveraging its built-in asyncio and WebSocket broadcast features.\n2. Use the latest HTTPX to asynchronously fetch trending topics from the Twitter API v2 over HTTP\/3.\n3. Use the latest Hugging Face transformers to perform sentiment analysis via a pretrained pipeline.\n4. Use the latest SQLModel to define async SQLite models and persist topics with sentiment scores.\n5. Use the latest Jinja2 to render a minimal HTML dashboard for initial page load before WebSocket updates.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the latest versions of FastAPI, HTTPX, transformers, SQLModel, and Jinja2 as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":36,"modules":["fastapi","sqlmodel","sqlalchemy","httpx","transformers","jinja2","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"Senior Software Engineer: You are developing an intelligent IoT monitoring dashboard that integrates real-time ingestion, streaming anomaly detection, WebSocket updates, dynamic UI rendering, and cloud infrastructure provisioning.\n\nFunctional Requirements:\n1. Build an ASGI web server using the latest ASGI framework (as of 2025-05-06) with HTTP\/2 and WebSocket endpoints for real-time data streaming.\n2. Connect asynchronously to an MQTT broker using the latest Python MQTT client library (as of 2025-05-06), subscribe to sensor topics, and relay messages to the server.\n3. Implement real-time anomaly detection on incoming sensor data using the latest Python anomaly detection library supporting incremental or deep learning.\n4. Serve a live, interactive front-end auto-generated from Python to JavaScript using the latest Python-to-JS UI framework.\n5. Provision and deploy all components via infrastructure-as-code using the latest Python IaC SDK on a major cloud provider.\n\nDeliverables:\n\u2022 Restate the numbered functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code integrating all five requirements, under 150 lines.  \n\u2022 Clear inline comments indicating where and how each external \u201clatest\u201d library (as of 2025-05-06) is imported and used.","question_index":49,"modules":["starlite","asyncio-mqtt","anomalib","pynecone","pulumi","pulumi-aws","hypercorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"Scenario: Build a real-time collaborative note-taking web application with authentication, persistence, caching, live updates, and scheduled cleanup.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (as of today) for note creation, retrieval, update, and deletion.\n2. Secure endpoints with JWT authentication using the latest PyJWT.\n3. Persist notes in a relational database via the latest SQLModel ORM, defining Note and User models.\n4. Enable real-time note synchronization between clients using WebSockets with the latest python-socketio.\n5. Cache frequently accessed note metadata in Redis using the latest redis-om to reduce DB load.\n6. Schedule automatic archiving of notes older than 30 days using the latest APScheduler.\n7. Document all endpoints with OpenAPI and include example request\/response schemas.\n\nDeliverables:\n- A numbered list of all third-party libraries (with versions) you\u2019ve chosen.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear comments indicating where each external library is used.","question_index":69,"modules":["jwt","fastapi","pydantic","sqlmodel","socketio","redis_om","apscheduler","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-mini","question":"Scenario: Build a high-performance asynchronous message processing service for real-time sentiment analysis, storage, notification, and search.\n\nFunctional Requirements:\n1. Implement an async HTTP API using the latest FastAPI (as of May 6, 2025) to accept user messages.\n2. Define and persist message models in PostgreSQL via the latest SQLModel ORM.\n3. Schedule and execute sentiment analysis in the background using the latest Dramatiq with RabbitMQ.\n4. After analysis, update the database and push real-time notifications to connected clients over WebSockets.\n5. Index messages and sentiment scores into Elasticsearch using the latest official Python client.\n6. Provide a search endpoint to query stored messages by keyword and sentiment.\n\nDeliverables:\n\u2022 A numbered list of the \u201clatest\u201d third-party libraries (with versions) used for each sub-domain.  \n\u2022 Complete, runnable Python 3.11+ application code under 150 lines, with clear comments indicating where each external library is used.","question_index":48,"modules":["fastapi","sqlmodel","asyncpg","dramatiq","elasticsearch","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-mini","question":"Scenario: Build a microservice that accepts text input, asynchronously generates summaries, persists data, and provides real-time metrics.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI 3, HTTP\/3 support) to implement POST \/summaries accepting JSON { \"text\": \"\u2026\" }, enqueue a background job via Arq, and return a job ID.\n2. Persist requests and summaries in SQLite asynchronously with the latest SQLModel (Pydantic V2 model support).\n3. Offload summarization jobs to Redis using the latest Arq, invoking the latest LangChain to call OpenAI for a concise summary.\n4. Expose GET \/summaries\/{id} to retrieve stored summaries from the database.\n5. Instrument request counts and job durations with the latest prometheus_client, exposing metrics on GET \/metrics.\n\nDeliverables:\n- A bulleted list of chosen \u201clatest\u201d libraries with exact version numbers.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear inline comments indicating where each external library is used.\n- Self-contained implementation: after installing dependencies, code runs without further setup.","question_index":95,"modules":["starlite","sqlmodel","sqlalchemy","arq","langchain","openai","prometheus_client","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-mini","question":"Scenario: Create a serverless real-time analytics API that ingests WebSocket events, performs on-the-fly NLP summarization, indexes embeddings, exposes a GraphQL endpoint, and deploys to AWS Lambda.\n\nFunctional Requirements:\n1. Use the latest Litestar (as of 2025-05-06) to build an HTTP\/2 GraphQL endpoint with schema-based typing.\n2. Implement a real-time WebSocket consumer using wsproto or litestar-websockets for event ingestion.\n3. Offload text summarization to an asynchronous background task leveraging the latest Transformers library.\n4. Generate embeddings with the newest OpenAI Embeddings API or sentence-transformers and store them in the latest ChromaDB vector store.\n5. Secure all endpoints with passwordless authentication via the latest fastapi-users or equivalent.\n6. Package and deploy the entire application to AWS Lambda using the latest Mangum adapter.\n\nDeliverables:\n- A numbered list of chosen \u201clatest\u201d library versions (as of 2025-05-06) mapping to each requirement.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":77,"modules":["litestar","litestar-websockets","transformers","openai","chromadb","fastapi-users","mangum","litestar-graphql","strawberry"],"modulenotfound":["litestar-websockets","litestar-graphql"],"modulenotfound_count":2,"module_count":9}
{"model":"gpt-4.1-mini","question":"Scenario: Develop a modern, high-performance AI-powered web service for real-time text processing.\n\nFunctional Requirements:\n1. Build a JSON REST API with Litestar using its latest dependency-injection and ASGI lifespan hooks.\n2. Integrate Tortoise ORM (async) to manage a PostgreSQL \u201cjobs\u201d table with migrations.\n3. Use llama-cpp-python to generate text embeddings or responses via a local LLaMA model.\n4. Expose a WebSocket endpoint with python-socketio (asyncio mode) for live client notifications.\n5. Cache recent embeddings in Redis using redis-py\u2019s asyncio client.\n6. Schedule background tasks (e.g., stale job cleanup) leveraging asyncio\u2019s latest task groups.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":60,"modules":["litestar","tortoise-orm","python-socketio","redis","llama-cpp-python","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-mini","question":"Scenario: You are a senior software engineer building a real-time social media monitoring application that ingests tweets, analyzes sentiment, classifies images, displays live charts, and deploys as a serverless microservice using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Ingest tweets in real time from Twitter API v2 using the latest HTTPX async streaming client.\n2. Perform NLP sentiment analysis on tweet text with the latest Hugging Face Transformers pipeline.\n3. Classify embedded images using the latest Ultralytics YOLOv8 model.\n4. Render an interactive, live-updating dashboard using the latest Plotly Dash framework.\n5. Scaffold serverless deployment with the latest AWS Lambda Powertools for Python.\n\nDeliverables:\n- A requirements list of all external dependencies.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":98,"modules":["httpx","Pillow","dash","plotly","transformers","ultralytics","aws-lambda-powertools"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"Scenario: A knowledge management platform must allow users to upload PDFs, index their content, and perform real-time similarity searches via a high-performance API.\n\nFunctional Requirements:\n1. Implement a PDF upload REST endpoint using the latest unstructured-sdk to extract text.\n2. Use the latest ChromaDB Python client to generate and store embeddings for each document.\n3. Provide a search REST endpoint that queries ChromaDB for top-k similar documents given a text query.\n4. Create a real-time WebSocket over HTTP\/3 endpoint using the latest aioquic to stream incremental search results.\n5. Assemble the application as an asynchronous service using the latest Starlite web framework.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":6,"modules":["numpy","starlite","unstructured","chromadb","aioquic","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-mini","question":"Scenario: As a senior software engineer, build an asyncio-based Python microservice showcasing HTTP\/3 REST, GraphQL, async ORM, real-time WebSockets, and scheduled tasks using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST endpoint at `\/api\/items` using FastAPI (latest) and Hypercorn (latest) with HTTP\/3 support.\n2. Expose a GraphQL API at `\/graphql` using Strawberry (latest) with asynchronous resolvers fetching from the database.\n3. Integrate PostgreSQL via Tortoise ORM (latest), define an `Item` model, and apply programmatic migrations on startup.\n4. Provide WebSocket notifications at `\/ws\/updates` using python-socketio (latest), broadcasting item creation and deletion events.\n5. Schedule an hourly cleanup job with APScheduler (latest) in asyncio mode to remove `Item` records older than 7 days.\n\nDeliverables:\n- A `requirements.txt` listing the latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is imported and utilized.","question_index":66,"modules":["fastapi","strawberry-graphql","tortoise-orm","python-socketio","apscheduler","uvloop","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"Scenario: Develop a real-time AI-driven sentiment analysis web service that ingests text, processes it via an on-device LLM, persists results, and streams live analytics to clients.\n\nFunctional Requirements:\n1. Expose RESTful and WebSocket endpoints using the latest Litestar Python framework (released within the last 12 months).\n2. Perform local LLM inference for sentiment analysis with the latest llama-cpp-python library (released within the last 12 months).\n3. Asynchronously persist raw text and sentiment scores in MongoDB using the latest async MongoDB driver (released within the last 12 months) with code-first schema.\n4. Offload sentiment analysis to a distributed background worker using the latest Dramatiq (or equivalent) async task queue released in the last 12 months.\n5. Broadcast real-time sentiment updates to connected clients over WebSockets.\n6. Render an interactive dashboard in server-side templates with the latest Plotly Python library (released within the last 12 months).\n7. Instrument the entire application with distributed tracing via the latest OpenTelemetry Python SDK (released within the last 12 months).\n\nDeliverables:\n- A requirements list specifying each third-party library and its exact latest version as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines total.\n- Clear inline comments indicating where and how each external library is used.","question_index":5,"modules":["litestar","llama-cpp-python","motor","dramatiq","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation","plotly","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1-mini","question":"You are a senior software engineer tasked with building a real-time analytics dashboard with AI-driven anomaly alerts using the latest Python libraries as of 2025-05-06.  \n\nFunctional Requirements:  \n1. Implement asynchronous REST API endpoints using the latest FastAPI.  \n2. Add WebSocket-based real-time alert streaming using the latest websockets.  \n3. Persist incoming metrics in PostgreSQL via async ORM operations using the latest SQLModel.  \n4. Cache expensive queries in Redis for high throughput using the latest aioredis.  \n5. Perform streaming anomaly detection on incoming metrics using the latest river library.  \n6. Enable file uploads of detailed reports to S3-compatible storage via the latest minio SDK.  \n\nDeliverables:  \n\u2022 A numbered list of required Python libraries (with versions as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":20,"modules":["fastapi","sqlmodel","asyncpg","aioredis","minio","river","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"Build a real-time analytics microservice that lets authenticated users ingest sales data, stores it in PostgreSQL, streams live updates to connected clients, and generates PDF reports with embedded charts.\n\n1. Implement OAuth2 JWT authentication using the latest fastapi-jwt-auth as of May 6, 2025.  \n2. Create async CRUD endpoints for sales records with SQLModel and asyncpg.  \n3. Broadcast new sales events over WebSocket using python-socketio (latest).  \n4. Generate interactive charts in HTML via Plotly (latest) and convert them to PDF using WeasyPrint (latest).  \n5. Expose generated PDFs via an endpoint, serving files asynchronously with aiofiles.  \n\nDeliverables:  \n- A numbered confirmation of the functional requirements above.  \n- A single Python 3.11+ file under 150 lines\u2014complete, runnable code\u2014with clear comments indicating where each external library is used. Use the latest versions of all third-party libraries as of today\u2019s date.","question_index":26,"modules":["fastapi-jwt-auth","sqlmodel","sqlalchemy","socketio","plotly","weasyprint","aiofiles","fastapi","pydantic","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1-mini","question":"Scenario: You are tasked with building a real-time collaborative task management web service for enterprise teams.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to expose HTTP endpoints and WebSocket routes, leveraging its new dependency injection and WebSocket lifetime management.\n2. Configure an async PostgreSQL database using the latest Tortoise-ORM with its auto-migration CLI and Pydantic model integration.\n3. Broadcast real-time task updates to connected clients via the latest python-socketio in async mode.\n4. Implement OAuth2 authentication with JWT access and refresh tokens using the latest Authlib, including token rotation.\n5. Cache sessions and publish update events using the latest aioredis async Redis client.\n6. Schedule and execute background jobs (e.g., daily summaries) with the latest APScheduler async support.\n7. Instrument request tracing and logging using the latest OpenTelemetry Python SDK, exporting to console.\n8. Auto-generate interactive API docs using FastAPI\u2019s built-in Swagger UI and Redoc.\n\nDeliverables:\n\u2022 A bullet list of selected libraries (with version numbers).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":15,"modules":["fastapi","tortoise-orm","python-socketio","authlib","aioredis","apscheduler","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1-mini","question":"Scenario: You are building a high-performance Python backend that ingests user articles, generates AI-powered summaries, stores and caches results, and exposes them via REST, GraphQL, and real-time channels with background processing.\n\nFunctional Requirements:\n1. Accept article submissions over a REST API using FastAPI and validate payloads with Pydantic v2.  \n2. Offload summarization to a background worker with Dramatiq using Redis as a broker.  \n3. Summarize articles via the latest OpenAI Python SDK and cache summaries with aiocache.  \n4. Persist articles and summaries in SQLite using SQLModel\u2019s async ORM features.  \n5. Expose stored summaries through a GraphQL endpoint implemented with Strawberry GraphQL.  \n6. Broadcast new summary notifications to connected clients over WebSockets with python-socketio.  \n\nDeliverables:\n\u2022 List of the latest third-party libraries (as of today) and their install commands.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":63,"modules":["fastapi","pydantic","dramatiq","redis","openai","aiocache","sqlmodel","aiosqlite","strawberry-graphql","python-socketio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1-mini","question":"Scenario: Develop an internal Python microservice where authenticated users can submit text to be summarized by an LLM and receive real-time notifications via WebSockets.\n\nFunctional Requirements:\n1. Implement an asynchronous REST API using the latest Starlite framework (2025-05-06) leveraging its dependency injection and lifespan event system.\n2. Integrate OAuth2 Authorization Code Flow for user authentication with Authlib\u2019s async client libraries released in the last 12 months.\n3. Persist users, submissions, and summaries in PostgreSQL using the latest Piccolo ORM with async schema migrations and connection pooling.\n4. Summarize submitted text by calling OpenAI\u2019s GPT-4 Turbo via the newest openai Python SDK and its function-calling feature.\n5. Broadcast summary completion events over WebSockets using a modern Python websockets library (with HTTP\/2 support) released in the past year.\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":82,"modules":["starlite","piccolo","piccolo_migrations","piccolo_api","authlib","openai","websockets"],"modulenotfound":["piccolo_migrations"],"modulenotfound_count":1,"module_count":7}
{"model":"gpt-4.1-mini","question":"You are tasked with building a real-time analytics web dashboard that ingests streaming data, processes it, summarizes it with AI, secures user access, and visualizes results dynamically using the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Implement a REST API in Python 3.11+ with FastAPI (HTTP\/3 support via Hypercorn) to accept JSON event streams.\n2. Broadcast processed insights in real time over WebSocket using python-socketio.\n3. Buffer incoming events and generate streaming AI summaries using the latest LangChain and OpenAI Python SDK.\n4. Perform real-time aggregation and lazy analytics on events with Polars.\n5. Produce dynamic Plotly charts of key metrics and serve them as JSON for front-end rendering.\n6. Secure all endpoints and WebSocket connections with JWT-based auth via fastapi-users.\n\nDeliverables:\n\u2022 A numbered list of the above functional requirements.  \n\u2022 A single Python 3.11+ file (under 150 lines) that fulfills all requirements, complete and runnable. Include clear comments indicating where and how each external library is used.","question_index":84,"modules":["fastapi","fastapi-users","python-socketio","polars","plotly","langchain","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"Scenario: You are a senior software engineer tasked with building a next-generation real-time collaborative data analytics dashboard. Use the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a FastAPI server with a WebSocket endpoint for live data updates using the latest fastapi[full].\n2. Expose a type-safe GraphQL API at `\/graphql` using Strawberry with async schema stitching.\n3. Integrate AI-driven insights by generating embeddings on incoming data with llama-cpp-python.\n4. Perform vector similarity search over embeddings via the Weaviate Python client\u2019s async API.\n5. Secure all endpoints with passwordless authentication using py_webauthn for WebAuthn flows.\n6. Containerize the application using docker-py to build images and dynamically generate a docker-compose config.\n7. Instrument the app with async Prometheus metrics using prometheus-client.\n\nDeliverables:\n- A list of required libraries (with versions) using the latest releases as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":72,"modules":["fastapi","prometheus-client","strawberry-graphql","llama-cpp-python","weaviate-client","py_webauthn","docker","uvicorn","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1-mini","question":"Scenario: Develop a real-time collaborative note-taking web application with AI-powered summarization.\n\nFunctional Requirements:\n1. Implement user authentication via OAuth2 password flow using the latest FastAPI features for secure login.  \n2. Expose a WebSocket endpoint for real-time CRDT-based document state synchronization using the newest FastAPI WebSocket enhancements.  \n3. Persist users and documents in PostgreSQL via the latest Piccolo ORM async API with JSONB indexing for rich querying.  \n4. Cache live presence and session metadata in Redis using aioredis with RedisJSON support for low-latency reads\/writes.  \n5. Summarize document edits on demand using the latest LangChain library and ChatOpenAI (GPT-4) integration.  \n6. Serve a minimal HTML frontend via Jinja2 templates that connects to the WebSocket and displays live updates.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of each dependency as of 2025-05-06.  \n\u2022 main.py: Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":52,"modules":["fastapi","jose","passlib","piccolo","aioredis","langchain"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-mini","question":"Scenario: Senior Software Engineer, you are tasked with building a prototype real-time collaborative note-taking service using only the latest Python libraries released in the last 12 months.\n\nFunctional Requirements:\n1. Develop an asynchronous REST API for note CRUD operations using the latest Litestar framework.\n2. Persist notes in a SQL database via a newly released async ORM library.\n3. Cache frequent note queries in Redis with a recently published async Redis client.\n4. Enable real-time collaboration via WebSockets using a modern Python WebSocket library launched within the last year.\n5. Expose an AI-powered summarization endpoint using the latest OpenAI Python client.\n6. Render an interactive HTML dashboard showing active collaborators with a new Python-to-JS visualization tool.\n7. Instrument distributed tracing and structured logging using a cutting-edge observability library released in the past 12 months.\n\nDeliverables:\n- requirements.txt listing only libraries created in the last 12 months with exact versions.\n- A single Python 3.11+ file under 150 lines containing the complete, runnable application, with clear comments indicating where each external library is used.","question_index":41,"modules":["litestar","sqlmodel","databases","redis","wsproto","openai","lightstep","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-mini","question":"Scenario:\nCreate an AI-powered article summarization service that accepts user-submitted text, stores it, generates summaries asynchronously via an AI API, caches results, and streams them back to clients in real time.\n\nFunctional Requirements:\n1. Implement a REST API using FastAPI (latest) with endpoints POST \/articles to submit text and GET \/stream\/{article_id} for server-sent events.\n2. Persist submissions in a database using SQLModel (latest) with an async SQLite or PostgreSQL engine.\n3. Orchestrate a background job queue using Celery (latest) with Redis as broker to process newly submitted articles.\n4. Perform summarization using the OpenAI Python SDK (latest) and its chat completion endpoint.\n5. Cache generated summaries in Redis via aioredis (latest) with a configurable TTL.\n6. Stream summaries to connected clients in real time using server-sent events via sse-starlette (latest).\n\nDeliverables:\n\u2022 A list of all third-party libraries (with versions pinned to the latest as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":64,"modules":["fastapi","sqlmodel","asyncpg","celery","redis","openai","sse-starlette","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-mini","question":"Scenario: Build a lightweight feedback API that records user comments, analyzes sentiment in real time, stores results, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (as of 2025-05-06) to implement asynchronous REST endpoints for submitting feedback and querying stored analytics.\n2. Use the latest SQLModel to define async ORM models and persist incoming feedback in an SQLite database.\n3. Use the latest PyWebSocketX to broadcast each new sentiment result over an ASGI WebSocket endpoint.\n4. Use the latest VulcanMind inference library to perform on-the-fly sentiment analysis of submitted feedback.\n5. Implement structured logging and error handling throughout the request, inference, database, and WebSocket flows.\n\nDeliverables:\n- A requirements list with exact version pins for Starlite, SQLModel, PyWebSocketX, and VulcanMind (latest as of 2025-05-06).\n- Complete, runnable Python 3.11+ code (\u2264150 lines) that integrates all four libraries with clear comments marking where each is used.","question_index":83,"modules":["starlite","sqlmodel","sqlalchemy","pywebsocketx","vulcanmind","uvicorn","aiosqlite"],"modulenotfound":["pywebsocketx","vulcanmind"],"modulenotfound_count":2,"module_count":7}
{"model":"gpt-4.1-mini","question":"You are a senior software engineer building a high-performance, real-time analytics web service for IoT sensor data.  \n\nFunctional Requirements:  \n1. Expose HTTP REST ingest endpoints using the latest Nebula framework (released 2024-06) with its async dependency-injection.  \n2. Persist incoming readings to PostgreSQL via StarDB ORM (v1.0+, released 2024-08), leveraging async transactions and auto-migration.  \n3. Secure all endpoints with OAuth2 PKCE flows using AuthFusion (released 2024-12).  \n4. Broadcast processed data updates to dashboard clients via WaveSocket (released 2024-10) server-clustering.  \n5. Schedule hourly aggregation jobs using ChronoTask (released 2024-11) cron-style scheduler.  \n\nDeliverables:  \n\u2022 A requirements.txt listing the latest versions of all libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":92,"modules":["nebula","stardb","authfusion","wavesocket","chronotask","asyncpg","uvicorn","psycopg","python-dotenv"],"modulenotfound":["nebula","stardb","authfusion","wavesocket"],"modulenotfound_count":4,"module_count":9}
{"model":"gpt-4.1-mini","question":"Scenario: You are a senior software engineer building a high-throughput chat summarization microservice in Python 3.11+ using only the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Real-time WebSocket ingestion: implement a `\/ws` endpoint using fastapi-socketio (latest) with session storage in Redis via `redis.asyncio`.\n2. AI summarization: enqueue incoming messages in Arq (latest) and process them with the OpenAI Python SDK (latest) calling GPT-4 Turbo for summaries.\n3. Async data persistence: store raw messages and summaries in PostgreSQL using SQLModel + encode\/databases with the asyncpg driver.\n4. Dynamic admin dashboard: expose `\/admin` rendering Jinja2 templates enhanced with HTMX via fastapi-htmx (latest).\n5. Email alerts: send summary notifications asynchronously using FastAPI-Mail (latest).\n6. Containerization: provide a multi-stage Dockerfile based on `python:3.11-slim`.\n\nDeliverables:\n- A `requirements.txt` listing the exact versions of all third-party libraries (as of May 6, 2025).\n- A single `main.py` (under 150 lines) containing complete, runnable Python 3.11+ code with clear comments indicating where and how each external library is used.","question_index":23,"modules":["fastapi","fastapi-socketio","redis","arq","openai","sqlmodel","databases","fastapi-htmx","fastapi-mail","uvicorn","jinja2"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"gpt-4.1-mini","question":"Scenario: Build a compact Python microservice that ingests text and images, leverages AI summarization, real-time streams results, and stores everything in a modern database.\n\nFunctional Requirements:\n1. Use the latest Litestar (async web framework released Dec 2023) to define:\n   a. POST \/articles to receive JSON {\u201ctitle\u201d:\u2026, \u201ccontent\u201d:\u2026}.\n   b. WebSocket endpoint \/ws\/summary for streaming summaries.\n2. Connect asynchronously to PostgreSQL via the latest Prisma Python client (released Jul 2023), defining models for Article and Image.\n3. Summarize incoming content using the latest llama-cpp-python (released Sep 2023), utilizing its streaming interface for token-by-token output.\n4. Implement \/upload to accept multipart\/form-data image files, generate HEIF thumbnails with the latest pillow-heif (released Oct 2023), and persist file paths.\n5. Ensure all endpoints use async\/await, proper error handling, and pydantic-based validation.\n\nDeliverables:\n\u2013 A requirements.txt listing the latest library versions as of 2025-05-06.\n\u2013 A single, runnable Python 3.11+ script under 150 lines, with clear comments indicating where each external library is used.","question_index":16,"modules":["litestar","pydantic","prisma","llama_cpp","pillow_heif","Pillow","uvicorn"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":7}
{"model":"gpt-4.1-mini","question":"You are tasked with building a prototype web service for authenticated semantic search with real-time visual feedback.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) server with:\n   a. an OAuth2 login flow using PyOIDC (latest) against an external Identity Provider  \n   b. a POST \/query endpoint to accept natural-language queries  \n   c. a WebSocket \/ws endpoint to stream real-time results  \n2. Upon authentication, embed incoming queries using sentence-transformers (latest) and perform vector similarity search in ChromaDB (latest).  \n3. As results arrive, push them incrementally over the WebSocket to clients.  \n4. Generate a Plotly (latest) bar chart of top-k result scores and serve it as an HTML snippet.  \n5. Ensure all components run under Python 3.11+, integrate only \u201cthe latest\u201d library releases as of 2025-05-06, and keep implementation concise.  \n\nDeliverables:\n\u2022 A list of chosen libraries with exact versions (the \u201crequirements\u201d).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":42,"modules":["fastapi","pyoidc","sentence_transformers","chromadb","plotly","uvicorn"],"modulenotfound":["pyoidc"],"modulenotfound_count":1,"module_count":6}
{"model":"gpt-4.1-mini","question":"Scenario: You are building a high-throughput, real-time analytics web application for processing IoT sensor streams using the latest Python async frameworks and libraries.\n\nFunctional Requirements:\n1. Ingest JSON sensor events via a REST API built on FastAPI (use its latest HTTP\/3 support).\n2. Broadcast processed metrics to connected clients over WebSockets using python-socketio (asyncio).\n3. Offload CPU-intensive aggregation to background workers with Arq (latest Redis-backed queue).\n4. Persist time-series summaries in PostgreSQL via SQLModel (Pydantic v2-powered ORM).\n5. Cache hot query results in Redis using aioredis for sub-millisecond lookups.\n6. Expose tracing and Prometheus metrics using OpenTelemetry and prometheus-client.\n\nDeliverables:\n- A bullet list of the latest library dependencies as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments indicating where each external library is used.","question_index":40,"modules":["fastapi","python-socketio","uvloop","arq","sqlmodel","aioredis","prometheus-client","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","hypercorn","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"gpt-4.1-mini","question":"You are a senior software engineer tasked with building a Python web application that integrates user authentication, real-time notifications, AI-based recommendations, and payment processing.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (2025-05-06) and Python 3.11 features (e.g. match-case).\n2. Secure all endpoints with OAuth2 JWT authentication via the latest fastapi-users, including email verification flows.\n3. Provide real-time notifications over WebSockets using the latest python-socketio async server.\n4. Handle one-time charges using the latest Stripe Python SDK via the Payment Intents API.\n5. Generate AI-driven product recommendations by invoking gpt-4-turbo through the latest openai library.\n6. Emit structured, context-rich logs for each operation using the latest structlog.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06  \n\u2022 Complete, runnable Python 3.11+ code (under 150 lines) with clear comments marking where each external library is used","question_index":96,"modules":["fastapi","fastapi-users","pydantic","sqlalchemy","structlog","python-socketio","stripe","openai"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-mini","question":"Scenario: As a senior software engineer, design a high-performance microservice for real-time document collaboration with secure access, persistence, and cloud integration.  \n\nFunctional Requirements:  \n1. Implement an asynchronous JSON REST API using the latest Python web framework available as of 2025-05-06, supporting pagination and input validation.  \n2. Secure all endpoints with OAuth2 PKCE and JWT using the latest authentication library (released within the last 12 months).  \n3. Enable bidirectional real-time collaboration events over WebSocket using the latest WebSocket library (released within the last 12 months).  \n4. Persist document data in an async NoSQL database (e.g., MongoDB) using the newest driver supporting transactions and change streams.  \n5. Upload and serve document attachments to an S3-compatible object storage using the latest SDK with multipart upload support.  \n6. Cache frequently accessed documents in Redis with TTL and automatic invalidation using the newest Redis client library.  \n\nDeliverables:  \n- A requirements.txt listing \u201cthe latest\u201d versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":80,"modules":["fastapi","pydantic","motor","redis","minio","python-jose","httpx-oauth","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-mini","question":"Scenario: Build a high-performance Python microservice that combines cutting-edge web, real-time, AI and observability features into a single ASGI application.\n\nFunctional Requirements:\n1. HTTP\/3 ASGI server with automatic TLS and QUIC support using the latest HTTP\/3 library released within the past 12 months.  \n2. Real-time WebSocket endpoint that streams JSON events with zero-copy backpressure control using the newest streaming library created in the last year.  \n3. Semantic search pipeline: accept text uploads, generate embeddings, and perform vector similarity queries via the latest vector-store client released within 12 months.  \n4. On-device LLM inference endpoint that consumes streamed text segments and returns summaries using the most recent quantized model library published this year.  \n5. Distributed tracing and metrics export using the newest OpenTelemetry OTLP exporter released within the past 12 months.\n\nDeliverables:\n- A requirements list (requirements.txt) specifying the exact, latest versions of each third-party library as of 2025-05-06.  \n- A single Python 3.11+ script under 150 lines that implements all features above, with clear comments marking where each external library is used.","question_index":51,"modules":["fastapi","starlette","opentelemetry-api","opentelemetry-sdk","opentelemetry-exporter-otlp","opentelemetry-instrumentation-asgi","hypercorn","httpx","text-generation"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1-mini","question":"Scenario  \nDevelop an end-to-end Python web service that provides passwordless WebAuthn authentication, stores and summarizes text inputs via OpenAI, broadcasts real-time summary events, and serves interactive charts.\n\nFunctional Requirements  \n1. Implement passwordless registration and login using the latest \u201cwebauthn\u201d library (FIDO2) with Litestar middleware.  \n2. Create REST endpoints under Litestar (latest version) for submitting text to summarize and retrieving stored summaries.  \n3. Use the latest SQLModel with an async PostgreSQL engine to persist input texts, summaries, timestamps, and user IDs.  \n4. Invoke the latest \u201copenai\u201d Python client to perform GPT-4 Turbo summarization with function-calling.  \n5. Serialize and broadcast new summaries over WebSockets using the latest \u201cmsgspec\u201d for high-performance JSON.  \n6. Serve an HTML page at \u201c\/charts\u201d that embeds an interactive Plotly chart of summary lengths over time.\n\nDeliverables  \n\u2022 A numbered list of all external libraries (with exact latest versions as of 2025-05-06) and their roles.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":13,"modules":["litestar","webauthn","sqlmodel","asyncpg","openai","msgspec","plotly","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-mini","question":"Scenario: As a senior software engineer, you must develop a real-time analytics dashboard backend that requires user authentication, data ingestion, live updates, background enrichment via an external API, and a web-based dashboard.\n\nFunctional Requirements:\n1. Implement user registration and login endpoints using JWT authentication with asymmetric keys via FastAPI-JWT-Auth (latest).\n2. Create a POST \/data endpoint to ingest JSON payloads and store them asynchronously in SQLite using SQLModel (latest).\n3. Broadcast each new record to connected WebSocket clients at \/ws using the latest websockets library, leveraging async broadcast.\n4. Schedule a background job every minute with arq (latest) to enrich stored records by streaming JSON from a third-party API via httpx (latest).\n5. Serve a simple HTML dashboard at GET \/dashboard that opens a WebSocket to \/ws and displays incoming records in real time.\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":75,"modules":["fastapi","fastapi-jwt-auth","sqlmodel","httpx","websockets","arq","uvicorn","sqlalchemy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-mini","question":"Scenario: Design and implement a Python microservice that exposes HTTP endpoints, real-time WebSocket notifications, persistent storage, caching, scheduled tasks, and email alerts using the latest third-party libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use FastAPI (latest) with HTTP\/2 support to expose async REST endpoints for user registration and notification retrieval.\n2. Use SQLModel (latest) as an async ORM for PostgreSQL, defining typed models for users and notifications.\n3. Implement WebSocket broadcasting via FastAPI\u2019s WebSocket support to push new notifications to connected clients.\n4. Leverage Aioredis (latest) Redis Streams to cache session state and stream notification events.\n5. Schedule periodic database cleanup and cache eviction using APScheduler (latest) with its asyncio integration.\n6. Send email alerts for critical notifications using Aiosmtplib (latest) with STARTTLS.\n7. Perform external health-check API calls using HTTPX (latest) with async retries and timeouts.\n\nDeliverables:\n- A requirements list naming each library and its latest version as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, including clear comments that indicate where each external library is used.","question_index":44,"modules":["fastapi","sqlmodel","sqlalchemy","aioredis","apscheduler","aiosmtplib","httpx","tenacity","uvicorn","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1-mini","question":"Scenario: You are tasked as a senior software engineer to build an end-to-end collaborative document editing service with real-time updates, async persistence, GraphQL subscriptions, and cloud file storage.\n\nFunctional Requirements:\n1. Use the latest Litestar (first released within the last 12 months) to create an HTTP\/2 web server with WebSocket endpoints for real-time edit streams.\n2. Implement a GraphQL API with Strawberry GraphQL (latest) that supports queries, mutations, and live subscriptions for document change events.\n3. Persist documents in a PostgreSQL database via a fully async ORM of your choice that was first released in the last 12 months and supports automated migrations.\n4. Generate AWS S3 presigned URLs for resumable file attachments using aiobotocore (latest) with async upload\/download.\n5. Schedule periodic cleanup of stale sessions using an async task scheduler library first released within the last 12 months.\n\nDeliverables:\n- A brief list of the chosen third-party libraries (with exact versions) that meet the \u201cfirst released within the last 12 months\u201d requirement.\n- Complete, runnable Python 3.11+ code (under 150 lines) fulfilling the above requirements, with clear comments indicating where and how each external library is used.","question_index":78,"modules":["litestar","strawberry-graphql","piccolo","aiobotocore","async-scheduler"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-mini","question":"Scenario:\nYou are a senior software engineer tasked with building a real-time AI summarization microservice.\n\nFunctional Requirements:\n1. Implement an async RESTful POST endpoint `\/summarize` that accepts JSON `{ \"text\": \"<string>\" }` and returns a summary generated via `llama-cpp-python`.\n2. Implement a WebSocket endpoint `\/stream` that streams incremental summary tokens to the client as they are produced.\n3. Persist each original text and its summary in an async SQLite database using `SQLModel`.\n4. Cache recent summaries in Redis for 10 minutes with `aioredis` to avoid redundant computation.\n5. Expose Prometheus-compatible metrics on `\/metrics` (request counts, latency histograms, cache hit\/miss) via `prometheus-client`.\n6. Use a background task to warm up the Llama model at startup and to periodically clean up expired cache entries.\n\nDeliverables:\n- A restatement of the functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Code must use the latest versions of FastAPI, SQLModel, aioredis, llama-cpp-python, and prometheus-client as of 2025-05-06.\n- Include clear comments indicating where each external library is used.","question_index":54,"modules":["fastapi","sqlmodel","sqlalchemy","aioredis","uvicorn","prometheus_client","llama_cpp"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":7}
{"model":"gpt-4.1-mini","question":"You are a senior software engineer tasked with building a high-throughput real-time sentiment analysis microservice.\n\nFunctional Requirements:\n1. Use FastAPI (latest) to expose a POST \/analyze endpoint that accepts JSON text payloads and stores request metadata in PostgreSQL via SQLModel (latest).\n2. Secure all endpoints with OAuth2 JWT authentication using Authlib (latest), leveraging its PKCE support.\n3. Perform real-time sentiment analysis on incoming text using the latest Hugging Face Transformers pipeline with GPU acceleration via PyTorch (latest) and cache results in Redis using aiocache (latest).\n4. Broadcast live sentiment scores to connected clients over WebSockets using python-socketio (latest) and run the server on Uvicorn with HTTP\/3 support.\n5. Asynchronously upload any user-submitted audio snippets to S3-compatible storage via aiobotocore (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all external libraries as of 2025-05-06  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":62,"modules":["fastapi","sqlmodel","authlib","transformers","torch","aiocache","redis","python-socketio","aiobotocore","uvicorn","pydantic","hypercorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"gpt-4.1-mini","question":"Scenario: Build a high-performance Python web application combining HTTP\/3 REST, GraphQL, real-time WebSockets, async database operations, data analytics, AI summarization, and distributed tracing.\n\nFunctional Requirements:\n1. HTTP\/3 REST API: use FastAPI (latest as of 2025-05-06) with Uvicorn HTTP\/3 support to serve \/items endpoints.\n2. Async GraphQL API: use Strawberry (latest as of 2025-05-06) to expose a GraphQL schema with DataLoader for batch fetching.\n3. Async DB access: use SQLModel (latest as of 2025-05-06) with an async SQLAlchemy engine to perform PostgreSQL CRUD.\n4. Real-time WebSockets: use websockets library (latest as of 2025-05-06) to broadcast item updates at \/ws.\n5. Data analytics: use Polars (latest as of 2025-05-06) lazy API to compute summary statistics on items.\n6. AI summarization: use OpenAI Python SDK (latest as of 2025-05-06) to generate summaries of item descriptions.\n7. Observability: use OpenTelemetry Python SDK (latest as of 2025-05-06) to instrument all endpoints for distributed tracing.\n\nDeliverables:\n- requirements.txt listing exact latest versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments marking where each external library is used.","question_index":53,"modules":["fastapi","sqlmodel","sqlalchemy","strawberry-graphql","polars","openai","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1-mini","question":"Scenario: You are a senior software engineer building a real-time sentiment and OCR-enhanced geospatial chat dashboard for remote field teams.\n\nFunctional Requirements:\n1. Use FastAPI (latest version as of 2025-05-06) to implement an async REST API that serves a minimal HTML client.\n2. Implement bi-directional real-time chat via Socket.IO using python-socketio (latest) on the FastAPI server.\n3. Perform on-the-fly sentiment analysis of text messages using Hugging Face\u2019s transformers pipeline (latest) and broadcast sentiment scores.\n4. Accept image uploads from clients, extract embedded text with EasyOCR (latest), and inject the OCR result into the chat stream.\n5. Plot message geolocation metadata on a Folium (latest) map and serve it as an embeddable HTML snippet.\n\nDeliverables:\n- A requirements list enumerating the exact \u201clatest\u201d third-party libraries and versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":2,"modules":["fastapi","python-socketio","uvicorn","transformers","torch","easyocr","folium","aiofiles","jinja2"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1-mini","question":"Scenario: Develop a Python microservice that supports AI-driven product imagery, real-time stock updates, secure user authentication, and checkout processing.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) app with JWT-based auth using Authlib (latest) and secure password hashing.  \n2. Define async ORM models for Users and Products with SQLModel (latest) on PostgreSQL, including migration setup.  \n3. On product creation, generate an AI image via Diffusers (latest) or Stability-SDK (latest) and expose it via a REST endpoint.  \n4. Provide a WebSocket endpoint for real-time inventory updates using Starlette\u2019s WebSocket support (latest).  \n5. Create a \/checkout POST endpoint integrating Stripe\u2019s Python SDK (latest) with webhook handling to confirm payments.\n\nDeliverables:\n\u2022 A bullet list of the \u201clatest\u201d libraries (with pip install specifiers as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":85,"modules":["fastapi","sqlmodel","authlib","passlib","stripe","diffusers","torch","sqlalchemy","jose","pillow"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1-mini","question":"Scenario: You are developing a high-performance, real-time AI-powered web service for personalized content recommendations.\n\nFunctional Requirements:\n1. Implement a RESTful POST \/recommend endpoint using the latest Python web framework released within the last 12 months, accepting JSON input and returning JSON responses.\n2. Add a WebSocket \/ws endpoint for streaming live recommendation updates using the newest asyncio-based WebSocket library from the past year.\n3. Integrate semantic search over user profiles by connecting to a vector database via its newest Python client library released within the last 12 months.\n4. Schedule periodic model retraining in a background worker using the most recent Python task scheduler released in the last 12 months.\n5. Store and serve user-uploaded media files on cloud object storage using the latest cloud storage client library published within the past 12 months.\n\nDeliverables:\n\u2022 A requirements list (requirements.txt) specifying the exact library names and versions (all released within the last 12 months).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total, with clear comments marking where and why each external library is used.","question_index":71,"modules":["litestar","websockets","weaviate-client","APScheduler","google-cloud-storage"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-mini","question":"Scenario: Develop a high-performance Python microservice that ingests real-time social media posts via WebSockets, generates AI summaries, stores them in MongoDB Atlas, serves secured HTTP\/3 endpoints, and provides an interactive Streamlit dashboard for sentiment trends.\n\nFunctional Requirements:\n1. Initialize an asynchronous FastAPI app with HTTP\/3 (h2\/h3) and ASGI support.  \n2. Protect all HTTP endpoints with OAuth2 JWT using Authlib\u2019s latest OAuth library.  \n3. Consume a WebSocket stream of JSON posts from wss:\/\/stream.example.com asynchronously.  \n4. Summarize each post using OpenAI\u2019s latest Python SDK leveraging function-calling or embeddings.  \n5. Persist summaries and metadata in MongoDB Atlas via Motor (async driver).  \n6. Expose a secured REST endpoint `\/summaries` supporting pagination and filtering by sentiment.  \n7. Build a Streamlit dashboard that fetches `\/summaries` and visualizes sentiment trends over time with Plotly.\n\nDeliverables:\n- A requirements list naming the latest versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":28,"modules":["fastapi","authlib","jose","motor","openai","uvicorn","websockets","streamlit","httpx","plotly"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1-mini","question":"Scenario: Build a real-time HTTP\/3-powered AI chat service that streams user messages, generates LLM responses, persists embeddings in a vector DB, and exposes chat history via GraphQL.\n\nFunctional Requirements:\n1. HTTP\/3 streaming: use the latest aioquic (as of 2025-05-06) to handle bidirectional HTTP\/3 connections for chat.\n2. On-device LLM inference: integrate the latest llama-cpp-python for quantized model loading and response generation.\n3. Vector storage: use the latest chromadb Python client to embed each message (via llama-cpp-python\u2019s embed API) and store\/retrieve vectors.\n4. GraphQL API: implement an async GraphQL schema with the latest strawberry to query chat history from ChromaDB.\n5. Async orchestration: ensure end-to-end async I\/O under Python 3.11+, coordinating the QUIC server, LLM, and DB.\n6. Code constraints: keep the complete solution under 150 lines, include clear comments marking where each external library is used.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of aioquic, llama-cpp-python, chromadb, and strawberry (latest as of 2025-05-06)  \n\u2022 A single Python 3.11+ script (<150 lines) with runnable code and comments identifying each external library usage.","question_index":79,"modules":["aioquic","llama-cpp-python","chromadb","strawberry-graphql","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-mini","question":"Scenario: Design and implement a high-performance real-time chat backend with semantic search and background processing.\n\nFunctional Requirements:\n1. Asynchronous REST API: use FastAPI (latest as of 2025-05-06) to expose user and message endpoints.\n2. HTTP\/3 WebSockets: implement real-time chat over WebSockets with Hypercorn\u2019s HTTP\/3 support.\n3. Async ORM persistence: store users and messages in PostgreSQL via SQLModel (async).\n4. Vector semantic search: index and query messages in Qdrant using the latest qdrant-client.\n5. Background sentiment analysis: enqueue and run analysis jobs with ARQ (async Redis-backed queue).\n6. JWT authentication: secure all endpoints with python-jose for JWT issuance and validation.\n7. Redis caching: cache active WebSocket connections or session data via redis.asyncio.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06  \n\u2022 A single Python 3.11+ file under 150 lines that:\n  \u2013 Is fully runnable  \n  \u2013 Shows clear comments where each external library is used  \n  \u2013 Integrates all above requirements into a cohesive application","question_index":11,"modules":["fastapi","sqlmodel","asyncpg","python-jose","redis","arq","qdrant-client","httpx","sqlalchemy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1-mini","question":"Scenario: A fintech startup requires a live transaction monitoring service that ingests, processes, stores, and notifies users of suspicious activity in real time.\n\nFunctional Requirements:\n1. Build an HTTP endpoint to receive and stream-respond to JSON transaction payloads using the latest async web framework released in the last 12 months.\n2. Validate and serialize incoming data with the latest data validation library, enforcing strict type constraints and custom validators.\n3. Persist transactions asynchronously in a relational database using the latest async ORM library compatible with Python 3.11+.\n4. Offload CPU-intensive anomaly detection to a background worker queue using the latest task orchestration library.\n5. Broadcast real-time alerts to connected clients via WebSockets using the latest real-time communication library.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of the latest libraries (as of May 6, 2025)  \n\u2022 A single Python 3.11+ file under 150 lines, complete and runnable, with clear comments indicating where each external library is used.","question_index":39,"modules":["pydantic","fastapi","anyio","sqlalchemy","prefect","uvicorn","aiosqlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"Scenario: Build a real-time collaborative document editor service with AI-powered summarization, OAuth2 access control, WebSocket syncing, Redis caching, and NoSQL persistence.\n\nFunctional Requirements:\n1. Use the latest async Python web framework (as of 2025-05-06) to define RESTful CRUD endpoints for documents.\n2. Implement OAuth2 authorization with the latest Python OAuth library released within the last 12 months for secure user login and token issuance.\n3. Establish bi-directional real-time document synchronization over WebSockets using the newest real-time communication library (with built-in pub\/sub).\n4. Integrate the latest AI inference library published in the past year to generate live summaries of incremental document edits.\n5. Employ the latest Redis client library (released within the last year) to cache active sessions and coordinate pub\/sub channels for syncing.\n6. Persist document state and version history in a NoSQL database using the newest async database client released in the last 12 months.\n\nDeliverables:\n1. A numbered list of the chosen \u201clatest\u201d libraries (with a one-line description of their role).\n2. Complete, runnable Python 3.11+ code under 150 lines, including clear comments indicating where and why each external library is used.","question_index":86,"modules":["asyncio-webfw","next-oauth2","websoup","ai-summary","rejson-py","motor-next"],"modulenotfound":["asyncio-webfw","next-oauth2","websoup","ai-summary","rejson-py","motor-next"],"modulenotfound_count":6,"module_count":6}
{"model":"gpt-4.1-mini","question":"Scenario: Develop a user registration microservice over HTTP\/3 that validates input, persists user data, and integrates with an external email verification API.\n\nFunctional Requirements:\n1. Expose a POST \/users endpoint using Starlite (latest) with HTTP\/3 support.  \n2. Define request and response schemas with Pydantic v2 (latest) for data validation.  \n3. Persist user records in PostgreSQL via the latest Prisma Python client, including model definitions and migrations.  \n4. Perform an asynchronous HTTP\/3 request to an external email verification service using httpx (latest), and incorporate its response.  \n5. Return a JSON payload containing the stored user data and email verification status.\n\nDeliverables:\n- A list of required third-party libraries with exact versions (as of today)  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":100,"modules":["starlite","pydantic","prisma","httpx","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-mini","question":"Scenario: As a senior software engineer, build a modular web application that authenticates users with WebAuthn passkeys, streams real-time stock data, generates AI trading signals, processes payments, and delivers interactive dashboards.\n\nFunctional Requirements:\n1. Implement user authentication via WebAuthn using the latest pywebauthn library (as of 2025-05-06) with hardware security key support.\n2. Ingest and stream real-time stock price data over WebSockets using FastAPI and the latest Starlette HTTP\/2 push features.\n3. Generate AI-driven trading signals leveraging the latest OpenAI Python SDK\u2019s new function-calling interface.\n4. Integrate secure payment processing with the latest Stripe Python SDK supporting Payment Element and webhook handling.\n5. Serve interactive dashboards using the latest Plotly Dash or Reflex library for live data visualization and client-side callbacks.\n6. Add structured logging and Prometheus metrics using structlog and prometheus-client.\n\nDeliverables:\n- A concise mapping of each functional requirement to its implementation approach.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used and using the latest libraries available as of 2025-05-06.","question_index":56,"modules":["fastapi","prometheus_client","structlog","pywebauthn","starlette","openai","stripe","dash","requests","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1-mini","question":"You are a senior software engineer tasked with architecting a small, high-performance async web application using only the latest Python libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Litestar framework (with native HTTP\/3 support) to define async REST endpoints for user signup and profile retrieval.  \n2. Integrate the latest Piccolo-ORM to define a PostgreSQL \u201cusers\u201d table and perform async CRUD operations.  \n3. Implement real-time notifications via WebSocket using the latest python-socketio in asyncio mode.  \n4. Schedule a daily summary email job with APScheduler\u2019s new AsyncIOScheduler and send mail via an async SMTP library.  \n5. Fetch external data during signup from a third-party API using HTTPX\u2019s HTTP\/2 client.\n\nDeliverables:\n\u2022 A requirements.txt listing exact package names and latest versions.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":29,"modules":["litestar","piccolo","socketio","apscheduler","aiosmtplib","httpx","pydantic","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-mini","question":"Scenario: You are a senior software engineer building a high-performance real-time IoT analytics backend.\n\nFunctional Requirements:\n1. Expose an asynchronous HTTP API using FastAPI with WebSocket support to ingest and broadcast live sensor readings.\n2. Persist incoming data in PostgreSQL via SQLModel\u2019s async ORM and validate payloads with Pydantic v2 models.\n3. Enforce per-device rate limiting using Redis Streams and aioredis, leveraging RedisJSON for overflow handling.\n4. Secure all endpoints with OAuth2 passwordless login via FastAPI-Users, sending one-time codes.\n5. Schedule background aggregation jobs with Celery (beat scheduler) to compute rolling metrics and push updates over WebSockets.\n6. Provide a paginated historical metrics REST endpoint, caching hot queries in Redis for low-latency responses.\n\nDeliverables:\n- A list of required external libraries pinned to the latest versions available as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is utilized.","question_index":27,"modules":["fastapi","fastapi-users","pydantic","sqlmodel","aioredis","redis","celery","python-jose","email-validator","asyncpg","sqlalchemy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"gpt-4.1-mini","question":"Scenario: Build a real-time collaborative document review service with semantic search and AI summarization.\n\nFunctional Requirements:\n1. HTTP API (HTTP\/3) using the latest starlite (v1.x) to manage documents and user sessions.  \n2. Real-time updates over WebSockets for live editing and presence using starlite\u2019s WebSocket support.  \n3. Semantic indexing and search: on each document save, compute embeddings and store\/retrieve via the latest qdrant-client.  \n4. AI summarization endpoint: call an LLM using the latest llm-tools library to generate concise summaries on demand.  \n5. Background task scheduling with dramatiq (latest) to clean up stale sessions and regenerate indexes.\n\nDeliverables:\n\u2022 Reiterate the numbered requirements.  \n\u2022 A complete, runnable Python 3.11+ code file under 150 lines.  \n\u2022 Use the latest starlite, qdrant-client, llm-tools, and dramatiq libraries available as of 2025-05-06.  \n\u2022 Include clear comments indicating where and why each external library is used.","question_index":55,"modules":["starlite","pydantic","qdrant-client","llm-tools","dramatiq","uvicorn"],"modulenotfound":["llm-tools"],"modulenotfound_count":1,"module_count":6}
{"model":"gpt-4.1-mini","question":"Scenario: You are building a high-performance, AI-driven text-generation microservice for a real-time web application.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/generate endpoint using FastAPI (latest 0.100+) for incoming requests.\n2. Validate and parse the request payload with Pydantic v2 (latest) leveraging its new ArgModel feature.\n3. Enqueue each generation request as an asynchronous job using ARQ (latest 0.7+) backed by Redis.\n4. In the ARQ worker, invoke vLLM (latest 0.7+) AsyncLLMClient to perform streaming text inference.\n5. Persist each request and its full response to PostgreSQL using SQLModel (latest 0.0.x) with SQLAlchemy 2.0 async engine.\n6. Expose a WebSocket \/ws\/stream endpoint in FastAPI to broadcast partial LLM outputs to clients in real time.\n\nDeliverables:\n\u2022 The above numbered requirements list.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":30,"modules":["fastapi","arq","pydantic","sqlmodel","sqlalchemy","vllm","asyncpg","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-mini","question":"Scenario: You must build a Python microservice that fetches articles by URL, summarizes them with AI, stores results in Postgres, and provides REST, GraphQL, and WebSocket interfaces with background orchestration using only the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a POST \/submit REST endpoint using the latest Starlite framework (released within the last 12 months) over HTTP\/3 to accept JSON { \"url\": string }.  \n2. Fetch article content asynchronously using the latest httpx library and handle timeouts and redirects.  \n3. Summarize text with the latest HuggingFace Transformers v5 pipeline, leveraging GPU if available.  \n4. Persist the original URL and its summary in Postgres via Tortoise ORM v0.25 (async ORM released in the last 12 months).  \n5. Expose a \/graphql endpoint using Strawberry GraphQL (latest release) to query summaries by URL.  \n6. Provide a \/ws\/notifications WebSocket in Starlite to notify clients in real time when a summary is ready.  \n7. Orchestrate fetch-and-summarize tasks as background jobs using Prefect 2 (newly released workflow orchestrator).\n\nDeliverables:\n- A copy of the numbered functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Use only the latest versions of Starlite, httpx, Transformers v5, Tortoise ORM v0.25, Strawberry GraphQL, and Prefect 2 as of 2025-05-06.\n- Include clear comments indicating where and how each external library is used.","question_index":70,"modules":["starlite","pydantic","httpx","transformers","tortoise-orm","strawberry-graphql","prefect","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-mini","question":"Scenario: You are tasked with developing a next-generation chat analytics platform that seamlessly integrates HTTP\/3, JWT auth, real-time messaging, vector search, and on-device LLM inference.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) with HTTP\/3 support via Hypercorn to serve all endpoints.\n2. Implement JWT authentication with rotational refresh tokens using fastapi-users (latest).\n3. Provide a real-time WebSocket chat endpoint using fastapi-websocket-pubsub (latest) supporting pub\/sub channels.\n4. On each incoming chat message, generate vector embeddings using llama-cpp-python (latest) with an on-device streaming pipeline, and store them in ChromaDB (latest) with HNSW indexing.\n5. Expose a REST endpoint to query semantic similarity: accept a text query, compute embeddings, perform a ChromaDB search, and return the top-3 similar messages.\n6. Expose a streaming summarization endpoint that returns an on-the-fly summary of recent messages using llama-cpp-python\u2019s streaming completion interface.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements you\u2019ve implemented.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":93,"modules":["fastapi","fastapi-users","fastapi-websocket-pubsub","llama-cpp-python","chromadb","uvicorn","hypercorn","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-mini","question":"Scenario: As a senior software engineer, develop a Python 3.11+ microservice that manages real-time collaborative whiteboard sessions, persists data, exposes REST and GraphQL APIs, generates session snapshots, integrates external metrics, and secures endpoints with OAuth2.\n\nFunctional Requirements:\n1. Implement session and user management REST endpoints using FastAPI (latest) with async event hooks and Pydantic V2 models.  \n2. Persist all data in PostgreSQL via Tortoise ORM (latest) using model lifecycle events.  \n3. Expose a GraphQL API with Strawberry GraphQL (latest) dataclass resolvers for session and user queries\/mutations.  \n4. Broadcast real-time drawing events using python-socketio (latest) over WebSockets.  \n5. Generate on-demand PNG snapshots of whiteboard state with Pillow-SIMD (latest) leveraging multi-threaded rendering.  \n6. Send session metrics asynchronously to an external analytics service using httpx (latest) with HTTP\/2 support.  \n7. Secure all endpoints with OAuth2 (authorization code + PKCE) using Authlib (latest).  \n\nDeliverables:\n- A numbered list of the functional requirements above.  \n- Complete, runnable Python 3.11+ code (\u2264 150 lines) integrating all requirements.  \n- Inline comments indicating where each external library is used.  \n- Use the latest stable releases of all libraries as of today\u2019s date.","question_index":50,"modules":["fastapi","pydantic","tortoise-orm","strawberry-graphql","python-socketio","pillow-simd","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"Scenario: You are a senior software engineer tasked with building a real-time analytics microservice that combines REST, GraphQL, persistent storage, and background processing using the latest Python ecosystem tools as of 2025-05-06.\n\nFunctional Requirements:\n1. Expose HTTP and WebSocket endpoints using the latest Litestar ASGI framework with annotated routing and streaming support.\n2. Implement a GraphQL API using the latest Strawberry GraphQL code-first schema builder with asyncio support.\n3. Persist and query analytics data in PostgreSQL via the latest Piccolo-ORM\u2019s async-first models and automatic migrations.\n4. Offload long-running analytics jobs to a Redis-backed async queue using the latest Arq library.\n5. Send WebSocket notifications to clients when background jobs complete.\n\nDeliverables:\n\u2022 A requirements list (pip install commands) specifying the latest versions of Litestar, Strawberry, Piccolo-ORM, and Arq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":65,"modules":["litestar","strawberry-graphql","piccolo","arq","asyncpg","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-mini","question":"Scenario: You are a senior software engineer tasked with developing an end-to-end Python web application that leverages the latest libraries to showcase advanced API development, ORM integration, security, background processing, and real-time features.\n\nFunctional Requirements:\n1. Asynchronous REST API: implement CRUD endpoints for a `Task` model using FastAPI (latest version as of 2025-05-06).\n2. Data Modeling and Persistence: define `Task` ORM models with SQLModel (latest version) and connect to a SQLite or PostgreSQL database asynchronously.\n3. Security: secure all API endpoints with OAuth2 password flow using Authlib (latest version) to obtain and validate JWT tokens.\n4. Background Processing: create an asynchronous task queue with Arq (latest version) to process tasks in the background.\n5. Real-Time Notifications: establish a WebSocket endpoint using the websockets library (latest version) to push real-time task status updates.\n\nDeliverables:\n- The numbered list of functional requirements (as above).\n- Complete, runnable Python 3.11+ code under 150 lines. Use the latest versions of FastAPI, SQLModel, Authlib, Arq, and websockets as of 2025-05-06. Include clear comments indicating where and how each external library is used.","question_index":24,"modules":["fastapi","sqlmodel","sqlalchemy","authlib","arq","websockets","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"Scenario: Build a real-time collaborative messaging microservice integrating graph storage, vector search, JWT authentication, and WebSockets using the latest Python libraries (as of May 6, 2025).\n\nFunctional Requirements:\n1. Authentication: secure all endpoints with JWTs using the latest TokenGuard library.\n2. Message Ingestion: implement POST \/messages to accept user messages; decode JWT, extract user ID.\n3. Graph Storage: store each message and its \u201cauthored_by\u201d relationship in a graph database via the latest GraphitORM.\n4. Vector Indexing & Search: generate text embeddings with the latest Morpher library, store them in its vector store, and expose GET \/search?q={text} to return top-5 similar messages.\n5. Real-Time Updates: broadcast new messages to connected clients over WebSockets using the latest Voila framework.\n\nDeliverables:\n1. A requirements list with each third-party library pinned to its latest version as of May 6, 2025.\n2. A single Python 3.11+ script (\u2264150 lines) implementing all endpoints, with clear comments marking where each external library is used.","question_index":35,"modules":["fastapi","tokenguard","graphitorm","morpher","voila","uvicorn"],"modulenotfound":["tokenguard","graphitorm","morpher"],"modulenotfound_count":3,"module_count":6}
{"model":"gpt-4.1-mini","question":"You are tasked with building a high-performance text summarization service with real-time updates, persistent storage, and caching for internal research teams.\n\nFunctional Requirements:\n1. Implement OAuth2 password flow with JWT tokens for user authentication using the latest python-jose.\n2. Expose a POST \/summarize endpoint that uses the latest transformers library to perform text summarization.\n3. Provide a WebSocket at \/ws\/progress to stream summarization progress in real time using FastAPI\u2019s latest WebSocket support.\n4. Persist user requests and summaries in a SQLite database via the latest SQLModel ORM.\n5. Cache summary results in Redis for 5 minutes using the latest aioredis library to accelerate repeat requests.\n\nDeliverables:\n\u2022 A requirements list specifying the latest versions of FastAPI, transformers, python-jose, SQLModel, aioredis (as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":10,"modules":["fastapi","jose","sqlmodel","transformers","aioredis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-mini","question":"You are a senior software engineer.\n\nScenario: Build an end-to-end Python service for user profile CRUD with high-performance caching and real-time notifications.\n\nFunctional Requirements:\n1. Implement RESTful CRUD endpoints for user profiles using the latest FastAPI (as of 2025-05-06), leveraging its async lifespan context.\n2. Define an async PostgreSQL-backed User model with SQLModel (latest) using Relations and the new async engine.\n3. Cache GET \/users\/{id} responses in Redis with redis.asyncio (latest), utilizing cluster mode and automatic TTL invalidation on updates.\n4. Broadcast profile change events in real time over WebSocket using the websockets library (latest) with permessage-deflate compression.\n\nDeliverables:\n- A requirements.txt listing the latest versions of FastAPI, SQLModel, redis.asyncio and websockets as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":17,"modules":["fastapi","sqlmodel","asyncpg","uvicorn","redis","websockets","sqlalchemy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"Scenario: You are a senior software engineer building a real-time sentiment-analysis web service for social media feeds.\n\nFunctional Requirements:\n1. Use the latest Python web framework (released within the last 12 months) to expose REST endpoints with dependency injection and OpenAPI support.\n2. Implement real-time websocket notifications for incoming sentiment updates using a recently released WebSocket library (last 12 months).\n3. Persist incoming posts and sentiment scores in a NoSQL database via its newest async client library (released within the last 12 months).\n4. Offload sentiment inference to a CPU-optimized ML library (first released within the last 12 months) with zero-copy or memory-efficient batch inference.\n5. Schedule periodic cleanup and summary-generation tasks using a lightweight scheduler library introduced within the last 12 months.\n6. Serve a minimal interactive dashboard at \u201c\/dashboard\u201d using a modern, Python-native plotting\/UI library created in the last 12 months.\n\nDeliverables:\n- A bullet list of all third-party libraries chosen, pinned to \u201clatest\u201d versions as of today\u2019s date.\n- A single, complete Python 3.11+ file (under 150 lines) that implements all requirements, with clear comments marking where each external library is used.","question_index":94,"modules":["litestar","pydantic","motor","vllm","async_scheduler","flexx","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"Scenario: As a senior software engineer, you must build a real-time analytics backend that ingests user events, persists them, and serves both REST and GraphQL clients with low latency and live updates.\n\nFunctional Requirements:\n1. Implement a POST \/events endpoint using the latest FastAPI (as of 2025-05-06), leveraging its new lifespan context and async router features.\n2. Validate incoming JSON payloads with Pydantic v2\u2019s new @model_validator decorator for strict schema enforcement.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise ORM, utilizing its recent bulk_create optimization.\n4. Expose a GraphQL query and mutation schema via the latest Strawberry GraphQL with code-first federation support.\n5. Broadcast new events to connected clients over WebSockets using the latest websockets library with per-message deflate compression.\n\nDeliverables:\n\u2022 A requirements.txt listing each third-party library pinned to its latest version as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments indicating where and how each external library is used.","question_index":19,"modules":["fastapi","pydantic","tortoise-orm","strawberry-graphql","websockets","uvicorn","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"Scenario: You are a senior software engineer building a modern microblogging backend that supports authenticated posts, AI-generated images, real-time feed updates, and cloud storage integration.\n\nFunctional Requirements:\n1. Implement a REST API using a Python web framework released in the last 12 months (use the latest available as of 2025-05-06).\n2. Persist posts in a SQL database via an async ORM library created in the last 12 months, leveraging advanced async model features.\n3. Broadcast new posts to connected clients over WebSockets using a recently released high-performance async messaging library.\n4. Authenticate users with OAuth2 (Google) using the latest OAuth client library.\n5. Generate post images by calling Stable Diffusion through the most recent Python API client for AI image generation.\n6. Store and serve AI-generated images in AWS S3 using the newest AWS SDK for Python.\n7. Ensure all components run under Python 3.11+ and fit within 150 lines of code.\n\nDeliverables:\n\u2022 A numbered list of the specific libraries (with versions) you chose to satisfy each requirement.  \n\u2022 Complete, runnable Python 3.11+ code (<150 lines) combining all features.  \n\u2022 Clear inline comments marking where and why each external library is used.","question_index":7,"modules":["litestar","ormar","databases","sqlalchemy","authlib","diffusers","boto3","aiokafka"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-mini","question":"Scenario: You are a senior software engineer tasked with building a serverless real-time IoT analytics service that ingests sensor data, performs streaming aggregation, applies online anomaly detection, and streams results to clients.\n\nFunctional Requirements:\n1. Ingest JSON messages from an MQTT broker (\u201csensors\/+\/data\u201d) using the latest asyncio-based gmqtt library (as of 2025-05-06).  \n2. Stream-process incoming messages with streamz (latest version) to compute 1-minute tumbling-window averages for temperature and humidity.  \n3. Apply online anomaly detection on each windowed aggregation using River\u2019s latest HDBSCAN-based drift detector.  \n4. Persist raw messages and aggregated results into TimescaleDB via the asyncpg driver (latest version).  \n5. Expose a Python 3.11+ FastAPI application with a WebSocket endpoint for live metric and anomaly alert streaming.  \n6. Wrap the FastAPI app for AWS Lambda deployment using the latest mangum adapter.\n\nDeliverables:\n\u2022 A list of third-party dependencies pinned to \u201clatest\u201d versions as of 2025-05-06.  \n\u2022 A single, complete, runnable Python 3.11+ source file under 150 lines.  \n\u2022 Clear inline comments showing where each external library is used.  \n\u2022 The code must integrate all functional requirements and be deployable serverlessly on AWS Lambda.","question_index":38,"modules":["fastapi","mangum","gmqtt","streamz","river","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-mini","question":"Scenario:\nYou are tasked with developing a lightweight AI-powered semantic search microservice in Python 3.11+.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI framework with HTTP\/3 support) to expose a POST \/search endpoint.  \n2. Validate and parse incoming JSON with the latest Pydantic v2\u2019s strict model parsing and Python pattern-matching features.  \n3. Asynchronously fetch text embeddings from OpenAI\u2019s embedding API using the latest HTTPX client with HTTP\/3.  \n4. Store and query vectors in a GPU-accelerated ChromaDB instance via the latest ChromaDB Python client.  \n5. Return the top 5 semantically related document IDs and similarity scores as a JSON response.\n\nDeliverables:\n\u2022 A numbered list confirming the above functional requirements.  \n\u2022 A single, complete, runnable Python 3.11+ source file (\u2264150 lines) that implements the microservice, with clear comments indicating where each external library is used.","question_index":3,"modules":["starlite","pydantic","httpx","chromadb","openai","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-mini","question":"Scenario: Build a real-time chat analysis service that ingests user messages, performs on-device LLM summarization and sentiment analysis, caches results, persists chat history, and broadcasts updates via WebSockets.\n\nFunctional Requirements:\n1. Expose a POST \/messages endpoint using FastAPI (latest) to receive JSON payloads with \u201cuser_id\u201d and \u201ctext\u201d.\n2. Perform sentiment analysis and summarization on each message using llama-cpp-python (latest) in an async background task.\n3. Cache sentiment results for identical texts in Redis via redis-py (latest) to avoid recomputation.\n4. Persist messages, sentiments, and summaries in PostgreSQL using SQLModel (latest version).\n5. Provide a WebSocket \/ws\/{user_id} endpoint (FastAPI) that broadcasts new summaries and sentiments to connected clients in real time.\n6. Secure all endpoints with OAuth2 password flow via Authlib (latest).\n\nDeliverables:\n\u2022 Restate the numbered requirements above.  \n\u2022 Provide complete, runnable Python 3.11+ code integrating the latest versions of FastAPI, llama-cpp-python, redis-py, SQLModel, and Authlib, under 150 lines total.  \n\u2022 Include clear comments indicating where and how each external library is used.","question_index":32,"modules":["fastapi","sqlmodel","redis","authlib","pydantic","llama-cpp-python","uvicorn","psycopg2"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-mini","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python microservice for real-time IoT sensor analytics and visualization.\n\nFunctional Requirements:\n1. REST API: Use the latest asynchronous Python web framework available as of 2025-05-06 to expose CRUD endpoints with advanced dependency injection and OpenAPI schema generation.\n2. WebSockets: Integrate the latest real-time streaming library as of 2025-05-06 supporting protocol-level backpressure to push live sensor updates to clients.\n3. ML Inference: Embed an on-the-fly analytics model using the latest Python JIT-accelerated inference library as of 2025-05-06, auto-batching requests on CPU\/GPU.\n4. Graph Storage: Persist sensor relationships in a distributed graph database via the latest async driver as of 2025-05-06 with reactive query pipelining.\n5. Dashboard UI: Serve an interactive web dashboard using the latest server-driven UI component library as of 2025-05-06 for real-time charting and controls.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":14,"modules":["fastapi","pydantic","torch","neo4j","litestar","starlette-websocket","uvicorn"],"modulenotfound":["starlette-websocket"],"modulenotfound_count":1,"module_count":7}
{"model":"gpt-4.1-mini","question":"You are a Senior Software Engineer tasked with building a high-performance, real-time analytics microservice using the latest asynchronous Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled ASGI endpoint using the latest Litestar and aioquic (as of 2025-05-06) for bi-directional streaming.\n2. Implement real-time GraphQL subscriptions with the latest Strawberry GraphQL to push analytics updates.\n3. Define domain models and perform asynchronous CRUD operations on Postgres using the latest SQLModel.\n4. Integrate a Redis cache layer with the latest aioredis for hot-path data retrieval.\n5. Instrument request handling and background tasks with OpenTelemetry SDK for distributed tracing.\n\nDeliverables:\n- requirements.txt listing the latest library versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments indicating where each external library is used.","question_index":90,"modules":["litestar","strawberry-graphql","sqlmodel","asyncpg","aioredis","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-asgi","uvicorn","aioquic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1-mini","question":"You are building a real-time collaborative code-review web application that leverages the latest AI, async, storage and security frameworks.\n\nFunctional Requirements:\n1. Implement a FastAPI 0.95+ server with async endpoints and Jinja2 templates for the main UI.\n2. Enable real-time code editing and review comments via WebSockets using the latest websockets library as of 2025-05-06.\n3. On each code submission, stream analysis from OpenAI\u2019s GPT-4o API (or equivalent newest model) and display suggestions in real time.\n4. Persist code snapshots and review metadata in Pinecone\u2019s newest Python client (vector storage) for similarity search.\n5. Secure all HTTP and WS routes with JWT authentication using the latest PyJWT release.\n6. Add structured logging and a \/healthz endpoint using structlog for observability.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total.  \n\u2022 Clear comments indicating where and why each external library is used.","question_index":37,"modules":["structlog","jwt","fastapi","websockets","pinecone-client","openai"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-mini","question":"Scenario: Build a real-time Q&A web application that ingests documents, generates embeddings, serves semantic search, supports live WebSocket notifications, and exposes observability metrics.\n\nFunctional Requirements:\n1. Use the latest FastAPI (>=0.100) to implement asynchronous REST endpoints for document ingestion and search, plus a WebSocket endpoint leveraging its new lifecycle event hooks.\n2. Upon document ingestion, asynchronously generate embeddings with the latest llama-cpp-python (v0.2+) using 4-bit quantization and store vectors in Pinecone (latest) using hybrid search.\n3. Implement a `\/search` endpoint that queries Pinecone for nearest neighbors based on user input and returns ranked results.\n4. Schedule periodic re-indexing tasks with the latest arq (v1.10+), utilizing its async Redis job scheduler.\n5. Integrate OpenTelemetry Python SDK (v1.21+) for auto-instrumentation, exporting traces and metrics to Prometheus.\n\nDeliverables:\n- A requirements.txt listing the latest libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":33,"modules":["fastapi","llama_cpp","pinecone-client","arq","redis","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","opentelemetry-exporter-prometheus","prometheus-client","uvicorn","starlette"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":12}
{"model":"gpt-4.1-mini","question":"Scenario: As a senior software engineer, you must build a modular Python web application that serves real-time, ML-enhanced classification results with persistent storage and observability.\n\nFunctional Requirements:\n1. API Layer: Use the latest ASGI web framework available as of 2025-05-06 that supports HTTP\/3 and automatic OpenAPI generation to expose a POST \/classify endpoint.  \n2. Database Layer: Connect asynchronously to PostgreSQL using the latest async driver with statement pooling to record incoming requests and classification results.  \n3. Real-time Pub\/Sub: Implement server-side WebSocket broadcasts of classification results using the latest message queue library offering at-least-once delivery semantics.  \n4. ML Inference: Integrate on-demand image classification via the latest lightweight, GPU-accelerated inference engine released in the past 12 months.  \n5. Observability: Instrument distributed tracing and metrics collection using the latest OpenTelemetry-compatible tracing library.\n\nDeliverables:\n- A list of the latest library dependencies as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":18,"modules":["fastapi","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-asyncpg","opentelemetry-sdk","opentelemetry-exporter-otlp-proto-grpc","asyncpg","arq","tritonclient","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1-mini","question":"Scenario: As a senior software engineer, build a high-performance microblogging API using the latest Python libraries for REST, validation, async database operations, caching, background tasks, and real-time streaming.\n\nFunctional Requirements:\n1. Implement a user signup endpoint in FastAPI (>=0.100.0) using Pydantic v2 root_validator for advanced schema validation.  \n2. Create and list posts asynchronously in PostgreSQL via SQLModel\u2019s latest async engine.  \n3. Maintain a cache of the 10 most recent posts in Redis using redis-py (>=5.0) asyncio client, updating on post creation.  \n4. Offload image thumbnail generation to an async Dramatiq (>=2.0) background task triggered on new post with image.  \n5. Provide a WebSocket endpoint in FastAPI to broadcast newly created posts to connected clients in real time.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library (FastAPI, Pydantic v2, SQLModel, redis-py asyncio, Dramatiq) is used. Use the latest available versions of all libraries as of 2025-05-06.","question_index":87,"modules":["fastapi","pydantic","sqlmodel","redis","dramatiq","asyncpg","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-mini","question":"Scenario: You are building a real-time news aggregation microservice with live client updates.\n\nFunctional Requirements:\n1. Develop a REST API using Litestar (the latest version as of 2025-05-06) leveraging its OpenAPI v3.1 auto-generation feature.\n2. Asynchronously fetch and parse external RSS feeds over HTTP\/3 using HTTPX AsyncClient (the latest), then cache results in aiocache (the latest) with a TTL.\n3. Validate and serialize each article using Pydantic v2 (the latest) functional-style models to ensure data integrity.\n4. Expose a WebSocket endpoint using the websockets library (the latest) with permessage-deflate compression for real-time article broadcasting.\n5. Schedule periodic background tasks with Arq (the latest) to re-poll feeds every 5 minutes.\n\nDeliverables:\n- A list of the chosen libraries and their latest versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":31,"modules":["pydantic","litestar","httpx","aiocache","websockets","arq","feedparser","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-mini","question":"Scenario: Build a real-time collaborative note-taking microservice integrating REST, database persistence, caching, GraphQL, WebSockets, and background tasks.\n\nFunctional Requirements:\n1. Expose REST endpoints using FastAPI (latest) for creating, updating and retrieving notes; leverage Pydantic v2\u2019s new validation decorators.\n2. Persist notes asynchronously in SQLite via SQLModel (latest) with SQLAlchemy 2.0-style typing and async engine.\n3. Broadcast note creation and updates over WebSockets with FastAPI\u2019s WebSocket support to subscribed clients in real time.\n4. Cache note retrieval results in Redis using aioredis (latest), implement TTL and cache invalidation on updates.\n5. Provide a GraphQL schema and endpoint using Strawberry (latest) to query and filter notes with federation-style resolvers.\n6. Schedule a daily summary email using FastAPI BackgroundTasks and aiosmtplib (latest) for asynchronous SMTP delivery.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- A single Python 3.11+ source file under 150 lines that implements all requirements, with clear comments marking where each external library is used.","question_index":73,"modules":["fastapi","pydantic","sqlmodel","aioredis","strawberry-graphql","aiosmtplib","sqlalchemy","uvicorn","fastapi-utils"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1-mini","question":"Scenario: You are a senior software engineer building a proof-of-concept Python service that ingests user text, generates embeddings and classifications via an LLM, stores them, and streams nearest-neighbor results in real time.\n\nFunctional Requirements:\n1. Implement a JWT-secured POST \/ingest endpoint using the latest async Python web framework as of 2025-05-06 (e.g., Litestar).  \n2. In \/ingest, accept JSON `{ \"text\": \"...\" }`, call a state-of-the-art LLM (via the latest Transformers or LangChain client) to produce both an embedding vector and a classification label.  \n3. Persist the embedding vector in a vector database using the newest ChromaDB client.  \n4. Log the original text and its classification into a relational database through the latest async ORM (e.g., Prisma Python).  \n5. Expose a GET \/stream endpoint that streams nearest-neighbor matches for incoming embeddings over server-sent events using the newest SSE library (e.g., httpx-sse or framework-native support).  \n6. Target Python 3.11+, include type hints, and keep all code under 150 lines.\n\nDeliverables:\n\u2022 A `requirements.txt` (or equivalent) listing each library with exact versions (latest as of 2025-05-06).  \n\u2022 A single Python module (\u2264150 lines) that meets all requirements, with clear comments marking where and why each external library is used.","question_index":81,"modules":["litestar","pydantic","prisma","chromadb","langchain","transformers","httpx_sse","uvicorn","python-jose"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1-nano","question":"Scenario: As a senior software engineer, build a high-performance Python web application integrating real-time streaming, GraphQL, async ORM, JWT security, and caching using the latest third-party libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. HTTP & WebSocket server with the latest Litestar, utilizing its new streaming response API for real-time event delivery.\n2. GraphQL API with the latest Strawberry-GraphQL, defining queries, mutations, and subscription streams for live data updates.\n3. Async database models and CRUD operations using the latest Piccolo ORM\u2019s Python 3.11+ async support and built-in migration tooling.\n4. JWT authentication via the latest python-jose, supporting secure login, token rotation, and injection of user context into requests.\n5. Redis-backed caching with the latest aioredis to cache GraphQL query results and automatically invalidate on data changes.\n6. Trigger real-time client notifications over WebSockets when database events occur, leveraging Litestar\u2019s WebSocket routing.\n\nDeliverables:\n\u2022 A numbered list of the final functional requirements.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":57,"modules":["piccolo","strawberry-graphql","aioredis","python-jose","litestar"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-nano","question":"Scenario: You are tasked with building a modular Python 3.11+ microservice that delivers real-time chat with semantic search and scheduled maintenance, leveraging only the latest libraries released within the past 12 months.\n\nFunctional Requirements:\n1. Implement HTTP REST endpoints for posting and retrieving chat messages using Litestar (latest release).  \n2. Establish a WebSocket chat channel that broadcasts new messages to all connected clients via the official Litestar WebSocket plugin (latest).  \n3. Persist each message to MongoDB with Beanie ODM (latest) and simultaneously index its vector embedding into Qdrant using qdrant-client (latest).  \n4. Expose a GraphQL query endpoint that performs semantic search over stored messages via strawberry-graphql (latest).  \n5. Schedule a daily background job to purge messages older than 30 days using Dramatiq (latest).\n\nDeliverables:\n- A requirements.txt (or pyproject.toml) listing all dependencies with exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments marking where and how each external library is used.","question_index":76,"modules":["beanie","motor","litestar","strawberry","dramatiq","qdrant-client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python web service that combines asynchronous REST & GraphQL APIs, real-time notifications, persistent storage, caching, and AI-driven text summarization.\n\nFunctional Requirements:\n1. Implement async HTTP endpoints with FastAPI (latest as of 2025-05-06) using Python 3.11 structural pattern matching and Pydantic v2 dataclass models.\n2. Provide a GraphQL endpoint powered by Strawberry GraphQL v1.0+ with schema federation support.\n3. Persist and query data in PostgreSQL via the latest Tortoise-ORM with asyncio-optimized connection pooling.\n4. Push real-time notifications over WebSockets using python-socketio latest async server mode.\n5. Summarize user-submitted text on demand using the latest llama-cpp-python library with optional GPU offload.\n6. Cache frequent database queries in Redis using aioredis latest cluster-mode support.\n\nDeliverables:\n- A requirements list specifying exact versions of all third-party libraries.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":74,"modules":["fastapi","pydantic","starlette","strawberry-graphql","tortoise-orm","aioredis","python-socketio","llama-cpp-python","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1-nano","question":"Scenario: Build a real-time collaborative note-taking service that supports user auth, live updates, AI summaries, PDF export, and caching using the latest Python web stack.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST API for user signup\/login with FastAPI (latest) using OAuth2 password flow.\n2. Persist users and notes in PostgreSQL via SQLModel (latest async features).\n3. Create a WebSocket endpoint broadcasting live note edits using the newest websockets library.\n4. Add an endpoint `\/summarize` that calls an external AI summarization API via httpx\u2019s HTTP\/3 client (latest).\n5. Provide a `\/export\/{note_id}` endpoint that generates a PDF of the note using Playwright Python (latest).\n6. Cache active JWT tokens in Redis Streams using aioredis v2.x.\n7. Emit structured JSON logs via loguru (latest advanced configuration).\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- A complete, runnable Python 3.11+ codebase under 150 lines, with clear comments indicating where each external library is used.","question_index":9,"modules":["fastapi","uvicorn","sqlmodel","httpx","redis","playwright","loguru"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-nano","question":"You are a senior software engineer tasked with building a minimal Python web service for text ingestion, semantic search, and AI-powered summarization using the latest libraries.\n\nFunctional Requirements:\n1. HTTP API: use the latest Starlite framework (as of 2025-05-06) to expose async endpoints.\n2. Text Storage: on POST \/submit, accept JSON {\u201ctext\u201d: \u2026}, store original text and metadata in Redis via the latest redis-om Python client with async HashModel.\n3. Embedding: upon submission, generate a vector embedding using the latest ChromaDB Python client\u2019s async API and upsert into its vector store.\n4. Semantic Search: implement GET \/search?query=\u2026 that computes a query embedding via ChromaDB and returns top 3 matching texts from Redis.\n5. Summarization: implement GET \/summarize\/{id} to retrieve stored text by ID and produce a concise summary using the latest llama-cpp-python streaming API.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries (as of 2025-05-06).  \n\u2022 A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments indicating where and how each external library is used.","question_index":45,"modules":["starlite","redis-om","chromadb","llama-cpp-python"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4.1-nano","question":"Scenario: Senior software engineer tasked with developing a real-time GraphQL web service that validates incoming event data, persists it to PostgreSQL, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI web framework) as of 2025-05-06 to implement an async HTTP POST endpoint at `\/events`.\n2. Validate incoming JSON payloads with Pydantic v2 (latest) models, including at least one custom field validator.\n3. Persist validated events to PostgreSQL using the latest Prisma Python client (type-safe generated models and migrations).\n4. Expose a GraphQL API with the latest Strawberry GraphQL library, supporting queries for event history and subscriptions for real-time updates.\n5. Configure WebSocket support in Starlite to broadcast new events to all GraphQL subscription clients.\n\nDeliverables:\n- A brief recap of the functional requirements.\n- Complete, runnable Python 3.11+ code under 150 lines that uses the latest versions of each library as of 2025-05-06, with clear comments marking where each external library is imported and utilized.","question_index":22,"modules":["starlite","pydantic","prisma","strawberry","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-nano","question":"Scenario: Design a minimal real-time chat service combining HTTP endpoints, data validation, async ORM persistence, WebSocket broadcasting, and JWT security using the latest Python libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Starlite to expose POST \/register and GET \/messages endpoints, leveraging its advanced dependency injection system.\n2. Define and serialize request\/response models with the latest Pydantic v2, employing its new `model_serializer` API.\n3. Persist User and Message models to SQLite asynchronously with the latest Ormar ORM, utilizing its built-in async migration feature.\n4. Implement a `\/ws` WebSocket route using the latest websockets library for real-time message broadcasting to connected clients.\n5. Secure both HTTP routes and WebSocket connections with JWT authentication via the latest PyJWT release.\n\nDeliverables:\n- A concise mapping of each numbered requirement to the specific library and feature used.\n- Complete, runnable Python 3.11+ code under 150 lines total, with clear comments indicating where each external library is imported and applied.","question_index":21,"modules":["starlite","pydantic","ormar","websockets","PyJWT"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-nano","question":"Scenario: You are building a Python microservice that accepts user-uploaded text files, asynchronously generates summaries with a local LLM, and streams live progress to clients via WebSocket.\n\nFunctional Requirements:\n1. Implement a FastAPI app (use the latest FastAPI as of 2025-05-06) with an `\/upload` POST endpoint that accepts plain-text file uploads.  \n2. Upon upload, enqueue a background job using arq (latest version) to process the file asynchronously.  \n3. In the arq worker, load a local LLM via llama-cpp-python (latest version) using its new GPU offloading API to generate a summary.  \n4. Configure python-socketio (latest version) as an ASGI middleware in FastAPI to broadcast progress and final summary events to connected WebSocket clients.\n\nDeliverables:\n- A bullet list of the chosen third-party libraries with their exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":46,"modules":["fastapi","uvicorn","arq","llama-cpp-python","python-socketio","python-dotenv"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: Build an end-to-end real-time customer feedback summarization service with live dashboard updates.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled JSON ingest endpoint using Litestar (latest as of 2025-05-06) with async support.\n2. Define a feedback model with Pydantic v2 and persist submissions into SQLite asynchronously via SQLModel (latest).\n3. On each new submission, call the OpenAI Python SDK (latest) asynchronously to generate a brief summary.\n4. Cache each summary in Redis via redis-om (latest) with a 1-hour TTL.\n5. Serve a live dashboard using Reflex (latest) that connects over WebSockets to display incoming summaries in real time.\n6. Use HTTPX (latest) with HTTP\/3 support for any external HTTP calls.\n\nDeliverables:\n\u2022 requirements.txt listing all dependencies pinned to the latest versions as of 2025-05-06  \n\u2022 A single Python 3.11+ script (under 150 lines) that implements the above, with clear comments indicating where and why each external library is used","question_index":61,"modules":["litestar","pydantic","sqlmodel","redis_om","openai","httpx","asyncio","json"],"modulenotfound":["json"],"modulenotfound_count":1,"module_count":8}
{"model":"gpt-4.1-nano","question":"Scenario: Develop a Python 3.11+ web application that provides secure collaborative document management with GraphQL, real-time WebSocket updates, AI-powered summarization, and on-demand PDF export.\n\nFunctional Requirements:\n1. Implement a GraphQL API for document queries and mutations using the latest \u201cariadne\u201d library with subscription support.\n2. Enable OAuth2 login with passwordless WebAuthn using the latest \u201cauthlib\u201d and \u201cfido2\u201d libraries.\n3. Provide real-time document update notifications over WebSocket leveraging FastAPI\u2019s built-in WebSocket support.\n4. Schedule asynchronous AI summarization tasks using the latest \u201cllama-cpp-python\u201d library and \u201cAPScheduler\u201d for background job management.\n5. Generate PDF exports of documents on demand using the latest \u201cplaywright\u201d headless browser library in Python.\n6. Persist documents and user metadata in PostgreSQL via the latest \u201ctortoise-orm\u201d with native async and Pydantic v2 model support.\n\nDeliverables:\n- A brief requirements list naming each third-party library and its version.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used. Use the latest library versions available as of 2025-05-06.","question_index":88,"modules":["ariadne","fastapi","uvicorn","authlib","fido2","llama-cpp-python","APScheduler","playwright","tortoise-orm","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1-nano","question":"Scenario: You are designing a Python microservice for ingesting user text, storing embeddings, and providing real-time, rate-limited on-device summaries via WebSockets.\n\nFunctional Requirements:\n1. HTTP POST \/ingest  \n   - Validate payload (id: str, text: str) using Pydantic v2 root validators  \n   - Compute embeddings on-device with llama-cpp-python and store (id, embedding, text) in ChromaDB  \n2. HTTP GET \/query  \n   - Accept query text, validate with Pydantic v2  \n   - Compute embedding via llama-cpp-python, retrieve top-5 nearest documents from ChromaDB, return metadata  \n3. WebSocket \/summarize  \n   - Client sends document id  \n   - Fetch text from ChromaDB metadata  \n   - Stream back a summary using llama-cpp-python streaming API  \n   - Enforce token-per-second rate limiting via aiolimiter  \n4. Lifespan events  \n   - On startup, initialize ChromaDB client, Llama model, and aiolimiter  \n   - On shutdown, cleanly close any resources  \n\nDeliverables:\n- requirements.txt listing the latest available versions as of today of:\n  \u2022 starlite  \n  \u2022 pydantic v2  \n  \u2022 chromadb  \n  \u2022 llama-cpp-python  \n  \u2022 aiolimiter  \n- main.py: a single, runnable Python 3.11+ script (under 150 lines) implementing all above, with clear comments marking where each external library is used.","question_index":8,"modules":["starlite","pydantic","chromadb","llama-cpp-python","aiolimiter"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-nano","question":"Scenario: Build a serverless Python application enabling real-time geolocated chat with sentiment analysis, dynamic map visualization, and PDF session reporting.\n\nFunctional Requirements:\n1. Real\u2010time WebSocket chat using the latest Litestar (released in the last 12 months) with asynchronous message persistence via a modern async ORM.\n2. Interactive geospatial mapping of incoming messages on the client side using the latest Python geospatial visualization library released in the last 12 months.\n3. On-the-fly sentiment analysis of each message using the latest Python sentiment analysis library released in the last 12 months.\n4. PDF generation of the entire chat session\u2014including message timeline, geolocation markers, and sentiment scores\u2014using the latest Python PDF library released in the last 12 months.\n5. Deployment configuration for AWS Lambda using the latest Serverless Framework plugin and AWS Lambda Powertools (all released in the last 12 months).\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":97,"modules":["litestar","tortoise-orm","httpx","textblob","fpdf2","pydeck"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: You are a senior software engineer tasked with building a real-time event analytics dashboard that ingests events, stores them, generates AI-driven summaries, caches counts, and streams updates to authenticated clients with full observability.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to implement a POST \/events REST endpoint for event ingestion.\n2. Validate incoming JSON payloads with the latest Pydantic v2 models.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise-ORM.\n4. Cache aggregated event counts in Redis via the latest aioredis.\n5. Generate streaming summaries of event batches with the latest OpenAI Python client.\n6. Stream real-time updates to clients over WebSocket using FastAPI.\n7. Secure all HTTP and WebSocket endpoints with JWT authentication via the latest fastapi-jwt-auth.\n8. Instrument HTTP, database, cache, and AI client calls with the latest OpenTelemetry for distributed tracing.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest library names and versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines integrating all subdomains, with clear comments indicating where each external library is used.","question_index":4,"modules":["fastapi","aioredis","openai","opentelemetry","tortoise-orm","pydantic"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: You\u2019re building a high-performance async task management API using the latest libraries in web framework, async ORM, caching, background processing, and observability as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement CRUD REST endpoints for a Task model using the latest starlite.\n2. Persist tasks in Postgres asynchronously with the latest Tortoise-ORM.\n3. Enqueue a background job on task creation using the latest Dramatiq with Redis broker.\n4. Cache GET \/tasks\/{id} responses in Redis via the latest redis.asyncio.\n5. Instrument all HTTP and database calls with OpenTelemetry using the latest opentelemetry-instrumentation-starlite.\n\nDeliverables:\n- A requirements.txt listing each external dependency with explicit versions.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":34,"modules":["starlite","tortoise-orm","redis","dramatiq","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-nano","question":"Scenario  \nYou are building an AI-powered document search microservice that ingests user uploads, indexes them with a GPU-accelerated vector store, caches recent queries, and exposes HTTP\/3 endpoints for real-time search.  \n\nFunctional Requirements  \n1. HTTP API (Starlite, latest)  \n   1.1 POST \/upload: receive plain-text documents, validate with Pydantic.  \n   1.2 GET \/query?q=\u2026: return top-k results for query.  \n2. Document Ingestion (llama-index, latest)  \n   2.1 Stream tokenized embeddings upon upload.  \n3. Vector Store (ChromaDB, latest)  \n   3.1 Use GPU-backed indexing for fast similarity search.  \n4. Caching Layer (redis-om, latest)  \n   4.1 Cache each query + results with TTL and Pydantic models.  \n\nDeliverables  \n\u2022 A requirements.txt listing the latest versions (as of 2025-05-06) of all third-party libraries, each released within the past 12 months.  \n\u2022 Complete, runnable Python 3.11+ code (\u2264150 lines) implementing all requirements.  \n\u2022 Clear inline comments indicating where and how each external library is used.","question_index":47,"modules":["starlite","pydantic","llama-index","chromadb","redis-om","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: You are a senior engineer building a high-throughput job processing microservice with a modern async Python stack.  \n\nFunctional Requirements:  \n1. Expose a POST \/jobs endpoint using the latest Starlite (as of 2025-05-06) to accept JSON job submissions.  \n2. Validate incoming JSON payloads with the latest Pydantic v2 models.  \n3. Persist job records to PostgreSQL via the latest prisma-client-py async ORM.  \n4. Enqueue and process jobs in the background using the latest Taskiq queue.  \n5. Expose a GET \/jobs\/{id}\/status endpoint to retrieve current job status.  \n6. Ensure all I\/O is non-blocking and use Python 3.11+ async\/await throughout.  \n\nDeliverables:  \n\u2022 A requirements list specifying the exact latest versions of Starlite, Pydantic v2, prisma-client-py, and Taskiq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ source code under 150 lines.  \n\u2022 Clear inline comments marking where each external library is used.","question_index":99,"modules":["starlite","pydantic","prisma","taskiq"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4.1-nano","question":"Scenario: Build a secure, real-time AI-powered chat web service.\n\nFunctional Requirements:\n1. Expose an async REST API using the latest FastAPI for user registration and login endpoints.  \n2. Implement JWT authentication with rotating refresh tokens via the latest Authlib.  \n3. Persist users and chat messages in SQLite using the latest SQLModel, leveraging its new async support.  \n4. Enable real-time chat notifications over WebSockets using the latest python-socketio and ASGI.  \n5. Integrate AI chat completions using the latest OpenAI Python client in an async background task.\n\nDeliverables:\n1. A requirements.txt listing exact, up-to-date package versions as of today\u2019s date.  \n2. A single Python 3.11+ file (<150 lines) containing complete, runnable code with clear comments marking where each external library is used.","question_index":91,"modules":["fastapi","sqlmodel","authlib","openai","socketio","PyJWT"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: You are a senior software engineer tasked with building an AI-powered collaborative document annotation API.\n\nFunctional Requirements:\n1. Secure user signup and login with JWT-based authentication using Starlite (latest) and Pydantic v2.\n2. REST endpoint to upload documents and persist metadata in Redis OM Python (latest).\n3. WebSocket route for real-time annotation broadcasting using Starlite\u2019s built-in WebSocket support.\n4. Background summarization of uploaded documents using llama-cpp-python (latest).\n\nDeliverables:\n- A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":1,"modules":["starlite","pydantic","redis-om","llama-cpp-python","jwt"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-nano","question":"Scenario: You are a senior engineer tasked with building a secure, high-throughput microservice for customer order ingestion, persistence, background processing, and caching.\n\nFunctional Requirements:\n1. Use FastAPI (latest as of 2025-05-06) with HTTP\/3 support to implement POST \/orders accepting JSON order data.\n2. Define an asynchronous Order model with SQLModel (latest) and connect to an async database engine (SQLite or PostgreSQL).\n3. Enqueue and process new orders via Arq (latest) background worker.\n4. Implement OAuth2 with JWT issuance and protected endpoints using Authlib (latest).\n5. Cache GET \/orders\/{id} responses in Redis using aiocache (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments marking where each external library is used.","question_index":68,"modules":["fastapi","sqlmodel","arq","authlib","aiocache"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-nano","question":"Scenario: Build an end-to-end Python web service that accepts data submissions, runs ML inference asynchronously, and pushes results back in real time.\n\nFunctional Requirements:\n1. Implement a POST \/submit endpoint using the latest FastAPI (ASGI, HTTP\/2) to accept JSON payloads.\n2. Validate and persist submissions into PostgreSQL via the latest SQLModel with asynchronous sessions.\n3. Dispatch an asynchronous classification task via the latest Celery (Redis broker) when new data arrives.\n4. In the Celery task, perform inference using the latest Hugging Face Transformers pipeline (e.g., text-classification).\n5. Cache inference results in Redis using the latest aioredis with a 5-minute TTL.\n6. Expose a WebSocket \/ws endpoint (FastAPI) to stream cached or newly computed results to subscribed clients.\n7. Instrument all HTTP requests and Celery tasks with the latest OpenTelemetry SDK for tracing.\n\nDeliverables:\n\u2022 requirements.txt listing \u201cthe latest\u201d versions of FastAPI, SQLModel, Celery, Redis, aioredis, Transformers, and OpenTelemetry as of 2025-05-06.  \n\u2022 A single Python 3.11+ file under 150 lines of runnable code. Include clear comments where each external library is used.","question_index":59,"modules":["fastapi","sqlmodel","redis","transformers","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-redis","celery"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-nano","question":"Scenario: You are tasked with building a real-time IoT analytics dashboard featuring anomaly detection, edge caching, vector search, and HTTP\/3 streaming.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of May 6, 2025) with HTTP\/3 and ASGI concurrency for API endpoints.\n2. Ingest IoT telemetry via real-time WebSocket streams using the latest websockets library.\n3. Perform transformer-based anomaly detection using the latest HuggingFace transformers and PyTorch 2.x.\n4. Store and query time-series embeddings in a vector database using the latest Pinecone Python client with HNSW indexing.\n5. Cache recent query results at the edge using the latest aioredis client with TTL invalidation.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the \u201clatest\u201d versions of all third-party libraries as of today.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":67,"modules":["fastapi","websockets","transformers","torch","pinecone-client","aioredis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: Build a high-performance, AI-powered web dashboard for real-time analytics and user collaboration.\n\nFunctional Requirements:\n1. Web API: Use the latest FastAPI (as of 2025-05-06) with async endpoints and the new dependency-injection system.\n2. Real-Time Collaboration: Implement bidirectional WebSocket chat using python-socketio v5.x+ for live user updates.\n3. AI Integration: Leverage the latest llama-cpp-python (or equivalent) to compute embeddings on incoming messages.\n4. Database Persistence: Store users and message history asynchronously with SQLModel v0.7+ (Pydantic v2) on SQLite.\n5. Caching Layer: Cache AI embedding results in Redis via aioredis v3.x+ to minimize redundant computation.\n6. Security: Protect all endpoints with OAuth2 JWT authentication using Authlib v1.2+.\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all third-party libraries as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, with clear inline comments indicating where each external library is used.","question_index":58,"modules":["fastapi","uvicorn","python-socketio","aioredis","sqlmodel","pydantic","authlib","llama-cpp-python"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-nano","question":"You are a senior software engineer tasked with building a modern image-processing web service end-to-end.\n\nFunctional Requirements:\n1. Implement a REST endpoint in Litestar for image uploads that converts incoming images to AVIF using the latest pillow-avif-plugin.\n2. Asynchronously persist image metadata (filename, upload timestamp, AVIF URL, dimensions) in MongoDB via the latest Beanie ODM.\n3. Index and cache metadata in Redis for fast lookup using the latest Redis-OM Python library.\n4. Expose a GraphQL API via the latest Litestar-GraphQL plugin to query images with filtering, pagination, and type validation.\n5. Broadcast real-time upload notifications over WebSocket connections using Litestar\u2019s built-in WebSocket support.\n\nDeliverables:\n- A concise list of the five functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines, using the latest Litestar, Litestar-GraphQL, Beanie, Redis-OM, and pillow-avif-plugin (as of May 6, 2025), with clear comments indicating where each external library is used.","question_index":89,"modules":["pillow-avif-plugin","beanie","motor","redis-om","graphene"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-nano","question":"You are a Senior Software Engineer building a high-throughput, real-time web application that ingests user events, enriches them with AI, persists outcomes, and serves analytics via a modern dashboard.\n\nFunctional Requirements:\n1. Implement JWT-based user authentication using the latest async web framework with schema-first validation (e.g., HTTP\/3 support).\n2. Expose a WebSocket endpoint for real-time event ingestion using the latest WebSocket library with per-message compression.\n3. Perform fully async CRUD on PostgreSQL using the latest ORM with native asyncio support and advanced connection pooling.\n4. Offload heavy computations to background workers via the latest task queue library supporting asyncio and result backends.\n5. Integrate an AI text-classification pipeline using the latest AI inference library with streaming outputs.\n6. Instrument the service with distributed tracing using the latest observability library supporting OpenTelemetry protocols.\n\nDeliverables:\n\u2022 requirements.txt listing the latest available versions of all dependencies (as of May 6, 2025).  \n\u2022 A single Python 3.11+ file under 150 lines that is complete and runnable, with clear comments marking where each external library is used.","question_index":12,"modules":["fastapi","uvicorn","fastapi_websocket_pubsub","databases","sqlalchemy","httpx","celery","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","pydantic","PyJWT","jwt"],"modulenotfound":[],"modulenotfound_count":0,"module_count":13}
{"model":"gpt-4.1-nano","question":"Scenario: Develop a high-performance asynchronous web service for ingesting user datasets, persisting metadata, launching background processing jobs, and tracing all operations end-to-end.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/upload endpoint using Starlite (latest) with async file streaming and dependency injection.\n2. Define and migrate a PostgreSQL table via Piccolo ORM (latest), storing upload metadata and task IDs, leveraging its GraphQL auto-schema feature.\n3. Enqueue and process tasks with Asynq (latest) in Redis, using priority queues, retry policies, and task result callbacks.\n4. Instrument incoming HTTP requests and background tasks with OpenTelemetry (latest), enabling W3C trace-context propagation across components.\n5. Provide an HTTP GET \/status\/{task_id} endpoint that queries Piccolo ORM for real-time task status.\n\nDeliverables:\n\u2022 The numbered functional requirements list (as above)  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, using the latest versions of all libraries as of 2025-05-06, with clear comments indicating where each external library is used.","question_index":43,"modules":["starlite","piccolo","asyncpg","opentelemetry","opentelemetry-instrumentation-starlette","opentelemetry-instrumentation-redis","uuid","uvicorn"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":8}
{"model":"gpt-4.1-nano","question":"As a senior software engineer, you are tasked with building a secure, real-time collaborative whiteboard microservice using cutting-edge Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3 API using the latest FastAPI (as of 2025-05-06) and uvicorn[standard] for low-latency streaming.\n2. Implement OAuth2 authorization with JWTs and refresh-token rotation using the latest Authlib.\n3. Persist whiteboard sessions and user metadata in PostgreSQL via the latest SQLModel with an async engine.\n4. Broadcast drawing events in real time over WebSockets using the latest websockets library with room-based pub\/sub.\n5. Vectorize stroke data into SVG on the fly using the latest Shapely or pyvips for advanced geometry operations.\n6. Upload periodic session snapshots to AWS S3 using the latest aioboto3 with asynchronous multipart uploads.\n\nDeliverables:\n- A requirements.txt listing the latest available versions of all external libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, fully runnable, with clear comments indicating where each third-party library is used.","question_index":25,"modules":["fastapi","uvicorn","authlib","sqlmodel","asyncpg","websockets","shapely","aioboto3"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-nano","question":"Scenario: Build a real-time sentiment analysis dashboard that fetches trending Twitter topics, analyzes sentiment, stores results, and serves updates via WebSockets.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to create both REST and WebSocket endpoints, leveraging its built-in asyncio and WebSocket broadcast features.\n2. Use the latest HTTPX to asynchronously fetch trending topics from the Twitter API v2 over HTTP\/3.\n3. Use the latest Hugging Face transformers to perform sentiment analysis via a pretrained pipeline.\n4. Use the latest SQLModel to define async SQLite models and persist topics with sentiment scores.\n5. Use the latest Jinja2 to render a minimal HTML dashboard for initial page load before WebSocket updates.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the latest versions of FastAPI, HTTPX, transformers, SQLModel, and Jinja2 as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":36,"modules":["fastapi","httpx","transformers","sqlmodel","jinja2","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Senior Software Engineer: You are developing an intelligent IoT monitoring dashboard that integrates real-time ingestion, streaming anomaly detection, WebSocket updates, dynamic UI rendering, and cloud infrastructure provisioning.\n\nFunctional Requirements:\n1. Build an ASGI web server using the latest ASGI framework (as of 2025-05-06) with HTTP\/2 and WebSocket endpoints for real-time data streaming.\n2. Connect asynchronously to an MQTT broker using the latest Python MQTT client library (as of 2025-05-06), subscribe to sensor topics, and relay messages to the server.\n3. Implement real-time anomaly detection on incoming sensor data using the latest Python anomaly detection library supporting incremental or deep learning.\n4. Serve a live, interactive front-end auto-generated from Python to JavaScript using the latest Python-to-JS UI framework.\n5. Provision and deploy all components via infrastructure-as-code using the latest Python IaC SDK on a major cloud provider.\n\nDeliverables:\n\u2022 Restate the numbered functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code integrating all five requirements, under 150 lines.  \n\u2022 Clear inline comments indicating where and how each external \u201clatest\u201d library (as of 2025-05-06) is imported and used.","question_index":49,"modules":["asyncio-mqtt","skynet-ia","pyjsui","fastapi"],"modulenotfound":["skynet-ia","pyjsui"],"modulenotfound_count":2,"module_count":4}
{"model":"gpt-4.1-nano","question":"Scenario: Build a real-time collaborative note-taking web application with authentication, persistence, caching, live updates, and scheduled cleanup.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (as of today) for note creation, retrieval, update, and deletion.\n2. Secure endpoints with JWT authentication using the latest PyJWT.\n3. Persist notes in a relational database via the latest SQLModel ORM, defining Note and User models.\n4. Enable real-time note synchronization between clients using WebSockets with the latest python-socketio.\n5. Cache frequently accessed note metadata in Redis using the latest redis-om to reduce DB load.\n6. Schedule automatic archiving of notes older than 30 days using the latest APScheduler.\n7. Document all endpoints with OpenAPI and include example request\/response schemas.\n\nDeliverables:\n- A numbered list of all third-party libraries (with versions) you\u2019ve chosen.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear comments indicating where each external library is used.","question_index":69,"modules":["fastapi","sqlmodel","jwt","socketio","redis","redis_om","apscheduler"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-nano","question":"Scenario: Build a high-performance asynchronous message processing service for real-time sentiment analysis, storage, notification, and search.\n\nFunctional Requirements:\n1. Implement an async HTTP API using the latest FastAPI (as of May 6, 2025) to accept user messages.\n2. Define and persist message models in PostgreSQL via the latest SQLModel ORM.\n3. Schedule and execute sentiment analysis in the background using the latest Dramatiq with RabbitMQ.\n4. After analysis, update the database and push real-time notifications to connected clients over WebSockets.\n5. Index messages and sentiment scores into Elasticsearch using the latest official Python client.\n6. Provide a search endpoint to query stored messages by keyword and sentiment.\n\nDeliverables:\n\u2022 A numbered list of the \u201clatest\u201d third-party libraries (with versions) used for each sub-domain.  \n\u2022 Complete, runnable Python 3.11+ application code under 150 lines, with clear comments indicating where each external library is used.","question_index":48,"modules":["fastapi","sqlmodel","dramatiq","elasticsearch","importlib_metadata"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-nano","question":"Scenario: Build a microservice that accepts text input, asynchronously generates summaries, persists data, and provides real-time metrics.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI 3, HTTP\/3 support) to implement POST \/summaries accepting JSON { \"text\": \"\u2026\" }, enqueue a background job via Arq, and return a job ID.\n2. Persist requests and summaries in SQLite asynchronously with the latest SQLModel (Pydantic V2 model support).\n3. Offload summarization jobs to Redis using the latest Arq, invoking the latest LangChain to call OpenAI for a concise summary.\n4. Expose GET \/summaries\/{id} to retrieve stored summaries from the database.\n5. Instrument request counts and job durations with the latest prometheus_client, exposing metrics on GET \/metrics.\n\nDeliverables:\n- A bulleted list of chosen \u201clatest\u201d libraries with exact version numbers.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear inline comments indicating where each external library is used.\n- Self-contained implementation: after installing dependencies, code runs without further setup.","question_index":95,"modules":["starlite","sqlmodel","arq","prometheus-client","openai","langchain"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: Create a serverless real-time analytics API that ingests WebSocket events, performs on-the-fly NLP summarization, indexes embeddings, exposes a GraphQL endpoint, and deploys to AWS Lambda.\n\nFunctional Requirements:\n1. Use the latest Litestar (as of 2025-05-06) to build an HTTP\/2 GraphQL endpoint with schema-based typing.\n2. Implement a real-time WebSocket consumer using wsproto or litestar-websockets for event ingestion.\n3. Offload text summarization to an asynchronous background task leveraging the latest Transformers library.\n4. Generate embeddings with the newest OpenAI Embeddings API or sentence-transformers and store them in the latest ChromaDB vector store.\n5. Secure all endpoints with passwordless authentication via the latest fastapi-users or equivalent.\n6. Package and deploy the entire application to AWS Lambda using the latest Mangum adapter.\n\nDeliverables:\n- A numbered list of chosen \u201clatest\u201d library versions (as of 2025-05-06) mapping to each requirement.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":77,"modules":["litestar","litestar-websockets","transformers","sentence-transformers","chroma-db","fastapi-users","mangum"],"modulenotfound":["litestar-websockets","chroma-db"],"modulenotfound_count":2,"module_count":7}
{"model":"gpt-4.1-nano","question":"Scenario: Develop a modern, high-performance AI-powered web service for real-time text processing.\n\nFunctional Requirements:\n1. Build a JSON REST API with Litestar using its latest dependency-injection and ASGI lifespan hooks.\n2. Integrate Tortoise ORM (async) to manage a PostgreSQL \u201cjobs\u201d table with migrations.\n3. Use llama-cpp-python to generate text embeddings or responses via a local LLaMA model.\n4. Expose a WebSocket endpoint with python-socketio (asyncio mode) for live client notifications.\n5. Cache recent embeddings in Redis using redis-py\u2019s asyncio client.\n6. Schedule background tasks (e.g., stale job cleanup) leveraging asyncio\u2019s latest task groups.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":60,"modules":["litestar","tortoise-orm","llama-cpp-python","python-socketio","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-nano","question":"Scenario: You are a senior software engineer building a real-time social media monitoring application that ingests tweets, analyzes sentiment, classifies images, displays live charts, and deploys as a serverless microservice using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Ingest tweets in real time from Twitter API v2 using the latest HTTPX async streaming client.\n2. Perform NLP sentiment analysis on tweet text with the latest Hugging Face Transformers pipeline.\n3. Classify embedded images using the latest Ultralytics YOLOv8 model.\n4. Render an interactive, live-updating dashboard using the latest Plotly Dash framework.\n5. Scaffold serverless deployment with the latest AWS Lambda Powertools for Python.\n\nDeliverables:\n- A requirements list of all external dependencies.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":98,"modules":["httpx","transformers","ultralytics","dash","plotly","aws-lambda-powertools"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: A knowledge management platform must allow users to upload PDFs, index their content, and perform real-time similarity searches via a high-performance API.\n\nFunctional Requirements:\n1. Implement a PDF upload REST endpoint using the latest unstructured-sdk to extract text.\n2. Use the latest ChromaDB Python client to generate and store embeddings for each document.\n3. Provide a search REST endpoint that queries ChromaDB for top-k similar documents given a text query.\n4. Create a real-time WebSocket over HTTP\/3 endpoint using the latest aioquic to stream incremental search results.\n5. Assemble the application as an asynchronous service using the latest Starlite web framework.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":6,"modules":["starlite","unstructured","chromadb","aioquic","starlette"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-nano","question":"Scenario: As a senior software engineer, build an asyncio-based Python microservice showcasing HTTP\/3 REST, GraphQL, async ORM, real-time WebSockets, and scheduled tasks using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST endpoint at `\/api\/items` using FastAPI (latest) and Hypercorn (latest) with HTTP\/3 support.\n2. Expose a GraphQL API at `\/graphql` using Strawberry (latest) with asynchronous resolvers fetching from the database.\n3. Integrate PostgreSQL via Tortoise ORM (latest), define an `Item` model, and apply programmatic migrations on startup.\n4. Provide WebSocket notifications at `\/ws\/updates` using python-socketio (latest), broadcasting item creation and deletion events.\n5. Schedule an hourly cleanup job with APScheduler (latest) in asyncio mode to remove `Item` records older than 7 days.\n\nDeliverables:\n- A `requirements.txt` listing the latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is imported and utilized.","question_index":66,"modules":["fastapi","starlette","strawberry","tortoise","hypercorn","socketio","apscheduler"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-nano","question":"Scenario: Develop a real-time AI-driven sentiment analysis web service that ingests text, processes it via an on-device LLM, persists results, and streams live analytics to clients.\n\nFunctional Requirements:\n1. Expose RESTful and WebSocket endpoints using the latest Litestar Python framework (released within the last 12 months).\n2. Perform local LLM inference for sentiment analysis with the latest llama-cpp-python library (released within the last 12 months).\n3. Asynchronously persist raw text and sentiment scores in MongoDB using the latest async MongoDB driver (released within the last 12 months) with code-first schema.\n4. Offload sentiment analysis to a distributed background worker using the latest Dramatiq (or equivalent) async task queue released in the last 12 months.\n5. Broadcast real-time sentiment updates to connected clients over WebSockets.\n6. Render an interactive dashboard in server-side templates with the latest Plotly Python library (released within the last 12 months).\n7. Instrument the entire application with distributed tracing via the latest OpenTelemetry Python SDK (released within the last 12 months).\n\nDeliverables:\n- A requirements list specifying each third-party library and its exact latest version as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines total.\n- Clear inline comments indicating where and how each external library is used.","question_index":5,"modules":["litestar","motor","dramatiq","plotly","opentelemetry"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":5}
{"model":"gpt-4.1-nano","question":"You are a senior software engineer tasked with building a real-time analytics dashboard with AI-driven anomaly alerts using the latest Python libraries as of 2025-05-06.  \n\nFunctional Requirements:  \n1. Implement asynchronous REST API endpoints using the latest FastAPI.  \n2. Add WebSocket-based real-time alert streaming using the latest websockets.  \n3. Persist incoming metrics in PostgreSQL via async ORM operations using the latest SQLModel.  \n4. Cache expensive queries in Redis for high throughput using the latest aioredis.  \n5. Perform streaming anomaly detection on incoming metrics using the latest river library.  \n6. Enable file uploads of detailed reports to S3-compatible storage via the latest minio SDK.  \n\nDeliverables:  \n\u2022 A numbered list of required Python libraries (with versions as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":20,"modules":["fastapi","uvicorn","websockets","sqlmodel","databases","aioredis","river","minio","pydantic","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1-nano","question":"Build a real-time analytics microservice that lets authenticated users ingest sales data, stores it in PostgreSQL, streams live updates to connected clients, and generates PDF reports with embedded charts.\n\n1. Implement OAuth2 JWT authentication using the latest fastapi-jwt-auth as of May 6, 2025.  \n2. Create async CRUD endpoints for sales records with SQLModel and asyncpg.  \n3. Broadcast new sales events over WebSocket using python-socketio (latest).  \n4. Generate interactive charts in HTML via Plotly (latest) and convert them to PDF using WeasyPrint (latest).  \n5. Expose generated PDFs via an endpoint, serving files asynchronously with aiofiles.  \n\nDeliverables:  \n- A numbered confirmation of the functional requirements above.  \n- A single Python 3.11+ file under 150 lines\u2014complete, runnable code\u2014with clear comments indicating where each external library is used. Use the latest versions of all third-party libraries as of today\u2019s date.","question_index":26,"modules":["fastapi","sqlmodel","asyncpg","fastapi-jwt-auth","socketio","plotly","weasyprint","aiofiles"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-nano","question":"Scenario: You are tasked with building a real-time collaborative task management web service for enterprise teams.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to expose HTTP endpoints and WebSocket routes, leveraging its new dependency injection and WebSocket lifetime management.\n2. Configure an async PostgreSQL database using the latest Tortoise-ORM with its auto-migration CLI and Pydantic model integration.\n3. Broadcast real-time task updates to connected clients via the latest python-socketio in async mode.\n4. Implement OAuth2 authentication with JWT access and refresh tokens using the latest Authlib, including token rotation.\n5. Cache sessions and publish update events using the latest aioredis async Redis client.\n6. Schedule and execute background jobs (e.g., daily summaries) with the latest APScheduler async support.\n7. Instrument request tracing and logging using the latest OpenTelemetry Python SDK, exporting to console.\n8. Auto-generate interactive API docs using FastAPI\u2019s built-in Swagger UI and Redoc.\n\nDeliverables:\n\u2022 A bullet list of selected libraries (with version numbers).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":15,"modules":["fastapi","tortoise-orm","python-socketio","authlib","aioredis","apscheduler","opentelemetry"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":7}
{"model":"gpt-4.1-nano","question":"Scenario: You are building a high-performance Python backend that ingests user articles, generates AI-powered summaries, stores and caches results, and exposes them via REST, GraphQL, and real-time channels with background processing.\n\nFunctional Requirements:\n1. Accept article submissions over a REST API using FastAPI and validate payloads with Pydantic v2.  \n2. Offload summarization to a background worker with Dramatiq using Redis as a broker.  \n3. Summarize articles via the latest OpenAI Python SDK and cache summaries with aiocache.  \n4. Persist articles and summaries in SQLite using SQLModel\u2019s async ORM features.  \n5. Expose stored summaries through a GraphQL endpoint implemented with Strawberry GraphQL.  \n6. Broadcast new summary notifications to connected clients over WebSockets with python-socketio.  \n\nDeliverables:\n\u2022 List of the latest third-party libraries (as of today) and their install commands.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":63,"modules":["fastapi","pydantic","sqlmodel","dramatiq","openai","aiocache","strawberry","socketio","sqlmodel","aiosqlite","strawberry-graphql"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"gpt-4.1-nano","question":"Scenario: Develop an internal Python microservice where authenticated users can submit text to be summarized by an LLM and receive real-time notifications via WebSockets.\n\nFunctional Requirements:\n1. Implement an asynchronous REST API using the latest Starlite framework (2025-05-06) leveraging its dependency injection and lifespan event system.\n2. Integrate OAuth2 Authorization Code Flow for user authentication with Authlib\u2019s async client libraries released in the last 12 months.\n3. Persist users, submissions, and summaries in PostgreSQL using the latest Piccolo ORM with async schema migrations and connection pooling.\n4. Summarize submitted text by calling OpenAI\u2019s GPT-4 Turbo via the newest openai Python SDK and its function-calling feature.\n5. Broadcast summary completion events over WebSockets using a modern Python websockets library (with HTTP\/2 support) released in the past year.\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":82,"modules":["authlib","piccolo","openai","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4.1-nano","question":"You are tasked with building a real-time analytics web dashboard that ingests streaming data, processes it, summarizes it with AI, secures user access, and visualizes results dynamically using the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Implement a REST API in Python 3.11+ with FastAPI (HTTP\/3 support via Hypercorn) to accept JSON event streams.\n2. Broadcast processed insights in real time over WebSocket using python-socketio.\n3. Buffer incoming events and generate streaming AI summaries using the latest LangChain and OpenAI Python SDK.\n4. Perform real-time aggregation and lazy analytics on events with Polars.\n5. Produce dynamic Plotly charts of key metrics and serve them as JSON for front-end rendering.\n6. Secure all endpoints and WebSocket connections with JWT-based auth via fastapi-users.\n\nDeliverables:\n\u2022 A numbered list of the above functional requirements.  \n\u2022 A single Python 3.11+ file (under 150 lines) that fulfills all requirements, complete and runnable. Include clear comments indicating where and how each external library is used.","question_index":84,"modules":["fastapi","python-socketio","polars","langchain","openai","plotly","PyJWT"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-nano","question":"Scenario: You are a senior software engineer tasked with building a next-generation real-time collaborative data analytics dashboard. Use the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a FastAPI server with a WebSocket endpoint for live data updates using the latest fastapi[full].\n2. Expose a type-safe GraphQL API at `\/graphql` using Strawberry with async schema stitching.\n3. Integrate AI-driven insights by generating embeddings on incoming data with llama-cpp-python.\n4. Perform vector similarity search over embeddings via the Weaviate Python client\u2019s async API.\n5. Secure all endpoints with passwordless authentication using py_webauthn for WebAuthn flows.\n6. Containerize the application using docker-py to build images and dynamically generate a docker-compose config.\n7. Instrument the app with async Prometheus metrics using prometheus-client.\n\nDeliverables:\n- A list of required libraries (with versions) using the latest releases as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":72,"modules":["fastapi","strawberry","weaviate-client","py_webauthn","docker","prometheus-client","llama-cpp-python"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-nano","question":"Scenario: Develop a real-time collaborative note-taking web application with AI-powered summarization.\n\nFunctional Requirements:\n1. Implement user authentication via OAuth2 password flow using the latest FastAPI features for secure login.  \n2. Expose a WebSocket endpoint for real-time CRDT-based document state synchronization using the newest FastAPI WebSocket enhancements.  \n3. Persist users and documents in PostgreSQL via the latest Piccolo ORM async API with JSONB indexing for rich querying.  \n4. Cache live presence and session metadata in Redis using aioredis with RedisJSON support for low-latency reads\/writes.  \n5. Summarize document edits on demand using the latest LangChain library and ChatOpenAI (GPT-4) integration.  \n6. Serve a minimal HTML frontend via Jinja2 templates that connects to the WebSocket and displays live updates.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of each dependency as of 2025-05-06.  \n\u2022 main.py: Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":52,"modules":["fastapi","uvicorn","piccolo","aioredis","langchain","openai","jinja2","python-dotenv","passlib","pydantic","httpx","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"gpt-4.1-nano","question":"Scenario: Senior Software Engineer, you are tasked with building a prototype real-time collaborative note-taking service using only the latest Python libraries released in the last 12 months.\n\nFunctional Requirements:\n1. Develop an asynchronous REST API for note CRUD operations using the latest Litestar framework.\n2. Persist notes in a SQL database via a newly released async ORM library.\n3. Cache frequent note queries in Redis with a recently published async Redis client.\n4. Enable real-time collaboration via WebSockets using a modern Python WebSocket library launched within the last year.\n5. Expose an AI-powered summarization endpoint using the latest OpenAI Python client.\n6. Render an interactive HTML dashboard showing active collaborators with a new Python-to-JS visualization tool.\n7. Instrument distributed tracing and structured logging using a cutting-edge observability library released in the past 12 months.\n\nDeliverables:\n- requirements.txt listing only libraries created in the last 12 months with exact versions.\n- A single Python 3.11+ file under 150 lines containing the complete, runnable application, with clear comments indicating where each external library is used.","question_index":41,"modules":["litestar","gino","aioredis","openai","pyvis","opentelemetry"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario:\nCreate an AI-powered article summarization service that accepts user-submitted text, stores it, generates summaries asynchronously via an AI API, caches results, and streams them back to clients in real time.\n\nFunctional Requirements:\n1. Implement a REST API using FastAPI (latest) with endpoints POST \/articles to submit text and GET \/stream\/{article_id} for server-sent events.\n2. Persist submissions in a database using SQLModel (latest) with an async SQLite or PostgreSQL engine.\n3. Orchestrate a background job queue using Celery (latest) with Redis as broker to process newly submitted articles.\n4. Perform summarization using the OpenAI Python SDK (latest) and its chat completion endpoint.\n5. Cache generated summaries in Redis via aioredis (latest) with a configurable TTL.\n6. Stream summaries to connected clients in real time using server-sent events via sse-starlette (latest).\n\nDeliverables:\n\u2022 A list of all third-party libraries (with versions pinned to the latest as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":64,"modules":["fastapi","sqlmodel","uvicorn","redis","openai","celery"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: Build a lightweight feedback API that records user comments, analyzes sentiment in real time, stores results, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (as of 2025-05-06) to implement asynchronous REST endpoints for submitting feedback and querying stored analytics.\n2. Use the latest SQLModel to define async ORM models and persist incoming feedback in an SQLite database.\n3. Use the latest PyWebSocketX to broadcast each new sentiment result over an ASGI WebSocket endpoint.\n4. Use the latest VulcanMind inference library to perform on-the-fly sentiment analysis of submitted feedback.\n5. Implement structured logging and error handling throughout the request, inference, database, and WebSocket flows.\n\nDeliverables:\n- A requirements list with exact version pins for Starlite, SQLModel, PyWebSocketX, and VulcanMind (latest as of 2025-05-06).\n- Complete, runnable Python 3.11+ code (\u2264150 lines) that integrates all four libraries with clear comments marking where each is used.","question_index":83,"modules":["starlite","sqlmodel","pywebsocketx","vulcan-mind"],"modulenotfound":["pywebsocketx","vulcan-mind"],"modulenotfound_count":2,"module_count":4}
{"model":"gpt-4.1-nano","question":"You are a senior software engineer building a high-performance, real-time analytics web service for IoT sensor data.  \n\nFunctional Requirements:  \n1. Expose HTTP REST ingest endpoints using the latest Nebula framework (released 2024-06) with its async dependency-injection.  \n2. Persist incoming readings to PostgreSQL via StarDB ORM (v1.0+, released 2024-08), leveraging async transactions and auto-migration.  \n3. Secure all endpoints with OAuth2 PKCE flows using AuthFusion (released 2024-12).  \n4. Broadcast processed data updates to dashboard clients via WaveSocket (released 2024-10) server-clustering.  \n5. Schedule hourly aggregation jobs using ChronoTask (released 2024-11) cron-style scheduler.  \n\nDeliverables:  \n\u2022 A requirements.txt listing the latest versions of all libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":92,"modules":["aiohttp","starDB","nebula","authfusion","wavesocket","chronotask"],"modulenotfound":["starDB","nebula","authfusion","wavesocket"],"modulenotfound_count":4,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: You are a senior software engineer building a high-throughput chat summarization microservice in Python 3.11+ using only the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Real-time WebSocket ingestion: implement a `\/ws` endpoint using fastapi-socketio (latest) with session storage in Redis via `redis.asyncio`.\n2. AI summarization: enqueue incoming messages in Arq (latest) and process them with the OpenAI Python SDK (latest) calling GPT-4 Turbo for summaries.\n3. Async data persistence: store raw messages and summaries in PostgreSQL using SQLModel + encode\/databases with the asyncpg driver.\n4. Dynamic admin dashboard: expose `\/admin` rendering Jinja2 templates enhanced with HTMX via fastapi-htmx (latest).\n5. Email alerts: send summary notifications asynchronously using FastAPI-Mail (latest).\n6. Containerization: provide a multi-stage Dockerfile based on `python:3.11-slim`.\n\nDeliverables:\n- A `requirements.txt` listing the exact versions of all third-party libraries (as of May 6, 2025).\n- A single `main.py` (under 150 lines) containing complete, runnable Python 3.11+ code with clear comments indicating where and how each external library is used.","question_index":23,"modules":["fastapi","fastapi_mail","fastapi_htmx","sqlmodel","databases","redis","openai","arq","fastapi_socketio","asyncio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1-nano","question":"Scenario: Build a compact Python microservice that ingests text and images, leverages AI summarization, real-time streams results, and stores everything in a modern database.\n\nFunctional Requirements:\n1. Use the latest Litestar (async web framework released Dec 2023) to define:\n   a. POST \/articles to receive JSON {\u201ctitle\u201d:\u2026, \u201ccontent\u201d:\u2026}.\n   b. WebSocket endpoint \/ws\/summary for streaming summaries.\n2. Connect asynchronously to PostgreSQL via the latest Prisma Python client (released Jul 2023), defining models for Article and Image.\n3. Summarize incoming content using the latest llama-cpp-python (released Sep 2023), utilizing its streaming interface for token-by-token output.\n4. Implement \/upload to accept multipart\/form-data image files, generate HEIF thumbnails with the latest pillow-heif (released Oct 2023), and persist file paths.\n5. Ensure all endpoints use async\/await, proper error handling, and pydantic-based validation.\n\nDeliverables:\n\u2013 A requirements.txt listing the latest library versions as of 2025-05-06.\n\u2013 A single, runnable Python 3.11+ script under 150 lines, with clear comments indicating where each external library is used.","question_index":16,"modules":["litestar","prisma","llama-cpp-python","pillow-heif","pydantic","aiofiles"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"You are tasked with building a prototype web service for authenticated semantic search with real-time visual feedback.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) server with:\n   a. an OAuth2 login flow using PyOIDC (latest) against an external Identity Provider  \n   b. a POST \/query endpoint to accept natural-language queries  \n   c. a WebSocket \/ws endpoint to stream real-time results  \n2. Upon authentication, embed incoming queries using sentence-transformers (latest) and perform vector similarity search in ChromaDB (latest).  \n3. As results arrive, push them incrementally over the WebSocket to clients.  \n4. Generate a Plotly (latest) bar chart of top-k result scores and serve it as an HTML snippet.  \n5. Ensure all components run under Python 3.11+, integrate only \u201cthe latest\u201d library releases as of 2025-05-06, and keep implementation concise.  \n\nDeliverables:\n\u2022 A list of chosen libraries with exact versions (the \u201crequirements\u201d).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":42,"modules":["fastapi","uvicorn","python-oauthlib","Authlib","pyOIDC","sentence-transformers","chromadb","plotly","httpx"],"modulenotfound":["python-oauthlib","pyOIDC"],"modulenotfound_count":2,"module_count":9}
{"model":"gpt-4.1-nano","question":"Scenario: You are building a high-throughput, real-time analytics web application for processing IoT sensor streams using the latest Python async frameworks and libraries.\n\nFunctional Requirements:\n1. Ingest JSON sensor events via a REST API built on FastAPI (use its latest HTTP\/3 support).\n2. Broadcast processed metrics to connected clients over WebSockets using python-socketio (asyncio).\n3. Offload CPU-intensive aggregation to background workers with Arq (latest Redis-backed queue).\n4. Persist time-series summaries in PostgreSQL via SQLModel (Pydantic v2-powered ORM).\n5. Cache hot query results in Redis using aioredis for sub-millisecond lookups.\n6. Expose tracing and Prometheus metrics using OpenTelemetry and prometheus-client.\n\nDeliverables:\n- A bullet list of the latest library dependencies as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments indicating where each external library is used.","question_index":40,"modules":["fastapi","uvicorn","python-socketio","arq","sqlmodel","aioredis","opentelemetry-api","opentelemetry-sdk","prometheus-client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1-nano","question":"You are a senior software engineer tasked with building a Python web application that integrates user authentication, real-time notifications, AI-based recommendations, and payment processing.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (2025-05-06) and Python 3.11 features (e.g. match-case).\n2. Secure all endpoints with OAuth2 JWT authentication via the latest fastapi-users, including email verification flows.\n3. Provide real-time notifications over WebSockets using the latest python-socketio async server.\n4. Handle one-time charges using the latest Stripe Python SDK via the Payment Intents API.\n5. Generate AI-driven product recommendations by invoking gpt-4-turbo through the latest openai library.\n6. Emit structured, context-rich logs for each operation using the latest structlog.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06  \n\u2022 Complete, runnable Python 3.11+ code (under 150 lines) with clear comments marking where each external library is used","question_index":96,"modules":["fastapi","uvicorn","fastapi-users","stripe","openai","structlog","PyJWT","passlib","aiohttp","socketio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1-nano","question":"Scenario: As a senior software engineer, design a high-performance microservice for real-time document collaboration with secure access, persistence, and cloud integration.  \n\nFunctional Requirements:  \n1. Implement an asynchronous JSON REST API using the latest Python web framework available as of 2025-05-06, supporting pagination and input validation.  \n2. Secure all endpoints with OAuth2 PKCE and JWT using the latest authentication library (released within the last 12 months).  \n3. Enable bidirectional real-time collaboration events over WebSocket using the latest WebSocket library (released within the last 12 months).  \n4. Persist document data in an async NoSQL database (e.g., MongoDB) using the newest driver supporting transactions and change streams.  \n5. Upload and serve document attachments to an S3-compatible object storage using the latest SDK with multipart upload support.  \n6. Cache frequently accessed documents in Redis with TTL and automatic invalidation using the newest Redis client library.  \n\nDeliverables:  \n- A requirements.txt listing \u201cthe latest\u201d versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":80,"modules":["fastapi","authlib","motor","boto3","redis","jwt"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: Build a high-performance Python microservice that combines cutting-edge web, real-time, AI and observability features into a single ASGI application.\n\nFunctional Requirements:\n1. HTTP\/3 ASGI server with automatic TLS and QUIC support using the latest HTTP\/3 library released within the past 12 months.  \n2. Real-time WebSocket endpoint that streams JSON events with zero-copy backpressure control using the newest streaming library created in the last year.  \n3. Semantic search pipeline: accept text uploads, generate embeddings, and perform vector similarity queries via the latest vector-store client released within 12 months.  \n4. On-device LLM inference endpoint that consumes streamed text segments and returns summaries using the most recent quantized model library published this year.  \n5. Distributed tracing and metrics export using the newest OpenTelemetry OTLP exporter released within the past 12 months.\n\nDeliverables:\n- A requirements list (requirements.txt) specifying the exact, latest versions of each third-party library as of 2025-05-06.  \n- A single Python 3.11+ script under 150 lines that implements all features above, with clear comments marking where each external library is used.","question_index":51,"modules":["fastapi","uvicorn","opentelemetry","sentence_transformers","transformers","streamy"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario  \nDevelop an end-to-end Python web service that provides passwordless WebAuthn authentication, stores and summarizes text inputs via OpenAI, broadcasts real-time summary events, and serves interactive charts.\n\nFunctional Requirements  \n1. Implement passwordless registration and login using the latest \u201cwebauthn\u201d library (FIDO2) with Litestar middleware.  \n2. Create REST endpoints under Litestar (latest version) for submitting text to summarize and retrieving stored summaries.  \n3. Use the latest SQLModel with an async PostgreSQL engine to persist input texts, summaries, timestamps, and user IDs.  \n4. Invoke the latest \u201copenai\u201d Python client to perform GPT-4 Turbo summarization with function-calling.  \n5. Serialize and broadcast new summaries over WebSockets using the latest \u201cmsgspec\u201d for high-performance JSON.  \n6. Serve an HTML page at \u201c\/charts\u201d that embeds an interactive Plotly chart of summary lengths over time.\n\nDeliverables  \n\u2022 A numbered list of all external libraries (with exact latest versions as of 2025-05-06) and their roles.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":13,"modules":["litestar","sqlmodel","openai","msgspec","plotly","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: As a senior software engineer, you must develop a real-time analytics dashboard backend that requires user authentication, data ingestion, live updates, background enrichment via an external API, and a web-based dashboard.\n\nFunctional Requirements:\n1. Implement user registration and login endpoints using JWT authentication with asymmetric keys via FastAPI-JWT-Auth (latest).\n2. Create a POST \/data endpoint to ingest JSON payloads and store them asynchronously in SQLite using SQLModel (latest).\n3. Broadcast each new record to connected WebSocket clients at \/ws using the latest websockets library, leveraging async broadcast.\n4. Schedule a background job every minute with arq (latest) to enrich stored records by streaming JSON from a third-party API via httpx (latest).\n5. Serve a simple HTML dashboard at GET \/dashboard that opens a WebSocket to \/ws and displays incoming records in real time.\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":75,"modules":["fastapi","sqlmodel","fastapi_jwt_auth","arq","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-nano","question":"Scenario: Design and implement a Python microservice that exposes HTTP endpoints, real-time WebSocket notifications, persistent storage, caching, scheduled tasks, and email alerts using the latest third-party libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use FastAPI (latest) with HTTP\/2 support to expose async REST endpoints for user registration and notification retrieval.\n2. Use SQLModel (latest) as an async ORM for PostgreSQL, defining typed models for users and notifications.\n3. Implement WebSocket broadcasting via FastAPI\u2019s WebSocket support to push new notifications to connected clients.\n4. Leverage Aioredis (latest) Redis Streams to cache session state and stream notification events.\n5. Schedule periodic database cleanup and cache eviction using APScheduler (latest) with its asyncio integration.\n6. Send email alerts for critical notifications using Aiosmtplib (latest) with STARTTLS.\n7. Perform external health-check API calls using HTTPX (latest) with async retries and timeouts.\n\nDeliverables:\n- A requirements list naming each library and its latest version as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, including clear comments that indicate where each external library is used.","question_index":44,"modules":["fastapi","uvicorn","sqlmodel","asyncpg","aioredis","apscheduler","aiosmtplib","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-nano","question":"Scenario: You are tasked as a senior software engineer to build an end-to-end collaborative document editing service with real-time updates, async persistence, GraphQL subscriptions, and cloud file storage.\n\nFunctional Requirements:\n1. Use the latest Litestar (first released within the last 12 months) to create an HTTP\/2 web server with WebSocket endpoints for real-time edit streams.\n2. Implement a GraphQL API with Strawberry GraphQL (latest) that supports queries, mutations, and live subscriptions for document change events.\n3. Persist documents in a PostgreSQL database via a fully async ORM of your choice that was first released in the last 12 months and supports automated migrations.\n4. Generate AWS S3 presigned URLs for resumable file attachments using aiobotocore (latest) with async upload\/download.\n5. Schedule periodic cleanup of stale sessions using an async task scheduler library first released within the last 12 months.\n\nDeliverables:\n- A brief list of the chosen third-party libraries (with exact versions) that meet the \u201cfirst released within the last 12 months\u201d requirement.\n- Complete, runnable Python 3.11+ code (under 150 lines) fulfilling the above requirements, with clear comments indicating where and how each external library is used.","question_index":78,"modules":["litestar","strawberry","tortoise-orm","aiobotocore","apscheduler"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-nano","question":"Scenario:\nYou are a senior software engineer tasked with building a real-time AI summarization microservice.\n\nFunctional Requirements:\n1. Implement an async RESTful POST endpoint `\/summarize` that accepts JSON `{ \"text\": \"<string>\" }` and returns a summary generated via `llama-cpp-python`.\n2. Implement a WebSocket endpoint `\/stream` that streams incremental summary tokens to the client as they are produced.\n3. Persist each original text and its summary in an async SQLite database using `SQLModel`.\n4. Cache recent summaries in Redis for 10 minutes with `aioredis` to avoid redundant computation.\n5. Expose Prometheus-compatible metrics on `\/metrics` (request counts, latency histograms, cache hit\/miss) via `prometheus-client`.\n6. Use a background task to warm up the Llama model at startup and to periodically clean up expired cache entries.\n\nDeliverables:\n- A restatement of the functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Code must use the latest versions of FastAPI, SQLModel, aioredis, llama-cpp-python, and prometheus-client as of 2025-05-06.\n- Include clear comments indicating where each external library is used.","question_index":54,"modules":["fastapi","sqlmodel","aioredis","prometheus-client","llama-cpp-python"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-nano","question":"You are a senior software engineer tasked with building a high-throughput real-time sentiment analysis microservice.\n\nFunctional Requirements:\n1. Use FastAPI (latest) to expose a POST \/analyze endpoint that accepts JSON text payloads and stores request metadata in PostgreSQL via SQLModel (latest).\n2. Secure all endpoints with OAuth2 JWT authentication using Authlib (latest), leveraging its PKCE support.\n3. Perform real-time sentiment analysis on incoming text using the latest Hugging Face Transformers pipeline with GPU acceleration via PyTorch (latest) and cache results in Redis using aiocache (latest).\n4. Broadcast live sentiment scores to connected clients over WebSockets using python-socketio (latest) and run the server on Uvicorn with HTTP\/3 support.\n5. Asynchronously upload any user-submitted audio snippets to S3-compatible storage via aiobotocore (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all external libraries as of 2025-05-06  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":62,"modules":["fastapi","sqlmodel","authlib","transformers","torch","redis","aiocache","python-socketio","uvicorn","aiobotocore"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4.1-nano","question":"Scenario: Build a high-performance Python web application combining HTTP\/3 REST, GraphQL, real-time WebSockets, async database operations, data analytics, AI summarization, and distributed tracing.\n\nFunctional Requirements:\n1. HTTP\/3 REST API: use FastAPI (latest as of 2025-05-06) with Uvicorn HTTP\/3 support to serve \/items endpoints.\n2. Async GraphQL API: use Strawberry (latest as of 2025-05-06) to expose a GraphQL schema with DataLoader for batch fetching.\n3. Async DB access: use SQLModel (latest as of 2025-05-06) with an async SQLAlchemy engine to perform PostgreSQL CRUD.\n4. Real-time WebSockets: use websockets library (latest as of 2025-05-06) to broadcast item updates at \/ws.\n5. Data analytics: use Polars (latest as of 2025-05-06) lazy API to compute summary statistics on items.\n6. AI summarization: use OpenAI Python SDK (latest as of 2025-05-06) to generate summaries of item descriptions.\n7. Observability: use OpenTelemetry Python SDK (latest as of 2025-05-06) to instrument all endpoints for distributed tracing.\n\nDeliverables:\n- requirements.txt listing exact latest versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments marking where each external library is used.","question_index":53,"modules":["fastapi","sqlmodel","strawberry-graphql","websockets","polars","openai","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-websockets"],"modulenotfound":["opentelemetry-instrumentation-websockets"],"modulenotfound_count":1,"module_count":10}
{"model":"gpt-4.1-nano","question":"Scenario: You are a senior software engineer building a real-time sentiment and OCR-enhanced geospatial chat dashboard for remote field teams.\n\nFunctional Requirements:\n1. Use FastAPI (latest version as of 2025-05-06) to implement an async REST API that serves a minimal HTML client.\n2. Implement bi-directional real-time chat via Socket.IO using python-socketio (latest) on the FastAPI server.\n3. Perform on-the-fly sentiment analysis of text messages using Hugging Face\u2019s transformers pipeline (latest) and broadcast sentiment scores.\n4. Accept image uploads from clients, extract embedded text with EasyOCR (latest), and inject the OCR result into the chat stream.\n5. Plot message geolocation metadata on a Folium (latest) map and serve it as an embeddable HTML snippet.\n\nDeliverables:\n- A requirements list enumerating the exact \u201clatest\u201d third-party libraries and versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":2,"modules":["fastapi","uvicorn","python-socketio","huggingface_hub","transformers","easyocr","folium"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-nano","question":"Scenario: Develop a Python microservice that supports AI-driven product imagery, real-time stock updates, secure user authentication, and checkout processing.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) app with JWT-based auth using Authlib (latest) and secure password hashing.  \n2. Define async ORM models for Users and Products with SQLModel (latest) on PostgreSQL, including migration setup.  \n3. On product creation, generate an AI image via Diffusers (latest) or Stability-SDK (latest) and expose it via a REST endpoint.  \n4. Provide a WebSocket endpoint for real-time inventory updates using Starlette\u2019s WebSocket support (latest).  \n5. Create a \/checkout POST endpoint integrating Stripe\u2019s Python SDK (latest) with webhook handling to confirm payments.\n\nDeliverables:\n\u2022 A bullet list of the \u201clatest\u201d libraries (with pip install specifiers as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":85,"modules":["fastapi","sqlmodel","authlib","stripe","diffusers"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-nano","question":"Scenario: You are developing a high-performance, real-time AI-powered web service for personalized content recommendations.\n\nFunctional Requirements:\n1. Implement a RESTful POST \/recommend endpoint using the latest Python web framework released within the last 12 months, accepting JSON input and returning JSON responses.\n2. Add a WebSocket \/ws endpoint for streaming live recommendation updates using the newest asyncio-based WebSocket library from the past year.\n3. Integrate semantic search over user profiles by connecting to a vector database via its newest Python client library released within the last 12 months.\n4. Schedule periodic model retraining in a background worker using the most recent Python task scheduler released in the last 12 months.\n5. Store and serve user-uploaded media files on cloud object storage using the latest cloud storage client library published within the past 12 months.\n\nDeliverables:\n\u2022 A requirements list (requirements.txt) specifying the exact library names and versions (all released within the last 12 months).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total, with clear comments marking where and why each external library is used.","question_index":71,"modules":["fastapi","uvicorn","websockets","pinecone-client","APScheduler","cloud-storage"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: Develop a high-performance Python microservice that ingests real-time social media posts via WebSockets, generates AI summaries, stores them in MongoDB Atlas, serves secured HTTP\/3 endpoints, and provides an interactive Streamlit dashboard for sentiment trends.\n\nFunctional Requirements:\n1. Initialize an asynchronous FastAPI app with HTTP\/3 (h2\/h3) and ASGI support.  \n2. Protect all HTTP endpoints with OAuth2 JWT using Authlib\u2019s latest OAuth library.  \n3. Consume a WebSocket stream of JSON posts from wss:\/\/stream.example.com asynchronously.  \n4. Summarize each post using OpenAI\u2019s latest Python SDK leveraging function-calling or embeddings.  \n5. Persist summaries and metadata in MongoDB Atlas via Motor (async driver).  \n6. Expose a secured REST endpoint `\/summaries` supporting pagination and filtering by sentiment.  \n7. Build a Streamlit dashboard that fetches `\/summaries` and visualizes sentiment trends over time with Plotly.\n\nDeliverables:\n- A requirements list naming the latest versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":28,"modules":["fastapi","uvicorn","authlib","motor","websockets","openai","streamlit","plotly","python-dotenv"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1-nano","question":"Scenario: Build a real-time HTTP\/3-powered AI chat service that streams user messages, generates LLM responses, persists embeddings in a vector DB, and exposes chat history via GraphQL.\n\nFunctional Requirements:\n1. HTTP\/3 streaming: use the latest aioquic (as of 2025-05-06) to handle bidirectional HTTP\/3 connections for chat.\n2. On-device LLM inference: integrate the latest llama-cpp-python for quantized model loading and response generation.\n3. Vector storage: use the latest chromadb Python client to embed each message (via llama-cpp-python\u2019s embed API) and store\/retrieve vectors.\n4. GraphQL API: implement an async GraphQL schema with the latest strawberry to query chat history from ChromaDB.\n5. Async orchestration: ensure end-to-end async I\/O under Python 3.11+, coordinating the QUIC server, LLM, and DB.\n6. Code constraints: keep the complete solution under 150 lines, include clear comments marking where each external library is used.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of aioquic, llama-cpp-python, chromadb, and strawberry (latest as of 2025-05-06)  \n\u2022 A single Python 3.11+ script (<150 lines) with runnable code and comments identifying each external library usage.","question_index":79,"modules":["aioquic","llama-cpp-python","chromadb","strawberry"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4.1-nano","question":"Scenario: Design and implement a high-performance real-time chat backend with semantic search and background processing.\n\nFunctional Requirements:\n1. Asynchronous REST API: use FastAPI (latest as of 2025-05-06) to expose user and message endpoints.\n2. HTTP\/3 WebSockets: implement real-time chat over WebSockets with Hypercorn\u2019s HTTP\/3 support.\n3. Async ORM persistence: store users and messages in PostgreSQL via SQLModel (async).\n4. Vector semantic search: index and query messages in Qdrant using the latest qdrant-client.\n5. Background sentiment analysis: enqueue and run analysis jobs with ARQ (async Redis-backed queue).\n6. JWT authentication: secure all endpoints with python-jose for JWT issuance and validation.\n7. Redis caching: cache active WebSocket connections or session data via redis.asyncio.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06  \n\u2022 A single Python 3.11+ file under 150 lines that:\n  \u2013 Is fully runnable  \n  \u2013 Shows clear comments where each external library is used  \n  \u2013 Integrates all above requirements into a cohesive application","question_index":11,"modules":["fastapi","starlette","sqlmodel","asyncpg","qdrant-client","arq","python-jose","redis","hypercorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1-nano","question":"Scenario: A fintech startup requires a live transaction monitoring service that ingests, processes, stores, and notifies users of suspicious activity in real time.\n\nFunctional Requirements:\n1. Build an HTTP endpoint to receive and stream-respond to JSON transaction payloads using the latest async web framework released in the last 12 months.\n2. Validate and serialize incoming data with the latest data validation library, enforcing strict type constraints and custom validators.\n3. Persist transactions asynchronously in a relational database using the latest async ORM library compatible with Python 3.11+.\n4. Offload CPU-intensive anomaly detection to a background worker queue using the latest task orchestration library.\n5. Broadcast real-time alerts to connected clients via WebSockets using the latest real-time communication library.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of the latest libraries (as of May 6, 2025)  \n\u2022 A single Python 3.11+ file under 150 lines, complete and runnable, with clear comments indicating where each external library is used.","question_index":39,"modules":["fastapi","pydantic","gino","celery"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4.1-nano","question":"Scenario: Build a real-time collaborative document editor service with AI-powered summarization, OAuth2 access control, WebSocket syncing, Redis caching, and NoSQL persistence.\n\nFunctional Requirements:\n1. Use the latest async Python web framework (as of 2025-05-06) to define RESTful CRUD endpoints for documents.\n2. Implement OAuth2 authorization with the latest Python OAuth library released within the last 12 months for secure user login and token issuance.\n3. Establish bi-directional real-time document synchronization over WebSockets using the newest real-time communication library (with built-in pub\/sub).\n4. Integrate the latest AI inference library published in the past year to generate live summaries of incremental document edits.\n5. Employ the latest Redis client library (released within the last year) to cache active sessions and coordinate pub\/sub channels for syncing.\n6. Persist document state and version history in a NoSQL database using the newest async database client released in the last 12 months.\n\nDeliverables:\n1. A numbered list of the chosen \u201clatest\u201d libraries (with a one-line description of their role).\n2. Complete, runnable Python 3.11+ code under 150 lines, including clear comments indicating where and why each external library is used.","question_index":86,"modules":["fastapi","fastapi-oauth2","python-socketio","openai","redis","motor"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: Develop a user registration microservice over HTTP\/3 that validates input, persists user data, and integrates with an external email verification API.\n\nFunctional Requirements:\n1. Expose a POST \/users endpoint using Starlite (latest) with HTTP\/3 support.  \n2. Define request and response schemas with Pydantic v2 (latest) for data validation.  \n3. Persist user records in PostgreSQL via the latest Prisma Python client, including model definitions and migrations.  \n4. Perform an asynchronous HTTP\/3 request to an external email verification service using httpx (latest), and incorporate its response.  \n5. Return a JSON payload containing the stored user data and email verification status.\n\nDeliverables:\n- A list of required third-party libraries with exact versions (as of today)  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":100,"modules":["starlite","pydantic","httpx","prisma"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4.1-nano","question":"Scenario: As a senior software engineer, build a modular web application that authenticates users with WebAuthn passkeys, streams real-time stock data, generates AI trading signals, processes payments, and delivers interactive dashboards.\n\nFunctional Requirements:\n1. Implement user authentication via WebAuthn using the latest pywebauthn library (as of 2025-05-06) with hardware security key support.\n2. Ingest and stream real-time stock price data over WebSockets using FastAPI and the latest Starlette HTTP\/2 push features.\n3. Generate AI-driven trading signals leveraging the latest OpenAI Python SDK\u2019s new function-calling interface.\n4. Integrate secure payment processing with the latest Stripe Python SDK supporting Payment Element and webhook handling.\n5. Serve interactive dashboards using the latest Plotly Dash or Reflex library for live data visualization and client-side callbacks.\n6. Add structured logging and Prometheus metrics using structlog and prometheus-client.\n\nDeliverables:\n- A concise mapping of each functional requirement to its implementation approach.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used and using the latest libraries available as of 2025-05-06.","question_index":56,"modules":["structlog","prometheus_client","fastapi","starlette","openai","stripe","dash","plotly"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-nano","question":"You are a senior software engineer tasked with architecting a small, high-performance async web application using only the latest Python libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Litestar framework (with native HTTP\/3 support) to define async REST endpoints for user signup and profile retrieval.  \n2. Integrate the latest Piccolo-ORM to define a PostgreSQL \u201cusers\u201d table and perform async CRUD operations.  \n3. Implement real-time notifications via WebSocket using the latest python-socketio in asyncio mode.  \n4. Schedule a daily summary email job with APScheduler\u2019s new AsyncIOScheduler and send mail via an async SMTP library.  \n5. Fetch external data during signup from a third-party API using HTTPX\u2019s HTTP\/2 client.\n\nDeliverables:\n\u2022 A requirements.txt listing exact package names and latest versions.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":29,"modules":["litestar","piccolo-orm","piccolo","asyncpg","python-socketio","httpx","apscheduler"],"modulenotfound":["piccolo-orm"],"modulenotfound_count":1,"module_count":7}
{"model":"gpt-4.1-nano","question":"Scenario: You are a senior software engineer building a high-performance real-time IoT analytics backend.\n\nFunctional Requirements:\n1. Expose an asynchronous HTTP API using FastAPI with WebSocket support to ingest and broadcast live sensor readings.\n2. Persist incoming data in PostgreSQL via SQLModel\u2019s async ORM and validate payloads with Pydantic v2 models.\n3. Enforce per-device rate limiting using Redis Streams and aioredis, leveraging RedisJSON for overflow handling.\n4. Secure all endpoints with OAuth2 passwordless login via FastAPI-Users, sending one-time codes.\n5. Schedule background aggregation jobs with Celery (beat scheduler) to compute rolling metrics and push updates over WebSockets.\n6. Provide a paginated historical metrics REST endpoint, caching hot queries in Redis for low-latency responses.\n\nDeliverables:\n- A list of required external libraries pinned to the latest versions available as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is utilized.","question_index":27,"modules":["fastapi","sqlmodel","redis","celery","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-nano","question":"Scenario: Build a real-time collaborative document review service with semantic search and AI summarization.\n\nFunctional Requirements:\n1. HTTP API (HTTP\/3) using the latest starlite (v1.x) to manage documents and user sessions.  \n2. Real-time updates over WebSockets for live editing and presence using starlite\u2019s WebSocket support.  \n3. Semantic indexing and search: on each document save, compute embeddings and store\/retrieve via the latest qdrant-client.  \n4. AI summarization endpoint: call an LLM using the latest llm-tools library to generate concise summaries on demand.  \n5. Background task scheduling with dramatiq (latest) to clean up stale sessions and regenerate indexes.\n\nDeliverables:\n\u2022 Reiterate the numbered requirements.  \n\u2022 A complete, runnable Python 3.11+ code file under 150 lines.  \n\u2022 Use the latest starlite, qdrant-client, llm-tools, and dramatiq libraries available as of 2025-05-06.  \n\u2022 Include clear comments indicating where and why each external library is used.","question_index":55,"modules":["starlite","qdrant-client","dramatiq"],"modulenotfound":[],"modulenotfound_count":0,"module_count":3}
{"model":"gpt-4.1-nano","question":"Scenario: You are building a high-performance, AI-driven text-generation microservice for a real-time web application.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/generate endpoint using FastAPI (latest 0.100+) for incoming requests.\n2. Validate and parse the request payload with Pydantic v2 (latest) leveraging its new ArgModel feature.\n3. Enqueue each generation request as an asynchronous job using ARQ (latest 0.7+) backed by Redis.\n4. In the ARQ worker, invoke vLLM (latest 0.7+) AsyncLLMClient to perform streaming text inference.\n5. Persist each request and its full response to PostgreSQL using SQLModel (latest 0.0.x) with SQLAlchemy 2.0 async engine.\n6. Expose a WebSocket \/ws\/stream endpoint in FastAPI to broadcast partial LLM outputs to clients in real time.\n\nDeliverables:\n\u2022 The above numbered requirements list.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":30,"modules":["fastapi","pydantic","arq","sqlmodel","asyncpg","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: You must build a Python microservice that fetches articles by URL, summarizes them with AI, stores results in Postgres, and provides REST, GraphQL, and WebSocket interfaces with background orchestration using only the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a POST \/submit REST endpoint using the latest Starlite framework (released within the last 12 months) over HTTP\/3 to accept JSON { \"url\": string }.  \n2. Fetch article content asynchronously using the latest httpx library and handle timeouts and redirects.  \n3. Summarize text with the latest HuggingFace Transformers v5 pipeline, leveraging GPU if available.  \n4. Persist the original URL and its summary in Postgres via Tortoise ORM v0.25 (async ORM released in the last 12 months).  \n5. Expose a \/graphql endpoint using Strawberry GraphQL (latest release) to query summaries by URL.  \n6. Provide a \/ws\/notifications WebSocket in Starlite to notify clients in real time when a summary is ready.  \n7. Orchestrate fetch-and-summarize tasks as background jobs using Prefect 2 (newly released workflow orchestrator).\n\nDeliverables:\n- A copy of the numbered functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Use only the latest versions of Starlite, httpx, Transformers v5, Tortoise ORM v0.25, Strawberry GraphQL, and Prefect 2 as of 2025-05-06.\n- Include clear comments indicating where and how each external library is used.","question_index":70,"modules":["starlite","tortoise-orm","prefect","httpx","transformers","strawberry","torch","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-nano","question":"Scenario: You are tasked with developing a next-generation chat analytics platform that seamlessly integrates HTTP\/3, JWT auth, real-time messaging, vector search, and on-device LLM inference.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) with HTTP\/3 support via Hypercorn to serve all endpoints.\n2. Implement JWT authentication with rotational refresh tokens using fastapi-users (latest).\n3. Provide a real-time WebSocket chat endpoint using fastapi-websocket-pubsub (latest) supporting pub\/sub channels.\n4. On each incoming chat message, generate vector embeddings using llama-cpp-python (latest) with an on-device streaming pipeline, and store them in ChromaDB (latest) with HNSW indexing.\n5. Expose a REST endpoint to query semantic similarity: accept a text query, compute embeddings, perform a ChromaDB search, and return the top-3 similar messages.\n6. Expose a streaming summarization endpoint that returns an on-the-fly summary of recent messages using llama-cpp-python\u2019s streaming completion interface.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements you\u2019ve implemented.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":93,"modules":["fastapi","fastapi-users","fastapi-websocket-pubsub","chromadb","llama-cpp","uvicorn","hypercorn"],"modulenotfound":["llama-cpp"],"modulenotfound_count":1,"module_count":7}
{"model":"gpt-4.1-nano","question":"Scenario: As a senior software engineer, develop a Python 3.11+ microservice that manages real-time collaborative whiteboard sessions, persists data, exposes REST and GraphQL APIs, generates session snapshots, integrates external metrics, and secures endpoints with OAuth2.\n\nFunctional Requirements:\n1. Implement session and user management REST endpoints using FastAPI (latest) with async event hooks and Pydantic V2 models.  \n2. Persist all data in PostgreSQL via Tortoise ORM (latest) using model lifecycle events.  \n3. Expose a GraphQL API with Strawberry GraphQL (latest) dataclass resolvers for session and user queries\/mutations.  \n4. Broadcast real-time drawing events using python-socketio (latest) over WebSockets.  \n5. Generate on-demand PNG snapshots of whiteboard state with Pillow-SIMD (latest) leveraging multi-threaded rendering.  \n6. Send session metrics asynchronously to an external analytics service using httpx (latest) with HTTP\/2 support.  \n7. Secure all endpoints with OAuth2 (authorization code + PKCE) using Authlib (latest).  \n\nDeliverables:\n- A numbered list of the functional requirements above.  \n- Complete, runnable Python 3.11+ code (\u2264 150 lines) integrating all requirements.  \n- Inline comments indicating where each external library is used.  \n- Use the latest stable releases of all libraries as of today\u2019s date.","question_index":50,"modules":["fastapi","pydantic","strawberry","python-socketio","Pillow-SIMD","httpx","authlib","tortoise-orm","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4.1-nano","question":"Scenario: You are a senior software engineer tasked with building a real-time analytics microservice that combines REST, GraphQL, persistent storage, and background processing using the latest Python ecosystem tools as of 2025-05-06.\n\nFunctional Requirements:\n1. Expose HTTP and WebSocket endpoints using the latest Litestar ASGI framework with annotated routing and streaming support.\n2. Implement a GraphQL API using the latest Strawberry GraphQL code-first schema builder with asyncio support.\n3. Persist and query analytics data in PostgreSQL via the latest Piccolo-ORM\u2019s async-first models and automatic migrations.\n4. Offload long-running analytics jobs to a Redis-backed async queue using the latest Arq library.\n5. Send WebSocket notifications to clients when background jobs complete.\n\nDeliverables:\n\u2022 A requirements list (pip install commands) specifying the latest versions of Litestar, Strawberry, Piccolo-ORM, and Arq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":65,"modules":["litestar","strawberry-graphql","piccolo-orm","arq"],"modulenotfound":["piccolo-orm"],"modulenotfound_count":1,"module_count":4}
{"model":"gpt-4.1-nano","question":"Scenario: You are a senior software engineer tasked with developing an end-to-end Python web application that leverages the latest libraries to showcase advanced API development, ORM integration, security, background processing, and real-time features.\n\nFunctional Requirements:\n1. Asynchronous REST API: implement CRUD endpoints for a `Task` model using FastAPI (latest version as of 2025-05-06).\n2. Data Modeling and Persistence: define `Task` ORM models with SQLModel (latest version) and connect to a SQLite or PostgreSQL database asynchronously.\n3. Security: secure all API endpoints with OAuth2 password flow using Authlib (latest version) to obtain and validate JWT tokens.\n4. Background Processing: create an asynchronous task queue with Arq (latest version) to process tasks in the background.\n5. Real-Time Notifications: establish a WebSocket endpoint using the websockets library (latest version) to push real-time task status updates.\n\nDeliverables:\n- The numbered list of functional requirements (as above).\n- Complete, runnable Python 3.11+ code under 150 lines. Use the latest versions of FastAPI, SQLModel, Authlib, Arq, and websockets as of 2025-05-06. Include clear comments indicating where and how each external library is used.","question_index":24,"modules":["fastapi","sqlmodel","authlib","starlette","arq","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: Build a real-time collaborative messaging microservice integrating graph storage, vector search, JWT authentication, and WebSockets using the latest Python libraries (as of May 6, 2025).\n\nFunctional Requirements:\n1. Authentication: secure all endpoints with JWTs using the latest TokenGuard library.\n2. Message Ingestion: implement POST \/messages to accept user messages; decode JWT, extract user ID.\n3. Graph Storage: store each message and its \u201cauthored_by\u201d relationship in a graph database via the latest GraphitORM.\n4. Vector Indexing & Search: generate text embeddings with the latest Morpher library, store them in its vector store, and expose GET \/search?q={text} to return top-5 similar messages.\n5. Real-Time Updates: broadcast new messages to connected clients over WebSockets using the latest Voila framework.\n\nDeliverables:\n1. A requirements list with each third-party library pinned to its latest version as of May 6, 2025.\n2. A single Python 3.11+ script (\u2264150 lines) implementing all endpoints, with clear comments marking where each external library is used.","question_index":35,"modules":["fastapi","uvicorn","tokenguard","graphitorm","morpher"],"modulenotfound":["tokenguard","graphitorm","morpher"],"modulenotfound_count":3,"module_count":5}
{"model":"gpt-4.1-nano","question":"You are tasked with building a high-performance text summarization service with real-time updates, persistent storage, and caching for internal research teams.\n\nFunctional Requirements:\n1. Implement OAuth2 password flow with JWT tokens for user authentication using the latest python-jose.\n2. Expose a POST \/summarize endpoint that uses the latest transformers library to perform text summarization.\n3. Provide a WebSocket at \/ws\/progress to stream summarization progress in real time using FastAPI\u2019s latest WebSocket support.\n4. Persist user requests and summaries in a SQLite database via the latest SQLModel ORM.\n5. Cache summary results in Redis for 5 minutes using the latest aioredis library to accelerate repeat requests.\n\nDeliverables:\n\u2022 A requirements list specifying the latest versions of FastAPI, transformers, python-jose, SQLModel, aioredis (as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":10,"modules":["fastapi","uvicorn","jose","sqlmodel","transformers","aioredis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"You are a senior software engineer.\n\nScenario: Build an end-to-end Python service for user profile CRUD with high-performance caching and real-time notifications.\n\nFunctional Requirements:\n1. Implement RESTful CRUD endpoints for user profiles using the latest FastAPI (as of 2025-05-06), leveraging its async lifespan context.\n2. Define an async PostgreSQL-backed User model with SQLModel (latest) using Relations and the new async engine.\n3. Cache GET \/users\/{id} responses in Redis with redis.asyncio (latest), utilizing cluster mode and automatic TTL invalidation on updates.\n4. Broadcast profile change events in real time over WebSocket using the websockets library (latest) with permessage-deflate compression.\n\nDeliverables:\n- A requirements.txt listing the latest versions of FastAPI, SQLModel, redis.asyncio and websockets as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":17,"modules":["fastapi","sqlmodel","databases","asyncpg","redis","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: You are a senior software engineer building a real-time sentiment-analysis web service for social media feeds.\n\nFunctional Requirements:\n1. Use the latest Python web framework (released within the last 12 months) to expose REST endpoints with dependency injection and OpenAPI support.\n2. Implement real-time websocket notifications for incoming sentiment updates using a recently released WebSocket library (last 12 months).\n3. Persist incoming posts and sentiment scores in a NoSQL database via its newest async client library (released within the last 12 months).\n4. Offload sentiment inference to a CPU-optimized ML library (first released within the last 12 months) with zero-copy or memory-efficient batch inference.\n5. Schedule periodic cleanup and summary-generation tasks using a lightweight scheduler library introduced within the last 12 months.\n6. Serve a minimal interactive dashboard at \u201c\/dashboard\u201d using a modern, Python-native plotting\/UI library created in the last 12 months.\n\nDeliverables:\n- A bullet list of all third-party libraries chosen, pinned to \u201clatest\u201d versions as of today\u2019s date.\n- A single, complete Python 3.11+ file (under 150 lines) that implements all requirements, with clear comments marking where each external library is used.","question_index":94,"modules":["fastapi","motor","apscheduler","websockets","tritonclient","pywebio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: As a senior software engineer, you must build a real-time analytics backend that ingests user events, persists them, and serves both REST and GraphQL clients with low latency and live updates.\n\nFunctional Requirements:\n1. Implement a POST \/events endpoint using the latest FastAPI (as of 2025-05-06), leveraging its new lifespan context and async router features.\n2. Validate incoming JSON payloads with Pydantic v2\u2019s new @model_validator decorator for strict schema enforcement.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise ORM, utilizing its recent bulk_create optimization.\n4. Expose a GraphQL query and mutation schema via the latest Strawberry GraphQL with code-first federation support.\n5. Broadcast new events to connected clients over WebSockets using the latest websockets library with per-message deflate compression.\n\nDeliverables:\n\u2022 A requirements.txt listing each third-party library pinned to its latest version as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments indicating where and how each external library is used.","question_index":19,"modules":["fastapi","uvicorn","pydantic","tortoise-orm","strawberry-graphql","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: You are a senior software engineer building a modern microblogging backend that supports authenticated posts, AI-generated images, real-time feed updates, and cloud storage integration.\n\nFunctional Requirements:\n1. Implement a REST API using a Python web framework released in the last 12 months (use the latest available as of 2025-05-06).\n2. Persist posts in a SQL database via an async ORM library created in the last 12 months, leveraging advanced async model features.\n3. Broadcast new posts to connected clients over WebSockets using a recently released high-performance async messaging library.\n4. Authenticate users with OAuth2 (Google) using the latest OAuth client library.\n5. Generate post images by calling Stable Diffusion through the most recent Python API client for AI image generation.\n6. Store and serve AI-generated images in AWS S3 using the newest AWS SDK for Python.\n7. Ensure all components run under Python 3.11+ and fit within 150 lines of code.\n\nDeliverables:\n\u2022 A numbered list of the specific libraries (with versions) you chose to satisfy each requirement.  \n\u2022 Complete, runnable Python 3.11+ code (<150 lines) combining all features.  \n\u2022 Clear inline comments marking where and why each external library is used.","question_index":7,"modules":["fastapi","starlette","authlib","tortoise-orm","nats-py","boto3","diffusers","torch"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-nano","question":"Scenario: You are a senior software engineer tasked with building a serverless real-time IoT analytics service that ingests sensor data, performs streaming aggregation, applies online anomaly detection, and streams results to clients.\n\nFunctional Requirements:\n1. Ingest JSON messages from an MQTT broker (\u201csensors\/+\/data\u201d) using the latest asyncio-based gmqtt library (as of 2025-05-06).  \n2. Stream-process incoming messages with streamz (latest version) to compute 1-minute tumbling-window averages for temperature and humidity.  \n3. Apply online anomaly detection on each windowed aggregation using River\u2019s latest HDBSCAN-based drift detector.  \n4. Persist raw messages and aggregated results into TimescaleDB via the asyncpg driver (latest version).  \n5. Expose a Python 3.11+ FastAPI application with a WebSocket endpoint for live metric and anomaly alert streaming.  \n6. Wrap the FastAPI app for AWS Lambda deployment using the latest mangum adapter.\n\nDeliverables:\n\u2022 A list of third-party dependencies pinned to \u201clatest\u201d versions as of 2025-05-06.  \n\u2022 A single, complete, runnable Python 3.11+ source file under 150 lines.  \n\u2022 Clear inline comments showing where each external library is used.  \n\u2022 The code must integrate all functional requirements and be deployable serverlessly on AWS Lambda.","question_index":38,"modules":["gmqtt","streamz","river","asyncpg","fastapi","mangum","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4.1-nano","question":"Scenario:\nYou are tasked with developing a lightweight AI-powered semantic search microservice in Python 3.11+.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI framework with HTTP\/3 support) to expose a POST \/search endpoint.  \n2. Validate and parse incoming JSON with the latest Pydantic v2\u2019s strict model parsing and Python pattern-matching features.  \n3. Asynchronously fetch text embeddings from OpenAI\u2019s embedding API using the latest HTTPX client with HTTP\/3.  \n4. Store and query vectors in a GPU-accelerated ChromaDB instance via the latest ChromaDB Python client.  \n5. Return the top 5 semantically related document IDs and similarity scores as a JSON response.\n\nDeliverables:\n\u2022 A numbered list confirming the above functional requirements.  \n\u2022 A single, complete, runnable Python 3.11+ source file (\u2264150 lines) that implements the microservice, with clear comments indicating where each external library is used.","question_index":3,"modules":["starlite","pydantic","httpx","chromadb"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4.1-nano","question":"Scenario: Build a real-time chat analysis service that ingests user messages, performs on-device LLM summarization and sentiment analysis, caches results, persists chat history, and broadcasts updates via WebSockets.\n\nFunctional Requirements:\n1. Expose a POST \/messages endpoint using FastAPI (latest) to receive JSON payloads with \u201cuser_id\u201d and \u201ctext\u201d.\n2. Perform sentiment analysis and summarization on each message using llama-cpp-python (latest) in an async background task.\n3. Cache sentiment results for identical texts in Redis via redis-py (latest) to avoid recomputation.\n4. Persist messages, sentiments, and summaries in PostgreSQL using SQLModel (latest version).\n5. Provide a WebSocket \/ws\/{user_id} endpoint (FastAPI) that broadcasts new summaries and sentiments to connected clients in real time.\n6. Secure all endpoints with OAuth2 password flow via Authlib (latest).\n\nDeliverables:\n\u2022 Restate the numbered requirements above.  \n\u2022 Provide complete, runnable Python 3.11+ code integrating the latest versions of FastAPI, llama-cpp-python, redis-py, SQLModel, and Authlib, under 150 lines total.  \n\u2022 Include clear comments indicating where and how each external library is used.","question_index":32,"modules":["fastapi","sqlmodel","redis","llama-cpp-python","authlib"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-nano","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python microservice for real-time IoT sensor analytics and visualization.\n\nFunctional Requirements:\n1. REST API: Use the latest asynchronous Python web framework available as of 2025-05-06 to expose CRUD endpoints with advanced dependency injection and OpenAPI schema generation.\n2. WebSockets: Integrate the latest real-time streaming library as of 2025-05-06 supporting protocol-level backpressure to push live sensor updates to clients.\n3. ML Inference: Embed an on-the-fly analytics model using the latest Python JIT-accelerated inference library as of 2025-05-06, auto-batching requests on CPU\/GPU.\n4. Graph Storage: Persist sensor relationships in a distributed graph database via the latest async driver as of 2025-05-06 with reactive query pipelining.\n5. Dashboard UI: Serve an interactive web dashboard using the latest server-driven UI component library as of 2025-05-06 for real-time charting and controls.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":14,"modules":["fastapi","websockets","torch","neo4j","dash"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4.1-nano","question":"You are a Senior Software Engineer tasked with building a high-performance, real-time analytics microservice using the latest asynchronous Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled ASGI endpoint using the latest Litestar and aioquic (as of 2025-05-06) for bi-directional streaming.\n2. Implement real-time GraphQL subscriptions with the latest Strawberry GraphQL to push analytics updates.\n3. Define domain models and perform asynchronous CRUD operations on Postgres using the latest SQLModel.\n4. Integrate a Redis cache layer with the latest aioredis for hot-path data retrieval.\n5. Instrument request handling and background tasks with OpenTelemetry SDK for distributed tracing.\n\nDeliverables:\n- requirements.txt listing the latest library versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments indicating where each external library is used.","question_index":90,"modules":["litestar","strawberry","sqlmodel","aioredis","opentelemetry"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":5}
{"model":"gpt-4.1-nano","question":"You are building a real-time collaborative code-review web application that leverages the latest AI, async, storage and security frameworks.\n\nFunctional Requirements:\n1. Implement a FastAPI 0.95+ server with async endpoints and Jinja2 templates for the main UI.\n2. Enable real-time code editing and review comments via WebSockets using the latest websockets library as of 2025-05-06.\n3. On each code submission, stream analysis from OpenAI\u2019s GPT-4o API (or equivalent newest model) and display suggestions in real time.\n4. Persist code snapshots and review metadata in Pinecone\u2019s newest Python client (vector storage) for similarity search.\n5. Secure all HTTP and WS routes with JWT authentication using the latest PyJWT release.\n6. Add structured logging and a \/healthz endpoint using structlog for observability.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total.  \n\u2022 Clear comments indicating where and why each external library is used.","question_index":37,"modules":["fastapi","uvicorn","jinja2","websockets","openai","pinecone-client","PyJWT","structlog"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-nano","question":"Scenario: Build a real-time Q&A web application that ingests documents, generates embeddings, serves semantic search, supports live WebSocket notifications, and exposes observability metrics.\n\nFunctional Requirements:\n1. Use the latest FastAPI (>=0.100) to implement asynchronous REST endpoints for document ingestion and search, plus a WebSocket endpoint leveraging its new lifecycle event hooks.\n2. Upon document ingestion, asynchronously generate embeddings with the latest llama-cpp-python (v0.2+) using 4-bit quantization and store vectors in Pinecone (latest) using hybrid search.\n3. Implement a `\/search` endpoint that queries Pinecone for nearest neighbors based on user input and returns ranked results.\n4. Schedule periodic re-indexing tasks with the latest arq (v1.10+), utilizing its async Redis job scheduler.\n5. Integrate OpenTelemetry Python SDK (v1.21+) for auto-instrumentation, exporting traces and metrics to Prometheus.\n\nDeliverables:\n- A requirements.txt listing the latest libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":33,"modules":["fastapi","pydantic","pinecone-client","llama-cpp-python","arq","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","opentelemetry-exporter-prometheus","prometheus-client","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"gpt-4.1-nano","question":"Scenario: As a senior software engineer, you must build a modular Python web application that serves real-time, ML-enhanced classification results with persistent storage and observability.\n\nFunctional Requirements:\n1. API Layer: Use the latest ASGI web framework available as of 2025-05-06 that supports HTTP\/3 and automatic OpenAPI generation to expose a POST \/classify endpoint.  \n2. Database Layer: Connect asynchronously to PostgreSQL using the latest async driver with statement pooling to record incoming requests and classification results.  \n3. Real-time Pub\/Sub: Implement server-side WebSocket broadcasts of classification results using the latest message queue library offering at-least-once delivery semantics.  \n4. ML Inference: Integrate on-demand image classification via the latest lightweight, GPU-accelerated inference engine released in the past 12 months.  \n5. Observability: Instrument distributed tracing and metrics collection using the latest OpenTelemetry-compatible tracing library.\n\nDeliverables:\n- A list of the latest library dependencies as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":18,"modules":["fastapi","uvicorn","asyncpg","nats","onnxruntime","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4.1-nano","question":"Scenario: As a senior software engineer, build a high-performance microblogging API using the latest Python libraries for REST, validation, async database operations, caching, background tasks, and real-time streaming.\n\nFunctional Requirements:\n1. Implement a user signup endpoint in FastAPI (>=0.100.0) using Pydantic v2 root_validator for advanced schema validation.  \n2. Create and list posts asynchronously in PostgreSQL via SQLModel\u2019s latest async engine.  \n3. Maintain a cache of the 10 most recent posts in Redis using redis-py (>=5.0) asyncio client, updating on post creation.  \n4. Offload image thumbnail generation to an async Dramatiq (>=2.0) background task triggered on new post with image.  \n5. Provide a WebSocket endpoint in FastAPI to broadcast newly created posts to connected clients in real time.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library (FastAPI, Pydantic v2, SQLModel, redis-py asyncio, Dramatiq) is used. Use the latest available versions of all libraries as of 2025-05-06.","question_index":87,"modules":["sqlmodel","redis","dramatiq"],"modulenotfound":[],"modulenotfound_count":0,"module_count":3}
{"model":"gpt-4.1-nano","question":"Scenario: You are building a real-time news aggregation microservice with live client updates.\n\nFunctional Requirements:\n1. Develop a REST API using Litestar (the latest version as of 2025-05-06) leveraging its OpenAPI v3.1 auto-generation feature.\n2. Asynchronously fetch and parse external RSS feeds over HTTP\/3 using HTTPX AsyncClient (the latest), then cache results in aiocache (the latest) with a TTL.\n3. Validate and serialize each article using Pydantic v2 (the latest) functional-style models to ensure data integrity.\n4. Expose a WebSocket endpoint using the websockets library (the latest) with permessage-deflate compression for real-time article broadcasting.\n5. Schedule periodic background tasks with Arq (the latest) to re-poll feeds every 5 minutes.\n\nDeliverables:\n- A list of the chosen libraries and their latest versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":31,"modules":["litestar","httpx","aiocache","pydantic","websockets","arq"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: Build a real-time collaborative note-taking microservice integrating REST, database persistence, caching, GraphQL, WebSockets, and background tasks.\n\nFunctional Requirements:\n1. Expose REST endpoints using FastAPI (latest) for creating, updating and retrieving notes; leverage Pydantic v2\u2019s new validation decorators.\n2. Persist notes asynchronously in SQLite via SQLModel (latest) with SQLAlchemy 2.0-style typing and async engine.\n3. Broadcast note creation and updates over WebSockets with FastAPI\u2019s WebSocket support to subscribed clients in real time.\n4. Cache note retrieval results in Redis using aioredis (latest), implement TTL and cache invalidation on updates.\n5. Provide a GraphQL schema and endpoint using Strawberry (latest) to query and filter notes with federation-style resolvers.\n6. Schedule a daily summary email using FastAPI BackgroundTasks and aiosmtplib (latest) for asynchronous SMTP delivery.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- A single Python 3.11+ source file under 150 lines that implements all requirements, with clear comments marking where each external library is used.","question_index":73,"modules":["fastapi","sqlmodel","strawberry-asyncpg","aioredis","apscheduler","aiosmtplib"],"modulenotfound":["strawberry-asyncpg"],"modulenotfound_count":1,"module_count":6}
{"model":"gpt-4.1-nano","question":"Scenario: You are a senior software engineer building a proof-of-concept Python service that ingests user text, generates embeddings and classifications via an LLM, stores them, and streams nearest-neighbor results in real time.\n\nFunctional Requirements:\n1. Implement a JWT-secured POST \/ingest endpoint using the latest async Python web framework as of 2025-05-06 (e.g., Litestar).  \n2. In \/ingest, accept JSON `{ \"text\": \"...\" }`, call a state-of-the-art LLM (via the latest Transformers or LangChain client) to produce both an embedding vector and a classification label.  \n3. Persist the embedding vector in a vector database using the newest ChromaDB client.  \n4. Log the original text and its classification into a relational database through the latest async ORM (e.g., Prisma Python).  \n5. Expose a GET \/stream endpoint that streams nearest-neighbor matches for incoming embeddings over server-sent events using the newest SSE library (e.g., httpx-sse or framework-native support).  \n6. Target Python 3.11+, include type hints, and keep all code under 150 lines.\n\nDeliverables:\n\u2022 A `requirements.txt` (or equivalent) listing each library with exact versions (latest as of 2025-05-06).  \n\u2022 A single Python module (\u2264150 lines) that meets all requirements, with clear comments marking where and why each external library is used.","question_index":81,"modules":["langchain","chromadb","prisma","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4o","question":"Scenario: As a senior software engineer, build a high-performance Python web application integrating real-time streaming, GraphQL, async ORM, JWT security, and caching using the latest third-party libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. HTTP & WebSocket server with the latest Litestar, utilizing its new streaming response API for real-time event delivery.\n2. GraphQL API with the latest Strawberry-GraphQL, defining queries, mutations, and subscription streams for live data updates.\n3. Async database models and CRUD operations using the latest Piccolo ORM\u2019s Python 3.11+ async support and built-in migration tooling.\n4. JWT authentication via the latest python-jose, supporting secure login, token rotation, and injection of user context into requests.\n5. Redis-backed caching with the latest aioredis to cache GraphQL query results and automatically invalidate on data changes.\n6. Trigger real-time client notifications over WebSockets when database events occur, leveraging Litestar\u2019s WebSocket routing.\n\nDeliverables:\n\u2022 A numbered list of the final functional requirements.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":57,"modules":["litestar","strawberry-graphql","piccolo","python-jose","aioredis","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: You are tasked with building a modular Python 3.11+ microservice that delivers real-time chat with semantic search and scheduled maintenance, leveraging only the latest libraries released within the past 12 months.\n\nFunctional Requirements:\n1. Implement HTTP REST endpoints for posting and retrieving chat messages using Litestar (latest release).  \n2. Establish a WebSocket chat channel that broadcasts new messages to all connected clients via the official Litestar WebSocket plugin (latest).  \n3. Persist each message to MongoDB with Beanie ODM (latest) and simultaneously index its vector embedding into Qdrant using qdrant-client (latest).  \n4. Expose a GraphQL query endpoint that performs semantic search over stored messages via strawberry-graphql (latest).  \n5. Schedule a daily background job to purge messages older than 30 days using Dramatiq (latest).\n\nDeliverables:\n- A requirements.txt (or pyproject.toml) listing all dependencies with exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments marking where and how each external library is used.","question_index":76,"modules":["litestar","beanie","qdrant-client","strawberry-graphql","dramatiq","pymongo","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python web service that combines asynchronous REST & GraphQL APIs, real-time notifications, persistent storage, caching, and AI-driven text summarization.\n\nFunctional Requirements:\n1. Implement async HTTP endpoints with FastAPI (latest as of 2025-05-06) using Python 3.11 structural pattern matching and Pydantic v2 dataclass models.\n2. Provide a GraphQL endpoint powered by Strawberry GraphQL v1.0+ with schema federation support.\n3. Persist and query data in PostgreSQL via the latest Tortoise-ORM with asyncio-optimized connection pooling.\n4. Push real-time notifications over WebSockets using python-socketio latest async server mode.\n5. Summarize user-submitted text on demand using the latest llama-cpp-python library with optional GPU offload.\n6. Cache frequent database queries in Redis using aioredis latest cluster-mode support.\n\nDeliverables:\n- A requirements list specifying exact versions of all third-party libraries.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":74,"modules":["fastapi","pydantic","strawberry-graphql","tortoise-orm","python-socketio","llama-cpp-python","aioredis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"Scenario: Build a real-time collaborative note-taking service that supports user auth, live updates, AI summaries, PDF export, and caching using the latest Python web stack.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST API for user signup\/login with FastAPI (latest) using OAuth2 password flow.\n2. Persist users and notes in PostgreSQL via SQLModel (latest async features).\n3. Create a WebSocket endpoint broadcasting live note edits using the newest websockets library.\n4. Add an endpoint `\/summarize` that calls an external AI summarization API via httpx\u2019s HTTP\/3 client (latest).\n5. Provide a `\/export\/{note_id}` endpoint that generates a PDF of the note using Playwright Python (latest).\n6. Cache active JWT tokens in Redis Streams using aioredis v2.x.\n7. Emit structured JSON logs via loguru (latest advanced configuration).\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- A complete, runnable Python 3.11+ codebase under 150 lines, with clear comments indicating where each external library is used.","question_index":9,"modules":["fastapi","sqlmodel","websockets","httpx","playwright","aioredis","loguru","jwt"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4o","question":"You are a senior software engineer tasked with building a minimal Python web service for text ingestion, semantic search, and AI-powered summarization using the latest libraries.\n\nFunctional Requirements:\n1. HTTP API: use the latest Starlite framework (as of 2025-05-06) to expose async endpoints.\n2. Text Storage: on POST \/submit, accept JSON {\u201ctext\u201d: \u2026}, store original text and metadata in Redis via the latest redis-om Python client with async HashModel.\n3. Embedding: upon submission, generate a vector embedding using the latest ChromaDB Python client\u2019s async API and upsert into its vector store.\n4. Semantic Search: implement GET \/search?query=\u2026 that computes a query embedding via ChromaDB and returns top 3 matching texts from Redis.\n5. Summarization: implement GET \/summarize\/{id} to retrieve stored text by ID and produce a concise summary using the latest llama-cpp-python streaming API.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries (as of 2025-05-06).  \n\u2022 A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments indicating where and how each external library is used.","question_index":45,"modules":["starlite","redis-om","chromadb","llama-cpp-python"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4o","question":"Scenario: Senior software engineer tasked with developing a real-time GraphQL web service that validates incoming event data, persists it to PostgreSQL, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI web framework) as of 2025-05-06 to implement an async HTTP POST endpoint at `\/events`.\n2. Validate incoming JSON payloads with Pydantic v2 (latest) models, including at least one custom field validator.\n3. Persist validated events to PostgreSQL using the latest Prisma Python client (type-safe generated models and migrations).\n4. Expose a GraphQL API with the latest Strawberry GraphQL library, supporting queries for event history and subscriptions for real-time updates.\n5. Configure WebSocket support in Starlite to broadcast new events to all GraphQL subscription clients.\n\nDeliverables:\n- A brief recap of the functional requirements.\n- Complete, runnable Python 3.11+ code under 150 lines that uses the latest versions of each library as of 2025-05-06, with clear comments marking where each external library is imported and utilized.","question_index":22,"modules":["starlite","pydantic","prisma","strawberry"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4o","question":"Scenario: Design a minimal real-time chat service combining HTTP endpoints, data validation, async ORM persistence, WebSocket broadcasting, and JWT security using the latest Python libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Starlite to expose POST \/register and GET \/messages endpoints, leveraging its advanced dependency injection system.\n2. Define and serialize request\/response models with the latest Pydantic v2, employing its new `model_serializer` API.\n3. Persist User and Message models to SQLite asynchronously with the latest Ormar ORM, utilizing its built-in async migration feature.\n4. Implement a `\/ws` WebSocket route using the latest websockets library for real-time message broadcasting to connected clients.\n5. Secure both HTTP routes and WebSocket connections with JWT authentication via the latest PyJWT release.\n\nDeliverables:\n- A concise mapping of each numbered requirement to the specific library and feature used.\n- Complete, runnable Python 3.11+ code under 150 lines total, with clear comments indicating where each external library is imported and applied.","question_index":21,"modules":["starlite","pydantic","jwt","sqlalchemy","ormar","websockets","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"Scenario: You are building a Python microservice that accepts user-uploaded text files, asynchronously generates summaries with a local LLM, and streams live progress to clients via WebSocket.\n\nFunctional Requirements:\n1. Implement a FastAPI app (use the latest FastAPI as of 2025-05-06) with an `\/upload` POST endpoint that accepts plain-text file uploads.  \n2. Upon upload, enqueue a background job using arq (latest version) to process the file asynchronously.  \n3. In the arq worker, load a local LLM via llama-cpp-python (latest version) using its new GPU offloading API to generate a summary.  \n4. Configure python-socketio (latest version) as an ASGI middleware in FastAPI to broadcast progress and final summary events to connected WebSocket clients.\n\nDeliverables:\n- A bullet list of the chosen third-party libraries with their exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":46,"modules":["fastapi","fastapi-concurrency","arq","llama-cpp-python","python-socketio"],"modulenotfound":["fastapi-concurrency"],"modulenotfound_count":1,"module_count":5}
{"model":"gpt-4o","question":"Scenario: Build an end-to-end real-time customer feedback summarization service with live dashboard updates.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled JSON ingest endpoint using Litestar (latest as of 2025-05-06) with async support.\n2. Define a feedback model with Pydantic v2 and persist submissions into SQLite asynchronously via SQLModel (latest).\n3. On each new submission, call the OpenAI Python SDK (latest) asynchronously to generate a brief summary.\n4. Cache each summary in Redis via redis-om (latest) with a 1-hour TTL.\n5. Serve a live dashboard using Reflex (latest) that connects over WebSockets to display incoming summaries in real time.\n6. Use HTTPX (latest) with HTTP\/3 support for any external HTTP calls.\n\nDeliverables:\n\u2022 requirements.txt listing all dependencies pinned to the latest versions as of 2025-05-06  \n\u2022 A single Python 3.11+ script (under 150 lines) that implements the above, with clear comments indicating where and why each external library is used","question_index":61,"modules":["sqlmodel","httpx","openai","pydantic","litestar","redis-om","reflex"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"Scenario: Develop a Python 3.11+ web application that provides secure collaborative document management with GraphQL, real-time WebSocket updates, AI-powered summarization, and on-demand PDF export.\n\nFunctional Requirements:\n1. Implement a GraphQL API for document queries and mutations using the latest \u201cariadne\u201d library with subscription support.\n2. Enable OAuth2 login with passwordless WebAuthn using the latest \u201cauthlib\u201d and \u201cfido2\u201d libraries.\n3. Provide real-time document update notifications over WebSocket leveraging FastAPI\u2019s built-in WebSocket support.\n4. Schedule asynchronous AI summarization tasks using the latest \u201cllama-cpp-python\u201d library and \u201cAPScheduler\u201d for background job management.\n5. Generate PDF exports of documents on demand using the latest \u201cplaywright\u201d headless browser library in Python.\n6. Persist documents and user metadata in PostgreSQL via the latest \u201ctortoise-orm\u201d with native async and Pydantic v2 model support.\n\nDeliverables:\n- A brief requirements list naming each third-party library and its version.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used. Use the latest library versions available as of 2025-05-06.","question_index":88,"modules":["ariadne","fastapi","authlib","apscheduler","tortoise-orm"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4o","question":"Scenario: You are designing a Python microservice for ingesting user text, storing embeddings, and providing real-time, rate-limited on-device summaries via WebSockets.\n\nFunctional Requirements:\n1. HTTP POST \/ingest  \n   - Validate payload (id: str, text: str) using Pydantic v2 root validators  \n   - Compute embeddings on-device with llama-cpp-python and store (id, embedding, text) in ChromaDB  \n2. HTTP GET \/query  \n   - Accept query text, validate with Pydantic v2  \n   - Compute embedding via llama-cpp-python, retrieve top-5 nearest documents from ChromaDB, return metadata  \n3. WebSocket \/summarize  \n   - Client sends document id  \n   - Fetch text from ChromaDB metadata  \n   - Stream back a summary using llama-cpp-python streaming API  \n   - Enforce token-per-second rate limiting via aiolimiter  \n4. Lifespan events  \n   - On startup, initialize ChromaDB client, Llama model, and aiolimiter  \n   - On shutdown, cleanly close any resources  \n\nDeliverables:\n- requirements.txt listing the latest available versions as of today of:\n  \u2022 starlite  \n  \u2022 pydantic v2  \n  \u2022 chromadb  \n  \u2022 llama-cpp-python  \n  \u2022 aiolimiter  \n- main.py: a single, runnable Python 3.11+ script (under 150 lines) implementing all above, with clear comments marking where each external library is used.","question_index":8,"modules":["starlite","chromadb","llama-cpp-python","aiolimiter"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4o","question":"Scenario: Build a serverless Python application enabling real-time geolocated chat with sentiment analysis, dynamic map visualization, and PDF session reporting.\n\nFunctional Requirements:\n1. Real\u2010time WebSocket chat using the latest Litestar (released in the last 12 months) with asynchronous message persistence via a modern async ORM.\n2. Interactive geospatial mapping of incoming messages on the client side using the latest Python geospatial visualization library released in the last 12 months.\n3. On-the-fly sentiment analysis of each message using the latest Python sentiment analysis library released in the last 12 months.\n4. PDF generation of the entire chat session\u2014including message timeline, geolocation markers, and sentiment scores\u2014using the latest Python PDF library released in the last 12 months.\n5. Deployment configuration for AWS Lambda using the latest Serverless Framework plugin and AWS Lambda Powertools (all released in the last 12 months).\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":97,"modules":["litestar","sqlmodel","geoalchemy2","transformers","reportlab","folium"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: You are a senior software engineer tasked with building a real-time event analytics dashboard that ingests events, stores them, generates AI-driven summaries, caches counts, and streams updates to authenticated clients with full observability.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to implement a POST \/events REST endpoint for event ingestion.\n2. Validate incoming JSON payloads with the latest Pydantic v2 models.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise-ORM.\n4. Cache aggregated event counts in Redis via the latest aioredis.\n5. Generate streaming summaries of event batches with the latest OpenAI Python client.\n6. Stream real-time updates to clients over WebSocket using FastAPI.\n7. Secure all HTTP and WebSocket endpoints with JWT authentication via the latest fastapi-jwt-auth.\n8. Instrument HTTP, database, cache, and AI client calls with the latest OpenTelemetry for distributed tracing.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest library names and versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines integrating all subdomains, with clear comments indicating where each external library is used.","question_index":4,"modules":["fastapi","pydantic","tortoise-orm","aioredis","openai","fastapi-jwt-auth","opentelemetry-api","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-aioredis"],"modulenotfound":["opentelemetry-instrumentation-aioredis"],"modulenotfound_count":1,"module_count":9}
{"model":"gpt-4o","question":"Scenario: You\u2019re building a high-performance async task management API using the latest libraries in web framework, async ORM, caching, background processing, and observability as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement CRUD REST endpoints for a Task model using the latest starlite.\n2. Persist tasks in Postgres asynchronously with the latest Tortoise-ORM.\n3. Enqueue a background job on task creation using the latest Dramatiq with Redis broker.\n4. Cache GET \/tasks\/{id} responses in Redis via the latest redis.asyncio.\n5. Instrument all HTTP and database calls with OpenTelemetry using the latest opentelemetry-instrumentation-starlite.\n\nDeliverables:\n- A requirements.txt listing each external dependency with explicit versions.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":34,"modules":["starlite","tortoise","dramatiq","redis-asyncio","opentelemetry-instrumentation"],"modulenotfound":["redis-asyncio"],"modulenotfound_count":1,"module_count":5}
{"model":"gpt-4o","question":"Scenario  \nYou are building an AI-powered document search microservice that ingests user uploads, indexes them with a GPU-accelerated vector store, caches recent queries, and exposes HTTP\/3 endpoints for real-time search.  \n\nFunctional Requirements  \n1. HTTP API (Starlite, latest)  \n   1.1 POST \/upload: receive plain-text documents, validate with Pydantic.  \n   1.2 GET \/query?q=\u2026: return top-k results for query.  \n2. Document Ingestion (llama-index, latest)  \n   2.1 Stream tokenized embeddings upon upload.  \n3. Vector Store (ChromaDB, latest)  \n   3.1 Use GPU-backed indexing for fast similarity search.  \n4. Caching Layer (redis-om, latest)  \n   4.1 Cache each query + results with TTL and Pydantic models.  \n\nDeliverables  \n\u2022 A requirements.txt listing the latest versions (as of 2025-05-06) of all third-party libraries, each released within the past 12 months.  \n\u2022 Complete, runnable Python 3.11+ code (\u2264150 lines) implementing all requirements.  \n\u2022 Clear inline comments indicating where and how each external library is used.","question_index":47,"modules":["starlite","llama-index","chromadb","redis-om"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4o","question":"Scenario: You are a senior engineer building a high-throughput job processing microservice with a modern async Python stack.  \n\nFunctional Requirements:  \n1. Expose a POST \/jobs endpoint using the latest Starlite (as of 2025-05-06) to accept JSON job submissions.  \n2. Validate incoming JSON payloads with the latest Pydantic v2 models.  \n3. Persist job records to PostgreSQL via the latest prisma-client-py async ORM.  \n4. Enqueue and process jobs in the background using the latest Taskiq queue.  \n5. Expose a GET \/jobs\/{id}\/status endpoint to retrieve current job status.  \n6. Ensure all I\/O is non-blocking and use Python 3.11+ async\/await throughout.  \n\nDeliverables:  \n\u2022 A requirements list specifying the exact latest versions of Starlite, Pydantic v2, prisma-client-py, and Taskiq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ source code under 150 lines.  \n\u2022 Clear inline comments marking where each external library is used.","question_index":99,"modules":["starlite","pydantic","prisma-client-py","taskiq"],"modulenotfound":["prisma-client-py"],"modulenotfound_count":1,"module_count":4}
{"model":"gpt-4o","question":"Scenario: Build a secure, real-time AI-powered chat web service.\n\nFunctional Requirements:\n1. Expose an async REST API using the latest FastAPI for user registration and login endpoints.  \n2. Implement JWT authentication with rotating refresh tokens via the latest Authlib.  \n3. Persist users and chat messages in SQLite using the latest SQLModel, leveraging its new async support.  \n4. Enable real-time chat notifications over WebSockets using the latest python-socketio and ASGI.  \n5. Integrate AI chat completions using the latest OpenAI Python client in an async background task.\n\nDeliverables:\n1. A requirements.txt listing exact, up-to-date package versions as of today\u2019s date.  \n2. A single Python 3.11+ file (<150 lines) containing complete, runnable code with clear comments marking where each external library is used.","question_index":91,"modules":["fastapi","sqlmodel","authlib","openai","python_socketio","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: You are a senior software engineer tasked with building an AI-powered collaborative document annotation API.\n\nFunctional Requirements:\n1. Secure user signup and login with JWT-based authentication using Starlite (latest) and Pydantic v2.\n2. REST endpoint to upload documents and persist metadata in Redis OM Python (latest).\n3. WebSocket route for real-time annotation broadcasting using Starlite\u2019s built-in WebSocket support.\n4. Background summarization of uploaded documents using llama-cpp-python (latest).\n\nDeliverables:\n- A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":1,"modules":["starlite","pydantic","redis-om","llama-cpp-python","jose","bcrypt"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: You are a senior engineer tasked with building a secure, high-throughput microservice for customer order ingestion, persistence, background processing, and caching.\n\nFunctional Requirements:\n1. Use FastAPI (latest as of 2025-05-06) with HTTP\/3 support to implement POST \/orders accepting JSON order data.\n2. Define an asynchronous Order model with SQLModel (latest) and connect to an async database engine (SQLite or PostgreSQL).\n3. Enqueue and process new orders via Arq (latest) background worker.\n4. Implement OAuth2 with JWT issuance and protected endpoints using Authlib (latest).\n5. Cache GET \/orders\/{id} responses in Redis using aiocache (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments marking where each external library is used.","question_index":68,"modules":["fastapi","sqlmodel","databases","authlib","aiocache","arq","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"Scenario: Build an end-to-end Python web service that accepts data submissions, runs ML inference asynchronously, and pushes results back in real time.\n\nFunctional Requirements:\n1. Implement a POST \/submit endpoint using the latest FastAPI (ASGI, HTTP\/2) to accept JSON payloads.\n2. Validate and persist submissions into PostgreSQL via the latest SQLModel with asynchronous sessions.\n3. Dispatch an asynchronous classification task via the latest Celery (Redis broker) when new data arrives.\n4. In the Celery task, perform inference using the latest Hugging Face Transformers pipeline (e.g., text-classification).\n5. Cache inference results in Redis using the latest aioredis with a 5-minute TTL.\n6. Expose a WebSocket \/ws endpoint (FastAPI) to stream cached or newly computed results to subscribed clients.\n7. Instrument all HTTP requests and Celery tasks with the latest OpenTelemetry SDK for tracing.\n\nDeliverables:\n\u2022 requirements.txt listing \u201cthe latest\u201d versions of FastAPI, SQLModel, Celery, Redis, aioredis, Transformers, and OpenTelemetry as of 2025-05-06.  \n\u2022 A single Python 3.11+ file under 150 lines of runnable code. Include clear comments where each external library is used.","question_index":59,"modules":["fastapi","sqlmodel","pydantic","celery","transformers","aioredis","opentelemetry-sdk","opentelemetry-instrumentation"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4o","question":"Scenario: You are tasked with building a real-time IoT analytics dashboard featuring anomaly detection, edge caching, vector search, and HTTP\/3 streaming.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of May 6, 2025) with HTTP\/3 and ASGI concurrency for API endpoints.\n2. Ingest IoT telemetry via real-time WebSocket streams using the latest websockets library.\n3. Perform transformer-based anomaly detection using the latest HuggingFace transformers and PyTorch 2.x.\n4. Store and query time-series embeddings in a vector database using the latest Pinecone Python client with HNSW indexing.\n5. Cache recent query results at the edge using the latest aioredis client with TTL invalidation.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the \u201clatest\u201d versions of all third-party libraries as of today.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":67,"modules":["fastapi","transformers","torch","pinecone-client","aioredis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4o","question":"Scenario: Build a high-performance, AI-powered web dashboard for real-time analytics and user collaboration.\n\nFunctional Requirements:\n1. Web API: Use the latest FastAPI (as of 2025-05-06) with async endpoints and the new dependency-injection system.\n2. Real-Time Collaboration: Implement bidirectional WebSocket chat using python-socketio v5.x+ for live user updates.\n3. AI Integration: Leverage the latest llama-cpp-python (or equivalent) to compute embeddings on incoming messages.\n4. Database Persistence: Store users and message history asynchronously with SQLModel v0.7+ (Pydantic v2) on SQLite.\n5. Caching Layer: Cache AI embedding results in Redis via aioredis v3.x+ to minimize redundant computation.\n6. Security: Protect all endpoints with OAuth2 JWT authentication using Authlib v1.2+.\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all third-party libraries as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, with clear inline comments indicating where each external library is used.","question_index":58,"modules":["fastapi","sqlmodel","llama-cpp-python","aioredis","authlib","uvicorn","python-socketio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"You are a senior software engineer tasked with building a modern image-processing web service end-to-end.\n\nFunctional Requirements:\n1. Implement a REST endpoint in Litestar for image uploads that converts incoming images to AVIF using the latest pillow-avif-plugin.\n2. Asynchronously persist image metadata (filename, upload timestamp, AVIF URL, dimensions) in MongoDB via the latest Beanie ODM.\n3. Index and cache metadata in Redis for fast lookup using the latest Redis-OM Python library.\n4. Expose a GraphQL API via the latest Litestar-GraphQL plugin to query images with filtering, pagination, and type validation.\n5. Broadcast real-time upload notifications over WebSocket connections using Litestar\u2019s built-in WebSocket support.\n\nDeliverables:\n- A concise list of the five functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines, using the latest Litestar, Litestar-GraphQL, Beanie, Redis-OM, and pillow-avif-plugin (as of May 6, 2025), with clear comments indicating where each external library is used.","question_index":89,"modules":["litestar","pillow-avif-plugin","motor","beanie","redis-om","fastapi"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"You are a Senior Software Engineer building a high-throughput, real-time web application that ingests user events, enriches them with AI, persists outcomes, and serves analytics via a modern dashboard.\n\nFunctional Requirements:\n1. Implement JWT-based user authentication using the latest async web framework with schema-first validation (e.g., HTTP\/3 support).\n2. Expose a WebSocket endpoint for real-time event ingestion using the latest WebSocket library with per-message compression.\n3. Perform fully async CRUD on PostgreSQL using the latest ORM with native asyncio support and advanced connection pooling.\n4. Offload heavy computations to background workers via the latest task queue library supporting asyncio and result backends.\n5. Integrate an AI text-classification pipeline using the latest AI inference library with streaming outputs.\n6. Instrument the service with distributed tracing using the latest observability library supporting OpenTelemetry protocols.\n\nDeliverables:\n\u2022 requirements.txt listing the latest available versions of all dependencies (as of May 6, 2025).  \n\u2022 A single Python 3.11+ file under 150 lines that is complete and runnable, with clear comments marking where each external library is used.","question_index":12,"modules":["fastapi","fastapi_jwt_auth","websockets","transformers","sqlalchemy","opentelemetry-instrumentation-fastapi","aio_pika","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4o","question":"Scenario: Develop a high-performance asynchronous web service for ingesting user datasets, persisting metadata, launching background processing jobs, and tracing all operations end-to-end.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/upload endpoint using Starlite (latest) with async file streaming and dependency injection.\n2. Define and migrate a PostgreSQL table via Piccolo ORM (latest), storing upload metadata and task IDs, leveraging its GraphQL auto-schema feature.\n3. Enqueue and process tasks with Asynq (latest) in Redis, using priority queues, retry policies, and task result callbacks.\n4. Instrument incoming HTTP requests and background tasks with OpenTelemetry (latest), enabling W3C trace-context propagation across components.\n5. Provide an HTTP GET \/status\/{task_id} endpoint that queries Piccolo ORM for real-time task status.\n\nDeliverables:\n\u2022 The numbered functional requirements list (as above)  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, using the latest versions of all libraries as of 2025-05-06, with clear comments indicating where each external library is used.","question_index":43,"modules":["starlite","piccolo","asynq","redis","asyncpg","opentelemetry"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":6}
{"model":"gpt-4o","question":"As a senior software engineer, you are tasked with building a secure, real-time collaborative whiteboard microservice using cutting-edge Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3 API using the latest FastAPI (as of 2025-05-06) and uvicorn[standard] for low-latency streaming.\n2. Implement OAuth2 authorization with JWTs and refresh-token rotation using the latest Authlib.\n3. Persist whiteboard sessions and user metadata in PostgreSQL via the latest SQLModel with an async engine.\n4. Broadcast drawing events in real time over WebSockets using the latest websockets library with room-based pub\/sub.\n5. Vectorize stroke data into SVG on the fly using the latest Shapely or pyvips for advanced geometry operations.\n6. Upload periodic session snapshots to AWS S3 using the latest aioboto3 with asynchronous multipart uploads.\n\nDeliverables:\n- A requirements.txt listing the latest available versions of all external libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, fully runnable, with clear comments indicating where each third-party library is used.","question_index":25,"modules":["fastapi","authlib","sqlmodel","websockets","shapely","aioboto3"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: Build a real-time sentiment analysis dashboard that fetches trending Twitter topics, analyzes sentiment, stores results, and serves updates via WebSockets.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to create both REST and WebSocket endpoints, leveraging its built-in asyncio and WebSocket broadcast features.\n2. Use the latest HTTPX to asynchronously fetch trending topics from the Twitter API v2 over HTTP\/3.\n3. Use the latest Hugging Face transformers to perform sentiment analysis via a pretrained pipeline.\n4. Use the latest SQLModel to define async SQLite models and persist topics with sentiment scores.\n5. Use the latest Jinja2 to render a minimal HTML dashboard for initial page load before WebSocket updates.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the latest versions of FastAPI, HTTPX, transformers, SQLModel, and Jinja2 as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":36,"modules":["fastapi","transformers","httpx","sqlmodel","jinja2"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4o","question":"Senior Software Engineer: You are developing an intelligent IoT monitoring dashboard that integrates real-time ingestion, streaming anomaly detection, WebSocket updates, dynamic UI rendering, and cloud infrastructure provisioning.\n\nFunctional Requirements:\n1. Build an ASGI web server using the latest ASGI framework (as of 2025-05-06) with HTTP\/2 and WebSocket endpoints for real-time data streaming.\n2. Connect asynchronously to an MQTT broker using the latest Python MQTT client library (as of 2025-05-06), subscribe to sensor topics, and relay messages to the server.\n3. Implement real-time anomaly detection on incoming sensor data using the latest Python anomaly detection library supporting incremental or deep learning.\n4. Serve a live, interactive front-end auto-generated from Python to JavaScript using the latest Python-to-JS UI framework.\n5. Provision and deploy all components via infrastructure-as-code using the latest Python IaC SDK on a major cloud provider.\n\nDeliverables:\n\u2022 Restate the numbered functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code integrating all five requirements, under 150 lines.  \n\u2022 Clear inline comments indicating where and how each external \u201clatest\u201d library (as of 2025-05-06) is imported and used.","question_index":49,"modules":["fastapi","asyncio_mqtt","pyod","py2js","pulumi_aws","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: Build a real-time collaborative note-taking web application with authentication, persistence, caching, live updates, and scheduled cleanup.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (as of today) for note creation, retrieval, update, and deletion.\n2. Secure endpoints with JWT authentication using the latest PyJWT.\n3. Persist notes in a relational database via the latest SQLModel ORM, defining Note and User models.\n4. Enable real-time note synchronization between clients using WebSockets with the latest python-socketio.\n5. Cache frequently accessed note metadata in Redis using the latest redis-om to reduce DB load.\n6. Schedule automatic archiving of notes older than 30 days using the latest APScheduler.\n7. Document all endpoints with OpenAPI and include example request\/response schemas.\n\nDeliverables:\n- A numbered list of all third-party libraries (with versions) you\u2019ve chosen.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear comments indicating where each external library is used.","question_index":69,"modules":["fastapi","jose","sqlmodel","apscheduler","python-socketio","redis-om"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: Build a high-performance asynchronous message processing service for real-time sentiment analysis, storage, notification, and search.\n\nFunctional Requirements:\n1. Implement an async HTTP API using the latest FastAPI (as of May 6, 2025) to accept user messages.\n2. Define and persist message models in PostgreSQL via the latest SQLModel ORM.\n3. Schedule and execute sentiment analysis in the background using the latest Dramatiq with RabbitMQ.\n4. After analysis, update the database and push real-time notifications to connected clients over WebSockets.\n5. Index messages and sentiment scores into Elasticsearch using the latest official Python client.\n6. Provide a search endpoint to query stored messages by keyword and sentiment.\n\nDeliverables:\n\u2022 A numbered list of the \u201clatest\u201d third-party libraries (with versions) used for each sub-domain.  \n\u2022 Complete, runnable Python 3.11+ application code under 150 lines, with clear comments indicating where each external library is used.","question_index":48,"modules":["fastapi","sqlmodel","dramatiq","elastic","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4o","question":"Scenario: Build a microservice that accepts text input, asynchronously generates summaries, persists data, and provides real-time metrics.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI 3, HTTP\/3 support) to implement POST \/summaries accepting JSON { \"text\": \"\u2026\" }, enqueue a background job via Arq, and return a job ID.\n2. Persist requests and summaries in SQLite asynchronously with the latest SQLModel (Pydantic V2 model support).\n3. Offload summarization jobs to Redis using the latest Arq, invoking the latest LangChain to call OpenAI for a concise summary.\n4. Expose GET \/summaries\/{id} to retrieve stored summaries from the database.\n5. Instrument request counts and job durations with the latest prometheus_client, exposing metrics on GET \/metrics.\n\nDeliverables:\n- A bulleted list of chosen \u201clatest\u201d libraries with exact version numbers.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear inline comments indicating where each external library is used.\n- Self-contained implementation: after installing dependencies, code runs without further setup.","question_index":95,"modules":["starlite","sqlmodel","asyncpg","redis","arq","langchain","prometheus_client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"Scenario: Create a serverless real-time analytics API that ingests WebSocket events, performs on-the-fly NLP summarization, indexes embeddings, exposes a GraphQL endpoint, and deploys to AWS Lambda.\n\nFunctional Requirements:\n1. Use the latest Litestar (as of 2025-05-06) to build an HTTP\/2 GraphQL endpoint with schema-based typing.\n2. Implement a real-time WebSocket consumer using wsproto or litestar-websockets for event ingestion.\n3. Offload text summarization to an asynchronous background task leveraging the latest Transformers library.\n4. Generate embeddings with the newest OpenAI Embeddings API or sentence-transformers and store them in the latest ChromaDB vector store.\n5. Secure all endpoints with passwordless authentication via the latest fastapi-users or equivalent.\n6. Package and deploy the entire application to AWS Lambda using the latest Mangum adapter.\n\nDeliverables:\n- A numbered list of chosen \u201clatest\u201d library versions (as of 2025-05-06) mapping to each requirement.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":77,"modules":["litestar","wsproto","transformers","sentence-transformers","chromadb","fastapi-users","mangum"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"Scenario: Develop a modern, high-performance AI-powered web service for real-time text processing.\n\nFunctional Requirements:\n1. Build a JSON REST API with Litestar using its latest dependency-injection and ASGI lifespan hooks.\n2. Integrate Tortoise ORM (async) to manage a PostgreSQL \u201cjobs\u201d table with migrations.\n3. Use llama-cpp-python to generate text embeddings or responses via a local LLaMA model.\n4. Expose a WebSocket endpoint with python-socketio (asyncio mode) for live client notifications.\n5. Cache recent embeddings in Redis using redis-py\u2019s asyncio client.\n6. Schedule background tasks (e.g., stale job cleanup) leveraging asyncio\u2019s latest task groups.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":60,"modules":["litestar","tortoise-orm","llama-cpp-python","python-socketio","aioredis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4o","question":"Scenario: You are a senior software engineer building a real-time social media monitoring application that ingests tweets, analyzes sentiment, classifies images, displays live charts, and deploys as a serverless microservice using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Ingest tweets in real time from Twitter API v2 using the latest HTTPX async streaming client.\n2. Perform NLP sentiment analysis on tweet text with the latest Hugging Face Transformers pipeline.\n3. Classify embedded images using the latest Ultralytics YOLOv8 model.\n4. Render an interactive, live-updating dashboard using the latest Plotly Dash framework.\n5. Scaffold serverless deployment with the latest AWS Lambda Powertools for Python.\n\nDeliverables:\n- A requirements list of all external dependencies.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":98,"modules":["httpx","transformers","ultralytics","dash","boto3","aws-lambda-powertools"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: A knowledge management platform must allow users to upload PDFs, index their content, and perform real-time similarity searches via a high-performance API.\n\nFunctional Requirements:\n1. Implement a PDF upload REST endpoint using the latest unstructured-sdk to extract text.\n2. Use the latest ChromaDB Python client to generate and store embeddings for each document.\n3. Provide a search REST endpoint that queries ChromaDB for top-k similar documents given a text query.\n4. Create a real-time WebSocket over HTTP\/3 endpoint using the latest aioquic to stream incremental search results.\n5. Assemble the application as an asynchronous service using the latest Starlite web framework.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":6,"modules":["starlite","unstructured-sdk","chroma-db","aioquic","starlette"],"modulenotfound":["unstructured-sdk","chroma-db"],"modulenotfound_count":2,"module_count":5}
{"model":"gpt-4o","question":"Scenario: As a senior software engineer, build an asyncio-based Python microservice showcasing HTTP\/3 REST, GraphQL, async ORM, real-time WebSockets, and scheduled tasks using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST endpoint at `\/api\/items` using FastAPI (latest) and Hypercorn (latest) with HTTP\/3 support.\n2. Expose a GraphQL API at `\/graphql` using Strawberry (latest) with asynchronous resolvers fetching from the database.\n3. Integrate PostgreSQL via Tortoise ORM (latest), define an `Item` model, and apply programmatic migrations on startup.\n4. Provide WebSocket notifications at `\/ws\/updates` using python-socketio (latest), broadcasting item creation and deletion events.\n5. Schedule an hourly cleanup job with APScheduler (latest) in asyncio mode to remove `Item` records older than 7 days.\n\nDeliverables:\n- A `requirements.txt` listing the latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is imported and utilized.","question_index":66,"modules":["fastapi","hypercorn","strawberry-graphql","tortoise-orm","socketio","apscheduler"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: Develop a real-time AI-driven sentiment analysis web service that ingests text, processes it via an on-device LLM, persists results, and streams live analytics to clients.\n\nFunctional Requirements:\n1. Expose RESTful and WebSocket endpoints using the latest Litestar Python framework (released within the last 12 months).\n2. Perform local LLM inference for sentiment analysis with the latest llama-cpp-python library (released within the last 12 months).\n3. Asynchronously persist raw text and sentiment scores in MongoDB using the latest async MongoDB driver (released within the last 12 months) with code-first schema.\n4. Offload sentiment analysis to a distributed background worker using the latest Dramatiq (or equivalent) async task queue released in the last 12 months.\n5. Broadcast real-time sentiment updates to connected clients over WebSockets.\n6. Render an interactive dashboard in server-side templates with the latest Plotly Python library (released within the last 12 months).\n7. Instrument the entire application with distributed tracing via the latest OpenTelemetry Python SDK (released within the last 12 months).\n\nDeliverables:\n- A requirements list specifying each third-party library and its exact latest version as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines total.\n- Clear inline comments indicating where and how each external library is used.","question_index":5,"modules":["litestar","llama-cpp-python","motor","dramatiq","plotly","opentelemetry-api","opentelemetry-sdk","opentelemetry-exporter-otlp-proto-grpc"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4o","question":"You are a senior software engineer tasked with building a real-time analytics dashboard with AI-driven anomaly alerts using the latest Python libraries as of 2025-05-06.  \n\nFunctional Requirements:  \n1. Implement asynchronous REST API endpoints using the latest FastAPI.  \n2. Add WebSocket-based real-time alert streaming using the latest websockets.  \n3. Persist incoming metrics in PostgreSQL via async ORM operations using the latest SQLModel.  \n4. Cache expensive queries in Redis for high throughput using the latest aioredis.  \n5. Perform streaming anomaly detection on incoming metrics using the latest river library.  \n6. Enable file uploads of detailed reports to S3-compatible storage via the latest minio SDK.  \n\nDeliverables:  \n\u2022 A numbered list of required Python libraries (with versions as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":20,"modules":["fastapi","sqlmodel","aioredis","river","minio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4o","question":"Build a real-time analytics microservice that lets authenticated users ingest sales data, stores it in PostgreSQL, streams live updates to connected clients, and generates PDF reports with embedded charts.\n\n1. Implement OAuth2 JWT authentication using the latest fastapi-jwt-auth as of May 6, 2025.  \n2. Create async CRUD endpoints for sales records with SQLModel and asyncpg.  \n3. Broadcast new sales events over WebSocket using python-socketio (latest).  \n4. Generate interactive charts in HTML via Plotly (latest) and convert them to PDF using WeasyPrint (latest).  \n5. Expose generated PDFs via an endpoint, serving files asynchronously with aiofiles.  \n\nDeliverables:  \n- A numbered confirmation of the functional requirements above.  \n- A single Python 3.11+ file under 150 lines\u2014complete, runnable code\u2014with clear comments indicating where each external library is used. Use the latest versions of all third-party libraries as of today\u2019s date.","question_index":26,"modules":["fastapi","fastapi_jwt_auth","sqlmodel","python_socketio","plotly","weasyprint","pdfkit","aiofiles"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4o","question":"Scenario: You are tasked with building a real-time collaborative task management web service for enterprise teams.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to expose HTTP endpoints and WebSocket routes, leveraging its new dependency injection and WebSocket lifetime management.\n2. Configure an async PostgreSQL database using the latest Tortoise-ORM with its auto-migration CLI and Pydantic model integration.\n3. Broadcast real-time task updates to connected clients via the latest python-socketio in async mode.\n4. Implement OAuth2 authentication with JWT access and refresh tokens using the latest Authlib, including token rotation.\n5. Cache sessions and publish update events using the latest aioredis async Redis client.\n6. Schedule and execute background jobs (e.g., daily summaries) with the latest APScheduler async support.\n7. Instrument request tracing and logging using the latest OpenTelemetry Python SDK, exporting to console.\n8. Auto-generate interactive API docs using FastAPI\u2019s built-in Swagger UI and Redoc.\n\nDeliverables:\n\u2022 A bullet list of selected libraries (with version numbers).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":15,"modules":["fastapi","tortoise-orm","pydantic","socketio","authlib","aioredis","apscheduler","opentelemetry-sdk"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4o","question":"Scenario: You are building a high-performance Python backend that ingests user articles, generates AI-powered summaries, stores and caches results, and exposes them via REST, GraphQL, and real-time channels with background processing.\n\nFunctional Requirements:\n1. Accept article submissions over a REST API using FastAPI and validate payloads with Pydantic v2.  \n2. Offload summarization to a background worker with Dramatiq using Redis as a broker.  \n3. Summarize articles via the latest OpenAI Python SDK and cache summaries with aiocache.  \n4. Persist articles and summaries in SQLite using SQLModel\u2019s async ORM features.  \n5. Expose stored summaries through a GraphQL endpoint implemented with Strawberry GraphQL.  \n6. Broadcast new summary notifications to connected clients over WebSockets with python-socketio.  \n\nDeliverables:\n\u2022 List of the latest third-party libraries (as of today) and their install commands.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":63,"modules":["fastapi","pydantic","sqlmodel","dramatiq","strawberry-graphql","aiocache","openai","python-socketio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4o","question":"Scenario: Develop an internal Python microservice where authenticated users can submit text to be summarized by an LLM and receive real-time notifications via WebSockets.\n\nFunctional Requirements:\n1. Implement an asynchronous REST API using the latest Starlite framework (2025-05-06) leveraging its dependency injection and lifespan event system.\n2. Integrate OAuth2 Authorization Code Flow for user authentication with Authlib\u2019s async client libraries released in the last 12 months.\n3. Persist users, submissions, and summaries in PostgreSQL using the latest Piccolo ORM with async schema migrations and connection pooling.\n4. Summarize submitted text by calling OpenAI\u2019s GPT-4 Turbo via the newest openai Python SDK and its function-calling feature.\n5. Broadcast summary completion events over WebSockets using a modern Python websockets library (with HTTP\/2 support) released in the past year.\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":82,"modules":["starlite","authlib","piccolo","asyncpg","openai","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"You are tasked with building a real-time analytics web dashboard that ingests streaming data, processes it, summarizes it with AI, secures user access, and visualizes results dynamically using the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Implement a REST API in Python 3.11+ with FastAPI (HTTP\/3 support via Hypercorn) to accept JSON event streams.\n2. Broadcast processed insights in real time over WebSocket using python-socketio.\n3. Buffer incoming events and generate streaming AI summaries using the latest LangChain and OpenAI Python SDK.\n4. Perform real-time aggregation and lazy analytics on events with Polars.\n5. Produce dynamic Plotly charts of key metrics and serve them as JSON for front-end rendering.\n6. Secure all endpoints and WebSocket connections with JWT-based auth via fastapi-users.\n\nDeliverables:\n\u2022 A numbered list of the above functional requirements.  \n\u2022 A single Python 3.11+ file (under 150 lines) that fulfills all requirements, complete and runnable. Include clear comments indicating where and how each external library is used.","question_index":84,"modules":["fastapi-users","langchain","socketio","polars","plotly","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: You are a senior software engineer tasked with building a next-generation real-time collaborative data analytics dashboard. Use the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a FastAPI server with a WebSocket endpoint for live data updates using the latest fastapi[full].\n2. Expose a type-safe GraphQL API at `\/graphql` using Strawberry with async schema stitching.\n3. Integrate AI-driven insights by generating embeddings on incoming data with llama-cpp-python.\n4. Perform vector similarity search over embeddings via the Weaviate Python client\u2019s async API.\n5. Secure all endpoints with passwordless authentication using py_webauthn for WebAuthn flows.\n6. Containerize the application using docker-py to build images and dynamically generate a docker-compose config.\n7. Instrument the app with async Prometheus metrics using prometheus-client.\n\nDeliverables:\n- A list of required libraries (with versions) using the latest releases as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":72,"modules":["fastapi","strawberry-graphql","llama-cpp-python","weaviate-client","py_webauthn","docker","prometheus-client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"Scenario: Develop a real-time collaborative note-taking web application with AI-powered summarization.\n\nFunctional Requirements:\n1. Implement user authentication via OAuth2 password flow using the latest FastAPI features for secure login.  \n2. Expose a WebSocket endpoint for real-time CRDT-based document state synchronization using the newest FastAPI WebSocket enhancements.  \n3. Persist users and documents in PostgreSQL via the latest Piccolo ORM async API with JSONB indexing for rich querying.  \n4. Cache live presence and session metadata in Redis using aioredis with RedisJSON support for low-latency reads\/writes.  \n5. Summarize document edits on demand using the latest LangChain library and ChatOpenAI (GPT-4) integration.  \n6. Serve a minimal HTML frontend via Jinja2 templates that connects to the WebSocket and displays live updates.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of each dependency as of 2025-05-06.  \n\u2022 main.py: Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":52,"modules":["fastapi","piccolo-orm","aioredis","openai","langchain","jinja2","starlette"],"modulenotfound":["piccolo-orm"],"modulenotfound_count":1,"module_count":7}
{"model":"gpt-4o","question":"Scenario: Senior Software Engineer, you are tasked with building a prototype real-time collaborative note-taking service using only the latest Python libraries released in the last 12 months.\n\nFunctional Requirements:\n1. Develop an asynchronous REST API for note CRUD operations using the latest Litestar framework.\n2. Persist notes in a SQL database via a newly released async ORM library.\n3. Cache frequent note queries in Redis with a recently published async Redis client.\n4. Enable real-time collaboration via WebSockets using a modern Python WebSocket library launched within the last year.\n5. Expose an AI-powered summarization endpoint using the latest OpenAI Python client.\n6. Render an interactive HTML dashboard showing active collaborators with a new Python-to-JS visualization tool.\n7. Instrument distributed tracing and structured logging using a cutting-edge observability library released in the past 12 months.\n\nDeliverables:\n- requirements.txt listing only libraries created in the last 12 months with exact versions.\n- A single Python 3.11+ file under 150 lines containing the complete, runnable application, with clear comments indicating where each external library is used.","question_index":41,"modules":["litestar","sqlalchemy_async","redis_async","websockets_modern","openai_latest","py_js_viz","obs_tracing_logs"],"modulenotfound":["sqlalchemy_async","websockets_modern","openai_latest","py_js_viz","obs_tracing_logs"],"modulenotfound_count":5,"module_count":7}
{"model":"gpt-4o","question":"Scenario:\nCreate an AI-powered article summarization service that accepts user-submitted text, stores it, generates summaries asynchronously via an AI API, caches results, and streams them back to clients in real time.\n\nFunctional Requirements:\n1. Implement a REST API using FastAPI (latest) with endpoints POST \/articles to submit text and GET \/stream\/{article_id} for server-sent events.\n2. Persist submissions in a database using SQLModel (latest) with an async SQLite or PostgreSQL engine.\n3. Orchestrate a background job queue using Celery (latest) with Redis as broker to process newly submitted articles.\n4. Perform summarization using the OpenAI Python SDK (latest) and its chat completion endpoint.\n5. Cache generated summaries in Redis via aioredis (latest) with a configurable TTL.\n6. Stream summaries to connected clients in real time using server-sent events via sse-starlette (latest).\n\nDeliverables:\n\u2022 A list of all third-party libraries (with versions pinned to the latest as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":64,"modules":["fastapi","sse-starlette","sqlmodel","celery","aioredis","openai"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: Build a lightweight feedback API that records user comments, analyzes sentiment in real time, stores results, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (as of 2025-05-06) to implement asynchronous REST endpoints for submitting feedback and querying stored analytics.\n2. Use the latest SQLModel to define async ORM models and persist incoming feedback in an SQLite database.\n3. Use the latest PyWebSocketX to broadcast each new sentiment result over an ASGI WebSocket endpoint.\n4. Use the latest VulcanMind inference library to perform on-the-fly sentiment analysis of submitted feedback.\n5. Implement structured logging and error handling throughout the request, inference, database, and WebSocket flows.\n\nDeliverables:\n- A requirements list with exact version pins for Starlite, SQLModel, PyWebSocketX, and VulcanMind (latest as of 2025-05-06).\n- Complete, runnable Python 3.11+ code (\u2264150 lines) that integrates all four libraries with clear comments marking where each is used.","question_index":83,"modules":["starlite","sqlmodel","pywebsocketx","vulcanmind"],"modulenotfound":["pywebsocketx","vulcanmind"],"modulenotfound_count":2,"module_count":4}
{"model":"gpt-4o","question":"You are a senior software engineer building a high-performance, real-time analytics web service for IoT sensor data.  \n\nFunctional Requirements:  \n1. Expose HTTP REST ingest endpoints using the latest Nebula framework (released 2024-06) with its async dependency-injection.  \n2. Persist incoming readings to PostgreSQL via StarDB ORM (v1.0+, released 2024-08), leveraging async transactions and auto-migration.  \n3. Secure all endpoints with OAuth2 PKCE flows using AuthFusion (released 2024-12).  \n4. Broadcast processed data updates to dashboard clients via WaveSocket (released 2024-10) server-clustering.  \n5. Schedule hourly aggregation jobs using ChronoTask (released 2024-11) cron-style scheduler.  \n\nDeliverables:  \n\u2022 A requirements.txt listing the latest versions of all libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":92,"modules":["nebula-framework","stardb","authfusion","wavesocket","chronotask"],"modulenotfound":["nebula-framework","stardb","authfusion","wavesocket"],"modulenotfound_count":4,"module_count":5}
{"model":"gpt-4o","question":"Scenario: You are a senior software engineer building a high-throughput chat summarization microservice in Python 3.11+ using only the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Real-time WebSocket ingestion: implement a `\/ws` endpoint using fastapi-socketio (latest) with session storage in Redis via `redis.asyncio`.\n2. AI summarization: enqueue incoming messages in Arq (latest) and process them with the OpenAI Python SDK (latest) calling GPT-4 Turbo for summaries.\n3. Async data persistence: store raw messages and summaries in PostgreSQL using SQLModel + encode\/databases with the asyncpg driver.\n4. Dynamic admin dashboard: expose `\/admin` rendering Jinja2 templates enhanced with HTMX via fastapi-htmx (latest).\n5. Email alerts: send summary notifications asynchronously using FastAPI-Mail (latest).\n6. Containerization: provide a multi-stage Dockerfile based on `python:3.11-slim`.\n\nDeliverables:\n- A `requirements.txt` listing the exact versions of all third-party libraries (as of May 6, 2025).\n- A single `main.py` (under 150 lines) containing complete, runnable Python 3.11+ code with clear comments indicating where and how each external library is used.","question_index":23,"modules":["fastapi-socketio","redis","arq","openai","sqlmodel","databases","fastapi-htmx","fastapi-mail"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4o","question":"Scenario: Build a compact Python microservice that ingests text and images, leverages AI summarization, real-time streams results, and stores everything in a modern database.\n\nFunctional Requirements:\n1. Use the latest Litestar (async web framework released Dec 2023) to define:\n   a. POST \/articles to receive JSON {\u201ctitle\u201d:\u2026, \u201ccontent\u201d:\u2026}.\n   b. WebSocket endpoint \/ws\/summary for streaming summaries.\n2. Connect asynchronously to PostgreSQL via the latest Prisma Python client (released Jul 2023), defining models for Article and Image.\n3. Summarize incoming content using the latest llama-cpp-python (released Sep 2023), utilizing its streaming interface for token-by-token output.\n4. Implement \/upload to accept multipart\/form-data image files, generate HEIF thumbnails with the latest pillow-heif (released Oct 2023), and persist file paths.\n5. Ensure all endpoints use async\/await, proper error handling, and pydantic-based validation.\n\nDeliverables:\n\u2013 A requirements.txt listing the latest library versions as of 2025-05-06.\n\u2013 A single, runnable Python 3.11+ script under 150 lines, with clear comments indicating where each external library is used.","question_index":16,"modules":["litestar","prisma","llama-cpp-python","pillow-heif"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4o","question":"You are tasked with building a prototype web service for authenticated semantic search with real-time visual feedback.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) server with:\n   a. an OAuth2 login flow using PyOIDC (latest) against an external Identity Provider  \n   b. a POST \/query endpoint to accept natural-language queries  \n   c. a WebSocket \/ws endpoint to stream real-time results  \n2. Upon authentication, embed incoming queries using sentence-transformers (latest) and perform vector similarity search in ChromaDB (latest).  \n3. As results arrive, push them incrementally over the WebSocket to clients.  \n4. Generate a Plotly (latest) bar chart of top-k result scores and serve it as an HTML snippet.  \n5. Ensure all components run under Python 3.11+, integrate only \u201cthe latest\u201d library releases as of 2025-05-06, and keep implementation concise.  \n\nDeliverables:\n\u2022 A list of chosen libraries with exact versions (the \u201crequirements\u201d).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":42,"modules":["fastapi","fastapi_websocket_pubsub","sentence-transformers","plotly","uvicorn","chromadb","pyop"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"Scenario: You are building a high-throughput, real-time analytics web application for processing IoT sensor streams using the latest Python async frameworks and libraries.\n\nFunctional Requirements:\n1. Ingest JSON sensor events via a REST API built on FastAPI (use its latest HTTP\/3 support).\n2. Broadcast processed metrics to connected clients over WebSockets using python-socketio (asyncio).\n3. Offload CPU-intensive aggregation to background workers with Arq (latest Redis-backed queue).\n4. Persist time-series summaries in PostgreSQL via SQLModel (Pydantic v2-powered ORM).\n5. Cache hot query results in Redis using aioredis for sub-millisecond lookups.\n6. Expose tracing and Prometheus metrics using OpenTelemetry and prometheus-client.\n\nDeliverables:\n- A bullet list of the latest library dependencies as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments indicating where each external library is used.","question_index":40,"modules":["fastapi","python-socketio","sqlmodel","aioredis","arq","opentelemetry-api","prometheus-client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"You are a senior software engineer tasked with building a Python web application that integrates user authentication, real-time notifications, AI-based recommendations, and payment processing.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (2025-05-06) and Python 3.11 features (e.g. match-case).\n2. Secure all endpoints with OAuth2 JWT authentication via the latest fastapi-users, including email verification flows.\n3. Provide real-time notifications over WebSockets using the latest python-socketio async server.\n4. Handle one-time charges using the latest Stripe Python SDK via the Payment Intents API.\n5. Generate AI-driven product recommendations by invoking gpt-4-turbo through the latest openai library.\n6. Emit structured, context-rich logs for each operation using the latest structlog.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06  \n\u2022 Complete, runnable Python 3.11+ code (under 150 lines) with clear comments marking where each external library is used","question_index":96,"modules":["fastapi","fastapi-users","python-socketio","openai","stripe","structlog","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"Scenario: As a senior software engineer, design a high-performance microservice for real-time document collaboration with secure access, persistence, and cloud integration.  \n\nFunctional Requirements:  \n1. Implement an asynchronous JSON REST API using the latest Python web framework available as of 2025-05-06, supporting pagination and input validation.  \n2. Secure all endpoints with OAuth2 PKCE and JWT using the latest authentication library (released within the last 12 months).  \n3. Enable bidirectional real-time collaboration events over WebSocket using the latest WebSocket library (released within the last 12 months).  \n4. Persist document data in an async NoSQL database (e.g., MongoDB) using the newest driver supporting transactions and change streams.  \n5. Upload and serve document attachments to an S3-compatible object storage using the latest SDK with multipart upload support.  \n6. Cache frequently accessed documents in Redis with TTL and automatic invalidation using the newest Redis client library.  \n\nDeliverables:  \n- A requirements.txt listing \u201cthe latest\u201d versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":80,"modules":["fastapi","pydantic","python-jose","motor","aioredis","boto3","websockets","python-dotenv"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4o","question":"Scenario: Build a high-performance Python microservice that combines cutting-edge web, real-time, AI and observability features into a single ASGI application.\n\nFunctional Requirements:\n1. HTTP\/3 ASGI server with automatic TLS and QUIC support using the latest HTTP\/3 library released within the past 12 months.  \n2. Real-time WebSocket endpoint that streams JSON events with zero-copy backpressure control using the newest streaming library created in the last year.  \n3. Semantic search pipeline: accept text uploads, generate embeddings, and perform vector similarity queries via the latest vector-store client released within 12 months.  \n4. On-device LLM inference endpoint that consumes streamed text segments and returns summaries using the most recent quantized model library published this year.  \n5. Distributed tracing and metrics export using the newest OpenTelemetry OTLP exporter released within the past 12 months.\n\nDeliverables:\n- A requirements list (requirements.txt) specifying the exact, latest versions of each third-party library as of 2025-05-06.  \n- A single Python 3.11+ script under 150 lines that implements all features above, with clear comments marking where each external library is used.","question_index":51,"modules":["uvicorn","aiohttp","http3_aiols","websockets_stream","openai_modern_embed","llm_infer_quant","opentelemetry-api","opentelemetry-exporter-otlp","opentelemetry-sdk"],"modulenotfound":["http3_aiols","websockets_stream","openai_modern_embed","llm_infer_quant"],"modulenotfound_count":4,"module_count":9}
{"model":"gpt-4o","question":"Scenario  \nDevelop an end-to-end Python web service that provides passwordless WebAuthn authentication, stores and summarizes text inputs via OpenAI, broadcasts real-time summary events, and serves interactive charts.\n\nFunctional Requirements  \n1. Implement passwordless registration and login using the latest \u201cwebauthn\u201d library (FIDO2) with Litestar middleware.  \n2. Create REST endpoints under Litestar (latest version) for submitting text to summarize and retrieving stored summaries.  \n3. Use the latest SQLModel with an async PostgreSQL engine to persist input texts, summaries, timestamps, and user IDs.  \n4. Invoke the latest \u201copenai\u201d Python client to perform GPT-4 Turbo summarization with function-calling.  \n5. Serialize and broadcast new summaries over WebSockets using the latest \u201cmsgspec\u201d for high-performance JSON.  \n6. Serve an HTML page at \u201c\/charts\u201d that embeds an interactive Plotly chart of summary lengths over time.\n\nDeliverables  \n\u2022 A numbered list of all external libraries (with exact latest versions as of 2025-05-06) and their roles.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":13,"modules":["litestar","sqlmodel","webauthn_async","openai","plotly","msgspec"],"modulenotfound":["webauthn_async"],"modulenotfound_count":1,"module_count":6}
{"model":"gpt-4o","question":"Scenario: As a senior software engineer, you must develop a real-time analytics dashboard backend that requires user authentication, data ingestion, live updates, background enrichment via an external API, and a web-based dashboard.\n\nFunctional Requirements:\n1. Implement user registration and login endpoints using JWT authentication with asymmetric keys via FastAPI-JWT-Auth (latest).\n2. Create a POST \/data endpoint to ingest JSON payloads and store them asynchronously in SQLite using SQLModel (latest).\n3. Broadcast each new record to connected WebSocket clients at \/ws using the latest websockets library, leveraging async broadcast.\n4. Schedule a background job every minute with arq (latest) to enrich stored records by streaming JSON from a third-party API via httpx (latest).\n5. Serve a simple HTML dashboard at GET \/dashboard that opens a WebSocket to \/ws and displays incoming records in real time.\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":75,"modules":["fastapi","fastapi_jwt_auth","sqlmodel","httpx","arq","sqlite_utils"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: Design and implement a Python microservice that exposes HTTP endpoints, real-time WebSocket notifications, persistent storage, caching, scheduled tasks, and email alerts using the latest third-party libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use FastAPI (latest) with HTTP\/2 support to expose async REST endpoints for user registration and notification retrieval.\n2. Use SQLModel (latest) as an async ORM for PostgreSQL, defining typed models for users and notifications.\n3. Implement WebSocket broadcasting via FastAPI\u2019s WebSocket support to push new notifications to connected clients.\n4. Leverage Aioredis (latest) Redis Streams to cache session state and stream notification events.\n5. Schedule periodic database cleanup and cache eviction using APScheduler (latest) with its asyncio integration.\n6. Send email alerts for critical notifications using Aiosmtplib (latest) with STARTTLS.\n7. Perform external health-check API calls using HTTPX (latest) with async retries and timeouts.\n\nDeliverables:\n- A requirements list naming each library and its latest version as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, including clear comments that indicate where each external library is used.","question_index":44,"modules":["fastapi","sqlmodel","aioredis","apscheduler","aiosmtplib","httpx","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"Scenario: You are tasked as a senior software engineer to build an end-to-end collaborative document editing service with real-time updates, async persistence, GraphQL subscriptions, and cloud file storage.\n\nFunctional Requirements:\n1. Use the latest Litestar (first released within the last 12 months) to create an HTTP\/2 web server with WebSocket endpoints for real-time edit streams.\n2. Implement a GraphQL API with Strawberry GraphQL (latest) that supports queries, mutations, and live subscriptions for document change events.\n3. Persist documents in a PostgreSQL database via a fully async ORM of your choice that was first released in the last 12 months and supports automated migrations.\n4. Generate AWS S3 presigned URLs for resumable file attachments using aiobotocore (latest) with async upload\/download.\n5. Schedule periodic cleanup of stale sessions using an async task scheduler library first released within the last 12 months.\n\nDeliverables:\n- A brief list of the chosen third-party libraries (with exact versions) that meet the \u201cfirst released within the last 12 months\u201d requirement.\n- Complete, runnable Python 3.11+ code (under 150 lines) fulfilling the above requirements, with clear comments indicating where and how each external library is used.","question_index":78,"modules":["litestar","strawberry-graphql","tortoise-orm","aiobotocore"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4o","question":"Scenario:\nYou are a senior software engineer tasked with building a real-time AI summarization microservice.\n\nFunctional Requirements:\n1. Implement an async RESTful POST endpoint `\/summarize` that accepts JSON `{ \"text\": \"<string>\" }` and returns a summary generated via `llama-cpp-python`.\n2. Implement a WebSocket endpoint `\/stream` that streams incremental summary tokens to the client as they are produced.\n3. Persist each original text and its summary in an async SQLite database using `SQLModel`.\n4. Cache recent summaries in Redis for 10 minutes with `aioredis` to avoid redundant computation.\n5. Expose Prometheus-compatible metrics on `\/metrics` (request counts, latency histograms, cache hit\/miss) via `prometheus-client`.\n6. Use a background task to warm up the Llama model at startup and to periodically clean up expired cache entries.\n\nDeliverables:\n- A restatement of the functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Code must use the latest versions of FastAPI, SQLModel, aioredis, llama-cpp-python, and prometheus-client as of 2025-05-06.\n- Include clear comments indicating where each external library is used.","question_index":54,"modules":["fastapi","fastapi_prometheus","sqlmodel","llama_cpp","aioredis","databases","uvicorn","prometheus_client"],"modulenotfound":["fastapi_prometheus","llama_cpp"],"modulenotfound_count":2,"module_count":8}
{"model":"gpt-4o","question":"You are a senior software engineer tasked with building a high-throughput real-time sentiment analysis microservice.\n\nFunctional Requirements:\n1. Use FastAPI (latest) to expose a POST \/analyze endpoint that accepts JSON text payloads and stores request metadata in PostgreSQL via SQLModel (latest).\n2. Secure all endpoints with OAuth2 JWT authentication using Authlib (latest), leveraging its PKCE support.\n3. Perform real-time sentiment analysis on incoming text using the latest Hugging Face Transformers pipeline with GPU acceleration via PyTorch (latest) and cache results in Redis using aiocache (latest).\n4. Broadcast live sentiment scores to connected clients over WebSockets using python-socketio (latest) and run the server on Uvicorn with HTTP\/3 support.\n5. Asynchronously upload any user-submitted audio snippets to S3-compatible storage via aiobotocore (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all external libraries as of 2025-05-06  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":62,"modules":["fastapi","sqlmodel","authlib","transformers","torch","aiocache","aioredis","aiobotocore","socketio","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gpt-4o","question":"Scenario: Build a high-performance Python web application combining HTTP\/3 REST, GraphQL, real-time WebSockets, async database operations, data analytics, AI summarization, and distributed tracing.\n\nFunctional Requirements:\n1. HTTP\/3 REST API: use FastAPI (latest as of 2025-05-06) with Uvicorn HTTP\/3 support to serve \/items endpoints.\n2. Async GraphQL API: use Strawberry (latest as of 2025-05-06) to expose a GraphQL schema with DataLoader for batch fetching.\n3. Async DB access: use SQLModel (latest as of 2025-05-06) with an async SQLAlchemy engine to perform PostgreSQL CRUD.\n4. Real-time WebSockets: use websockets library (latest as of 2025-05-06) to broadcast item updates at \/ws.\n5. Data analytics: use Polars (latest as of 2025-05-06) lazy API to compute summary statistics on items.\n6. AI summarization: use OpenAI Python SDK (latest as of 2025-05-06) to generate summaries of item descriptions.\n7. Observability: use OpenTelemetry Python SDK (latest as of 2025-05-06) to instrument all endpoints for distributed tracing.\n\nDeliverables:\n- requirements.txt listing exact latest versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments marking where each external library is used.","question_index":53,"modules":["fastapi","strawberry-graphql","sqlmodel","sqlalchemy","polars","openai","websockets","opentelemetry-api","opentelemetry-instrumentation-fastapi"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4o","question":"Scenario: You are a senior software engineer building a real-time sentiment and OCR-enhanced geospatial chat dashboard for remote field teams.\n\nFunctional Requirements:\n1. Use FastAPI (latest version as of 2025-05-06) to implement an async REST API that serves a minimal HTML client.\n2. Implement bi-directional real-time chat via Socket.IO using python-socketio (latest) on the FastAPI server.\n3. Perform on-the-fly sentiment analysis of text messages using Hugging Face\u2019s transformers pipeline (latest) and broadcast sentiment scores.\n4. Accept image uploads from clients, extract embedded text with EasyOCR (latest), and inject the OCR result into the chat stream.\n5. Plot message geolocation metadata on a Folium (latest) map and serve it as an embeddable HTML snippet.\n\nDeliverables:\n- A requirements list enumerating the exact \u201clatest\u201d third-party libraries and versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":2,"modules":["fastapi","python-socketio","transformers","easyocr","folium","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: Develop a Python microservice that supports AI-driven product imagery, real-time stock updates, secure user authentication, and checkout processing.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) app with JWT-based auth using Authlib (latest) and secure password hashing.  \n2. Define async ORM models for Users and Products with SQLModel (latest) on PostgreSQL, including migration setup.  \n3. On product creation, generate an AI image via Diffusers (latest) or Stability-SDK (latest) and expose it via a REST endpoint.  \n4. Provide a WebSocket endpoint for real-time inventory updates using Starlette\u2019s WebSocket support (latest).  \n5. Create a \/checkout POST endpoint integrating Stripe\u2019s Python SDK (latest) with webhook handling to confirm payments.\n\nDeliverables:\n\u2022 A bullet list of the \u201clatest\u201d libraries (with pip install specifiers as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":85,"modules":["fastapi","sqlmodel","authlib","passlib","stripe","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: You are developing a high-performance, real-time AI-powered web service for personalized content recommendations.\n\nFunctional Requirements:\n1. Implement a RESTful POST \/recommend endpoint using the latest Python web framework released within the last 12 months, accepting JSON input and returning JSON responses.\n2. Add a WebSocket \/ws endpoint for streaming live recommendation updates using the newest asyncio-based WebSocket library from the past year.\n3. Integrate semantic search over user profiles by connecting to a vector database via its newest Python client library released within the last 12 months.\n4. Schedule periodic model retraining in a background worker using the most recent Python task scheduler released in the last 12 months.\n5. Store and serve user-uploaded media files on cloud object storage using the latest cloud storage client library published within the past 12 months.\n\nDeliverables:\n\u2022 A requirements list (requirements.txt) specifying the exact library names and versions (all released within the last 12 months).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total, with clear comments marking where and why each external library is used.","question_index":71,"modules":["fastapi","uvicorn","websockets","apscheduler","minio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4o","question":"Scenario: Develop a high-performance Python microservice that ingests real-time social media posts via WebSockets, generates AI summaries, stores them in MongoDB Atlas, serves secured HTTP\/3 endpoints, and provides an interactive Streamlit dashboard for sentiment trends.\n\nFunctional Requirements:\n1. Initialize an asynchronous FastAPI app with HTTP\/3 (h2\/h3) and ASGI support.  \n2. Protect all HTTP endpoints with OAuth2 JWT using Authlib\u2019s latest OAuth library.  \n3. Consume a WebSocket stream of JSON posts from wss:\/\/stream.example.com asynchronously.  \n4. Summarize each post using OpenAI\u2019s latest Python SDK leveraging function-calling or embeddings.  \n5. Persist summaries and metadata in MongoDB Atlas via Motor (async driver).  \n6. Expose a secured REST endpoint `\/summaries` supporting pagination and filtering by sentiment.  \n7. Build a Streamlit dashboard that fetches `\/summaries` and visualizes sentiment trends over time with Plotly.\n\nDeliverables:\n- A requirements list naming the latest versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":28,"modules":["fastapi","authlib","motor","openai","plotly","streamlit","hypercorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"Scenario: Build a real-time HTTP\/3-powered AI chat service that streams user messages, generates LLM responses, persists embeddings in a vector DB, and exposes chat history via GraphQL.\n\nFunctional Requirements:\n1. HTTP\/3 streaming: use the latest aioquic (as of 2025-05-06) to handle bidirectional HTTP\/3 connections for chat.\n2. On-device LLM inference: integrate the latest llama-cpp-python for quantized model loading and response generation.\n3. Vector storage: use the latest chromadb Python client to embed each message (via llama-cpp-python\u2019s embed API) and store\/retrieve vectors.\n4. GraphQL API: implement an async GraphQL schema with the latest strawberry to query chat history from ChromaDB.\n5. Async orchestration: ensure end-to-end async I\/O under Python 3.11+, coordinating the QUIC server, LLM, and DB.\n6. Code constraints: keep the complete solution under 150 lines, include clear comments marking where each external library is used.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of aioquic, llama-cpp-python, chromadb, and strawberry (latest as of 2025-05-06)  \n\u2022 A single Python 3.11+ script (<150 lines) with runnable code and comments identifying each external library usage.","question_index":79,"modules":["aioquic","llama-cpp-python","chromadb","strawberry-graphql"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4o","question":"Scenario: Design and implement a high-performance real-time chat backend with semantic search and background processing.\n\nFunctional Requirements:\n1. Asynchronous REST API: use FastAPI (latest as of 2025-05-06) to expose user and message endpoints.\n2. HTTP\/3 WebSockets: implement real-time chat over WebSockets with Hypercorn\u2019s HTTP\/3 support.\n3. Async ORM persistence: store users and messages in PostgreSQL via SQLModel (async).\n4. Vector semantic search: index and query messages in Qdrant using the latest qdrant-client.\n5. Background sentiment analysis: enqueue and run analysis jobs with ARQ (async Redis-backed queue).\n6. JWT authentication: secure all endpoints with python-jose for JWT issuance and validation.\n7. Redis caching: cache active WebSocket connections or session data via redis.asyncio.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06  \n\u2022 A single Python 3.11+ file under 150 lines that:\n  \u2013 Is fully runnable  \n  \u2013 Shows clear comments where each external library is used  \n  \u2013 Integrates all above requirements into a cohesive application","question_index":11,"modules":["fastapi","sqlmodel","qdrant-client","jose","arq","redis-asyncio","uvicorn"],"modulenotfound":["redis-asyncio"],"modulenotfound_count":1,"module_count":7}
{"model":"gpt-4o","question":"Scenario: A fintech startup requires a live transaction monitoring service that ingests, processes, stores, and notifies users of suspicious activity in real time.\n\nFunctional Requirements:\n1. Build an HTTP endpoint to receive and stream-respond to JSON transaction payloads using the latest async web framework released in the last 12 months.\n2. Validate and serialize incoming data with the latest data validation library, enforcing strict type constraints and custom validators.\n3. Persist transactions asynchronously in a relational database using the latest async ORM library compatible with Python 3.11+.\n4. Offload CPU-intensive anomaly detection to a background worker queue using the latest task orchestration library.\n5. Broadcast real-time alerts to connected clients via WebSockets using the latest real-time communication library.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of the latest libraries (as of May 6, 2025)  \n\u2022 A single Python 3.11+ file under 150 lines, complete and runnable, with clear comments indicating where each external library is used.","question_index":39,"modules":["fastapi","pydantic","databases","sqlalchemy","celery"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4o","question":"Scenario: Build a real-time collaborative document editor service with AI-powered summarization, OAuth2 access control, WebSocket syncing, Redis caching, and NoSQL persistence.\n\nFunctional Requirements:\n1. Use the latest async Python web framework (as of 2025-05-06) to define RESTful CRUD endpoints for documents.\n2. Implement OAuth2 authorization with the latest Python OAuth library released within the last 12 months for secure user login and token issuance.\n3. Establish bi-directional real-time document synchronization over WebSockets using the newest real-time communication library (with built-in pub\/sub).\n4. Integrate the latest AI inference library published in the past year to generate live summaries of incremental document edits.\n5. Employ the latest Redis client library (released within the last year) to cache active sessions and coordinate pub\/sub channels for syncing.\n6. Persist document state and version history in a NoSQL database using the newest async database client released in the last 12 months.\n\nDeliverables:\n1. A numbered list of the chosen \u201clatest\u201d libraries (with a one-line description of their role).\n2. Complete, runnable Python 3.11+ code under 150 lines, including clear comments indicating where and why each external library is used.","question_index":86,"modules":["fastapi","authlib","motor","aioredis","openai"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4o","question":"Scenario: Develop a user registration microservice over HTTP\/3 that validates input, persists user data, and integrates with an external email verification API.\n\nFunctional Requirements:\n1. Expose a POST \/users endpoint using Starlite (latest) with HTTP\/3 support.  \n2. Define request and response schemas with Pydantic v2 (latest) for data validation.  \n3. Persist user records in PostgreSQL via the latest Prisma Python client, including model definitions and migrations.  \n4. Perform an asynchronous HTTP\/3 request to an external email verification service using httpx (latest), and incorporate its response.  \n5. Return a JSON payload containing the stored user data and email verification status.\n\nDeliverables:\n- A list of required third-party libraries with exact versions (as of today)  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":100,"modules":["starlite","pydantic","prisma","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4o","question":"Scenario: As a senior software engineer, build a modular web application that authenticates users with WebAuthn passkeys, streams real-time stock data, generates AI trading signals, processes payments, and delivers interactive dashboards.\n\nFunctional Requirements:\n1. Implement user authentication via WebAuthn using the latest pywebauthn library (as of 2025-05-06) with hardware security key support.\n2. Ingest and stream real-time stock price data over WebSockets using FastAPI and the latest Starlette HTTP\/2 push features.\n3. Generate AI-driven trading signals leveraging the latest OpenAI Python SDK\u2019s new function-calling interface.\n4. Integrate secure payment processing with the latest Stripe Python SDK supporting Payment Element and webhook handling.\n5. Serve interactive dashboards using the latest Plotly Dash or Reflex library for live data visualization and client-side callbacks.\n6. Add structured logging and Prometheus metrics using structlog and prometheus-client.\n\nDeliverables:\n- A concise mapping of each functional requirement to its implementation approach.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used and using the latest libraries available as of 2025-05-06.","question_index":56,"modules":["fastapi","starlette","pywebauthn","openai","stripe","plotly","dash","structlog","prometheus_client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4o","question":"You are a senior software engineer tasked with architecting a small, high-performance async web application using only the latest Python libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Litestar framework (with native HTTP\/3 support) to define async REST endpoints for user signup and profile retrieval.  \n2. Integrate the latest Piccolo-ORM to define a PostgreSQL \u201cusers\u201d table and perform async CRUD operations.  \n3. Implement real-time notifications via WebSocket using the latest python-socketio in asyncio mode.  \n4. Schedule a daily summary email job with APScheduler\u2019s new AsyncIOScheduler and send mail via an async SMTP library.  \n5. Fetch external data during signup from a third-party API using HTTPX\u2019s HTTP\/2 client.\n\nDeliverables:\n\u2022 A requirements.txt listing exact package names and latest versions.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":29,"modules":["litestar","piccolo","python-socketio","apscheduler","aiosmtplib","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: You are a senior software engineer building a high-performance real-time IoT analytics backend.\n\nFunctional Requirements:\n1. Expose an asynchronous HTTP API using FastAPI with WebSocket support to ingest and broadcast live sensor readings.\n2. Persist incoming data in PostgreSQL via SQLModel\u2019s async ORM and validate payloads with Pydantic v2 models.\n3. Enforce per-device rate limiting using Redis Streams and aioredis, leveraging RedisJSON for overflow handling.\n4. Secure all endpoints with OAuth2 passwordless login via FastAPI-Users, sending one-time codes.\n5. Schedule background aggregation jobs with Celery (beat scheduler) to compute rolling metrics and push updates over WebSockets.\n6. Provide a paginated historical metrics REST endpoint, caching hot queries in Redis for low-latency responses.\n\nDeliverables:\n- A list of required external libraries pinned to the latest versions available as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is utilized.","question_index":27,"modules":["fastapi","fastapi-users","sqlmodel","aioredis","redis","celery","pydantic","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gpt-4o","question":"Scenario: Build a real-time collaborative document review service with semantic search and AI summarization.\n\nFunctional Requirements:\n1. HTTP API (HTTP\/3) using the latest starlite (v1.x) to manage documents and user sessions.  \n2. Real-time updates over WebSockets for live editing and presence using starlite\u2019s WebSocket support.  \n3. Semantic indexing and search: on each document save, compute embeddings and store\/retrieve via the latest qdrant-client.  \n4. AI summarization endpoint: call an LLM using the latest llm-tools library to generate concise summaries on demand.  \n5. Background task scheduling with dramatiq (latest) to clean up stale sessions and regenerate indexes.\n\nDeliverables:\n\u2022 Reiterate the numbered requirements.  \n\u2022 A complete, runnable Python 3.11+ code file under 150 lines.  \n\u2022 Use the latest starlite, qdrant-client, llm-tools, and dramatiq libraries available as of 2025-05-06.  \n\u2022 Include clear comments indicating where and why each external library is used.","question_index":55,"modules":["starlite","qdrant-client","llm-tools","dramatiq"],"modulenotfound":["llm-tools"],"modulenotfound_count":1,"module_count":4}
{"model":"gpt-4o","question":"Scenario: You are building a high-performance, AI-driven text-generation microservice for a real-time web application.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/generate endpoint using FastAPI (latest 0.100+) for incoming requests.\n2. Validate and parse the request payload with Pydantic v2 (latest) leveraging its new ArgModel feature.\n3. Enqueue each generation request as an asynchronous job using ARQ (latest 0.7+) backed by Redis.\n4. In the ARQ worker, invoke vLLM (latest 0.7+) AsyncLLMClient to perform streaming text inference.\n5. Persist each request and its full response to PostgreSQL using SQLModel (latest 0.0.x) with SQLAlchemy 2.0 async engine.\n6. Expose a WebSocket \/ws\/stream endpoint in FastAPI to broadcast partial LLM outputs to clients in real time.\n\nDeliverables:\n\u2022 The above numbered requirements list.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":30,"modules":["fastapi","pydantic","arq","vllm","sqlmodel","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: You must build a Python microservice that fetches articles by URL, summarizes them with AI, stores results in Postgres, and provides REST, GraphQL, and WebSocket interfaces with background orchestration using only the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a POST \/submit REST endpoint using the latest Starlite framework (released within the last 12 months) over HTTP\/3 to accept JSON { \"url\": string }.  \n2. Fetch article content asynchronously using the latest httpx library and handle timeouts and redirects.  \n3. Summarize text with the latest HuggingFace Transformers v5 pipeline, leveraging GPU if available.  \n4. Persist the original URL and its summary in Postgres via Tortoise ORM v0.25 (async ORM released in the last 12 months).  \n5. Expose a \/graphql endpoint using Strawberry GraphQL (latest release) to query summaries by URL.  \n6. Provide a \/ws\/notifications WebSocket in Starlite to notify clients in real time when a summary is ready.  \n7. Orchestrate fetch-and-summarize tasks as background jobs using Prefect 2 (newly released workflow orchestrator).\n\nDeliverables:\n- A copy of the numbered functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Use only the latest versions of Starlite, httpx, Transformers v5, Tortoise ORM v0.25, Strawberry GraphQL, and Prefect 2 as of 2025-05-06.\n- Include clear comments indicating where and how each external library is used.","question_index":70,"modules":["starlite","httpx","transformers","tortoise-orm","strawberry-graphql","prefect"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: You are tasked with developing a next-generation chat analytics platform that seamlessly integrates HTTP\/3, JWT auth, real-time messaging, vector search, and on-device LLM inference.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) with HTTP\/3 support via Hypercorn to serve all endpoints.\n2. Implement JWT authentication with rotational refresh tokens using fastapi-users (latest).\n3. Provide a real-time WebSocket chat endpoint using fastapi-websocket-pubsub (latest) supporting pub\/sub channels.\n4. On each incoming chat message, generate vector embeddings using llama-cpp-python (latest) with an on-device streaming pipeline, and store them in ChromaDB (latest) with HNSW indexing.\n5. Expose a REST endpoint to query semantic similarity: accept a text query, compute embeddings, perform a ChromaDB search, and return the top-3 similar messages.\n6. Expose a streaming summarization endpoint that returns an on-the-fly summary of recent messages using llama-cpp-python\u2019s streaming completion interface.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements you\u2019ve implemented.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":93,"modules":["fastapi","fastapi_users","hypercorn","fastapi_websocket_pubsub","llama_cpp","chromadb","uvicorn","sqlalchemy"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":8}
{"model":"gpt-4o","question":"Scenario: As a senior software engineer, develop a Python 3.11+ microservice that manages real-time collaborative whiteboard sessions, persists data, exposes REST and GraphQL APIs, generates session snapshots, integrates external metrics, and secures endpoints with OAuth2.\n\nFunctional Requirements:\n1. Implement session and user management REST endpoints using FastAPI (latest) with async event hooks and Pydantic V2 models.  \n2. Persist all data in PostgreSQL via Tortoise ORM (latest) using model lifecycle events.  \n3. Expose a GraphQL API with Strawberry GraphQL (latest) dataclass resolvers for session and user queries\/mutations.  \n4. Broadcast real-time drawing events using python-socketio (latest) over WebSockets.  \n5. Generate on-demand PNG snapshots of whiteboard state with Pillow-SIMD (latest) leveraging multi-threaded rendering.  \n6. Send session metrics asynchronously to an external analytics service using httpx (latest) with HTTP\/2 support.  \n7. Secure all endpoints with OAuth2 (authorization code + PKCE) using Authlib (latest).  \n\nDeliverables:\n- A numbered list of the functional requirements above.  \n- Complete, runnable Python 3.11+ code (\u2264 150 lines) integrating all requirements.  \n- Inline comments indicating where each external library is used.  \n- Use the latest stable releases of all libraries as of today\u2019s date.","question_index":50,"modules":["fastapi","pydantic","tortoise","strawberry","socketio","pillow","httpx","starlette","authlib"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4o","question":"Scenario: You are a senior software engineer tasked with building a real-time analytics microservice that combines REST, GraphQL, persistent storage, and background processing using the latest Python ecosystem tools as of 2025-05-06.\n\nFunctional Requirements:\n1. Expose HTTP and WebSocket endpoints using the latest Litestar ASGI framework with annotated routing and streaming support.\n2. Implement a GraphQL API using the latest Strawberry GraphQL code-first schema builder with asyncio support.\n3. Persist and query analytics data in PostgreSQL via the latest Piccolo-ORM\u2019s async-first models and automatic migrations.\n4. Offload long-running analytics jobs to a Redis-backed async queue using the latest Arq library.\n5. Send WebSocket notifications to clients when background jobs complete.\n\nDeliverables:\n\u2022 A requirements list (pip install commands) specifying the latest versions of Litestar, Strawberry, Piccolo-ORM, and Arq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":65,"modules":["litestar","strawberry-graphql","piccolo","arq","aioredis","python-dotenv"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: You are a senior software engineer tasked with developing an end-to-end Python web application that leverages the latest libraries to showcase advanced API development, ORM integration, security, background processing, and real-time features.\n\nFunctional Requirements:\n1. Asynchronous REST API: implement CRUD endpoints for a `Task` model using FastAPI (latest version as of 2025-05-06).\n2. Data Modeling and Persistence: define `Task` ORM models with SQLModel (latest version) and connect to a SQLite or PostgreSQL database asynchronously.\n3. Security: secure all API endpoints with OAuth2 password flow using Authlib (latest version) to obtain and validate JWT tokens.\n4. Background Processing: create an asynchronous task queue with Arq (latest version) to process tasks in the background.\n5. Real-Time Notifications: establish a WebSocket endpoint using the websockets library (latest version) to push real-time task status updates.\n\nDeliverables:\n- The numbered list of functional requirements (as above).\n- Complete, runnable Python 3.11+ code under 150 lines. Use the latest versions of FastAPI, SQLModel, Authlib, Arq, and websockets as of 2025-05-06. Include clear comments indicating where and how each external library is used.","question_index":24,"modules":["fastapi","sqlmodel","authlib","arq"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4o","question":"Scenario: Build a real-time collaborative messaging microservice integrating graph storage, vector search, JWT authentication, and WebSockets using the latest Python libraries (as of May 6, 2025).\n\nFunctional Requirements:\n1. Authentication: secure all endpoints with JWTs using the latest TokenGuard library.\n2. Message Ingestion: implement POST \/messages to accept user messages; decode JWT, extract user ID.\n3. Graph Storage: store each message and its \u201cauthored_by\u201d relationship in a graph database via the latest GraphitORM.\n4. Vector Indexing & Search: generate text embeddings with the latest Morpher library, store them in its vector store, and expose GET \/search?q={text} to return top-5 similar messages.\n5. Real-Time Updates: broadcast new messages to connected clients over WebSockets using the latest Voila framework.\n\nDeliverables:\n1. A requirements list with each third-party library pinned to its latest version as of May 6, 2025.\n2. A single Python 3.11+ script (\u2264150 lines) implementing all endpoints, with clear comments marking where each external library is used.","question_index":35,"modules":["fastapi","grapthitorm","morpher","token_guard"],"modulenotfound":["grapthitorm","morpher","token_guard"],"modulenotfound_count":3,"module_count":4}
{"model":"gpt-4o","question":"You are tasked with building a high-performance text summarization service with real-time updates, persistent storage, and caching for internal research teams.\n\nFunctional Requirements:\n1. Implement OAuth2 password flow with JWT tokens for user authentication using the latest python-jose.\n2. Expose a POST \/summarize endpoint that uses the latest transformers library to perform text summarization.\n3. Provide a WebSocket at \/ws\/progress to stream summarization progress in real time using FastAPI\u2019s latest WebSocket support.\n4. Persist user requests and summaries in a SQLite database via the latest SQLModel ORM.\n5. Cache summary results in Redis for 5 minutes using the latest aioredis library to accelerate repeat requests.\n\nDeliverables:\n\u2022 A requirements list specifying the latest versions of FastAPI, transformers, python-jose, SQLModel, aioredis (as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":10,"modules":["fastapi","transformers","sqlmodel","python-jose","aioredis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4o","question":"You are a senior software engineer.\n\nScenario: Build an end-to-end Python service for user profile CRUD with high-performance caching and real-time notifications.\n\nFunctional Requirements:\n1. Implement RESTful CRUD endpoints for user profiles using the latest FastAPI (as of 2025-05-06), leveraging its async lifespan context.\n2. Define an async PostgreSQL-backed User model with SQLModel (latest) using Relations and the new async engine.\n3. Cache GET \/users\/{id} responses in Redis with redis.asyncio (latest), utilizing cluster mode and automatic TTL invalidation on updates.\n4. Broadcast profile change events in real time over WebSocket using the websockets library (latest) with permessage-deflate compression.\n\nDeliverables:\n- A requirements.txt listing the latest versions of FastAPI, SQLModel, redis.asyncio and websockets as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":17,"modules":["fastapi","sqlmodel","redis","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4o","question":"Scenario: You are a senior software engineer building a real-time sentiment-analysis web service for social media feeds.\n\nFunctional Requirements:\n1. Use the latest Python web framework (released within the last 12 months) to expose REST endpoints with dependency injection and OpenAPI support.\n2. Implement real-time websocket notifications for incoming sentiment updates using a recently released WebSocket library (last 12 months).\n3. Persist incoming posts and sentiment scores in a NoSQL database via its newest async client library (released within the last 12 months).\n4. Offload sentiment inference to a CPU-optimized ML library (first released within the last 12 months) with zero-copy or memory-efficient batch inference.\n5. Schedule periodic cleanup and summary-generation tasks using a lightweight scheduler library introduced within the last 12 months.\n6. Serve a minimal interactive dashboard at \u201c\/dashboard\u201d using a modern, Python-native plotting\/UI library created in the last 12 months.\n\nDeliverables:\n- A bullet list of all third-party libraries chosen, pinned to \u201clatest\u201d versions as of today\u2019s date.\n- A single, complete Python 3.11+ file (under 150 lines) that implements all requirements, with clear comments marking where each external library is used.","question_index":94,"modules":["fastapi","websockets","motor","onnxruntime","apscheduler","dash"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: As a senior software engineer, you must build a real-time analytics backend that ingests user events, persists them, and serves both REST and GraphQL clients with low latency and live updates.\n\nFunctional Requirements:\n1. Implement a POST \/events endpoint using the latest FastAPI (as of 2025-05-06), leveraging its new lifespan context and async router features.\n2. Validate incoming JSON payloads with Pydantic v2\u2019s new @model_validator decorator for strict schema enforcement.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise ORM, utilizing its recent bulk_create optimization.\n4. Expose a GraphQL query and mutation schema via the latest Strawberry GraphQL with code-first federation support.\n5. Broadcast new events to connected clients over WebSockets using the latest websockets library with per-message deflate compression.\n\nDeliverables:\n\u2022 A requirements.txt listing each third-party library pinned to its latest version as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments indicating where and how each external library is used.","question_index":19,"modules":["fastapi","pydantic","tortoise-orm","strawberry-graphql","websockets","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: You are a senior software engineer building a modern microblogging backend that supports authenticated posts, AI-generated images, real-time feed updates, and cloud storage integration.\n\nFunctional Requirements:\n1. Implement a REST API using a Python web framework released in the last 12 months (use the latest available as of 2025-05-06).\n2. Persist posts in a SQL database via an async ORM library created in the last 12 months, leveraging advanced async model features.\n3. Broadcast new posts to connected clients over WebSockets using a recently released high-performance async messaging library.\n4. Authenticate users with OAuth2 (Google) using the latest OAuth client library.\n5. Generate post images by calling Stable Diffusion through the most recent Python API client for AI image generation.\n6. Store and serve AI-generated images in AWS S3 using the newest AWS SDK for Python.\n7. Ensure all components run under Python 3.11+ and fit within 150 lines of code.\n\nDeliverables:\n\u2022 A numbered list of the specific libraries (with versions) you chose to satisfy each requirement.  \n\u2022 Complete, runnable Python 3.11+ code (<150 lines) combining all features.  \n\u2022 Clear inline comments marking where and why each external library is used.","question_index":7,"modules":["fastapi","tortoise-orm","stable_diffusion","boto3","aiohttp"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4o","question":"Scenario: You are a senior software engineer tasked with building a serverless real-time IoT analytics service that ingests sensor data, performs streaming aggregation, applies online anomaly detection, and streams results to clients.\n\nFunctional Requirements:\n1. Ingest JSON messages from an MQTT broker (\u201csensors\/+\/data\u201d) using the latest asyncio-based gmqtt library (as of 2025-05-06).  \n2. Stream-process incoming messages with streamz (latest version) to compute 1-minute tumbling-window averages for temperature and humidity.  \n3. Apply online anomaly detection on each windowed aggregation using River\u2019s latest HDBSCAN-based drift detector.  \n4. Persist raw messages and aggregated results into TimescaleDB via the asyncpg driver (latest version).  \n5. Expose a Python 3.11+ FastAPI application with a WebSocket endpoint for live metric and anomaly alert streaming.  \n6. Wrap the FastAPI app for AWS Lambda deployment using the latest mangum adapter.\n\nDeliverables:\n\u2022 A list of third-party dependencies pinned to \u201clatest\u201d versions as of 2025-05-06.  \n\u2022 A single, complete, runnable Python 3.11+ source file under 150 lines.  \n\u2022 Clear inline comments showing where each external library is used.  \n\u2022 The code must integrate all functional requirements and be deployable serverlessly on AWS Lambda.","question_index":38,"modules":["gmqtt","streamz","river","asyncpg","fastapi","mangum","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"Scenario:\nYou are tasked with developing a lightweight AI-powered semantic search microservice in Python 3.11+.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI framework with HTTP\/3 support) to expose a POST \/search endpoint.  \n2. Validate and parse incoming JSON with the latest Pydantic v2\u2019s strict model parsing and Python pattern-matching features.  \n3. Asynchronously fetch text embeddings from OpenAI\u2019s embedding API using the latest HTTPX client with HTTP\/3.  \n4. Store and query vectors in a GPU-accelerated ChromaDB instance via the latest ChromaDB Python client.  \n5. Return the top 5 semantically related document IDs and similarity scores as a JSON response.\n\nDeliverables:\n\u2022 A numbered list confirming the above functional requirements.  \n\u2022 A single, complete, runnable Python 3.11+ source file (\u2264150 lines) that implements the microservice, with clear comments indicating where each external library is used.","question_index":3,"modules":["starlite","pydantic","httpx","chromadb"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gpt-4o","question":"Scenario: Build a real-time chat analysis service that ingests user messages, performs on-device LLM summarization and sentiment analysis, caches results, persists chat history, and broadcasts updates via WebSockets.\n\nFunctional Requirements:\n1. Expose a POST \/messages endpoint using FastAPI (latest) to receive JSON payloads with \u201cuser_id\u201d and \u201ctext\u201d.\n2. Perform sentiment analysis and summarization on each message using llama-cpp-python (latest) in an async background task.\n3. Cache sentiment results for identical texts in Redis via redis-py (latest) to avoid recomputation.\n4. Persist messages, sentiments, and summaries in PostgreSQL using SQLModel (latest version).\n5. Provide a WebSocket \/ws\/{user_id} endpoint (FastAPI) that broadcasts new summaries and sentiments to connected clients in real time.\n6. Secure all endpoints with OAuth2 password flow via Authlib (latest).\n\nDeliverables:\n\u2022 Restate the numbered requirements above.  \n\u2022 Provide complete, runnable Python 3.11+ code integrating the latest versions of FastAPI, llama-cpp-python, redis-py, SQLModel, and Authlib, under 150 lines total.  \n\u2022 Include clear comments indicating where and how each external library is used.","question_index":32,"modules":["fastapi","sqlmodel","redis","llama_cpp","authlib"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":5}
{"model":"gpt-4o","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python microservice for real-time IoT sensor analytics and visualization.\n\nFunctional Requirements:\n1. REST API: Use the latest asynchronous Python web framework available as of 2025-05-06 to expose CRUD endpoints with advanced dependency injection and OpenAPI schema generation.\n2. WebSockets: Integrate the latest real-time streaming library as of 2025-05-06 supporting protocol-level backpressure to push live sensor updates to clients.\n3. ML Inference: Embed an on-the-fly analytics model using the latest Python JIT-accelerated inference library as of 2025-05-06, auto-batching requests on CPU\/GPU.\n4. Graph Storage: Persist sensor relationships in a distributed graph database via the latest async driver as of 2025-05-06 with reactive query pipelining.\n5. Dashboard UI: Serve an interactive web dashboard using the latest server-driven UI component library as of 2025-05-06 for real-time charting and controls.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":14,"modules":["fastapi","websockets","torch","neo4j","streamlit","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"You are a Senior Software Engineer tasked with building a high-performance, real-time analytics microservice using the latest asynchronous Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled ASGI endpoint using the latest Litestar and aioquic (as of 2025-05-06) for bi-directional streaming.\n2. Implement real-time GraphQL subscriptions with the latest Strawberry GraphQL to push analytics updates.\n3. Define domain models and perform asynchronous CRUD operations on Postgres using the latest SQLModel.\n4. Integrate a Redis cache layer with the latest aioredis for hot-path data retrieval.\n5. Instrument request handling and background tasks with OpenTelemetry SDK for distributed tracing.\n\nDeliverables:\n- requirements.txt listing the latest library versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments indicating where each external library is used.","question_index":90,"modules":["uvicorn","litestar","strawberry-graphql","sqlmodel","aioredis","databases","opentelemetry-sdk","opentelemetry-api","opentelemetry-instrumentation-asgi"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gpt-4o","question":"You are building a real-time collaborative code-review web application that leverages the latest AI, async, storage and security frameworks.\n\nFunctional Requirements:\n1. Implement a FastAPI 0.95+ server with async endpoints and Jinja2 templates for the main UI.\n2. Enable real-time code editing and review comments via WebSockets using the latest websockets library as of 2025-05-06.\n3. On each code submission, stream analysis from OpenAI\u2019s GPT-4o API (or equivalent newest model) and display suggestions in real time.\n4. Persist code snapshots and review metadata in Pinecone\u2019s newest Python client (vector storage) for similarity search.\n5. Secure all HTTP and WS routes with JWT authentication using the latest PyJWT release.\n6. Add structured logging and a \/healthz endpoint using structlog for observability.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total.  \n\u2022 Clear comments indicating where and why each external library is used.","question_index":37,"modules":["jwt","structlog","fastapi","openai","pinecone"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gpt-4o","question":"Scenario: Build a real-time Q&A web application that ingests documents, generates embeddings, serves semantic search, supports live WebSocket notifications, and exposes observability metrics.\n\nFunctional Requirements:\n1. Use the latest FastAPI (>=0.100) to implement asynchronous REST endpoints for document ingestion and search, plus a WebSocket endpoint leveraging its new lifecycle event hooks.\n2. Upon document ingestion, asynchronously generate embeddings with the latest llama-cpp-python (v0.2+) using 4-bit quantization and store vectors in Pinecone (latest) using hybrid search.\n3. Implement a `\/search` endpoint that queries Pinecone for nearest neighbors based on user input and returns ranked results.\n4. Schedule periodic re-indexing tasks with the latest arq (v1.10+), utilizing its async Redis job scheduler.\n5. Integrate OpenTelemetry Python SDK (v1.21+) for auto-instrumentation, exporting traces and metrics to Prometheus.\n\nDeliverables:\n- A requirements.txt listing the latest libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":33,"modules":["fastapi","llama-cpp-python","pinecone-client","arq","opentelemetry-sdk","opentelemetry-exporter-prometheus","opentelemetry-instrumentation-fastapi"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"Scenario: As a senior software engineer, you must build a modular Python web application that serves real-time, ML-enhanced classification results with persistent storage and observability.\n\nFunctional Requirements:\n1. API Layer: Use the latest ASGI web framework available as of 2025-05-06 that supports HTTP\/3 and automatic OpenAPI generation to expose a POST \/classify endpoint.  \n2. Database Layer: Connect asynchronously to PostgreSQL using the latest async driver with statement pooling to record incoming requests and classification results.  \n3. Real-time Pub\/Sub: Implement server-side WebSocket broadcasts of classification results using the latest message queue library offering at-least-once delivery semantics.  \n4. ML Inference: Integrate on-demand image classification via the latest lightweight, GPU-accelerated inference engine released in the past 12 months.  \n5. Observability: Instrument distributed tracing and metrics collection using the latest OpenTelemetry-compatible tracing library.\n\nDeliverables:\n- A list of the latest library dependencies as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":18,"modules":["fastapi","asyncpg","aioredis","onnxruntime","opentelemetry-api","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: As a senior software engineer, build a high-performance microblogging API using the latest Python libraries for REST, validation, async database operations, caching, background tasks, and real-time streaming.\n\nFunctional Requirements:\n1. Implement a user signup endpoint in FastAPI (>=0.100.0) using Pydantic v2 root_validator for advanced schema validation.  \n2. Create and list posts asynchronously in PostgreSQL via SQLModel\u2019s latest async engine.  \n3. Maintain a cache of the 10 most recent posts in Redis using redis-py (>=5.0) asyncio client, updating on post creation.  \n4. Offload image thumbnail generation to an async Dramatiq (>=2.0) background task triggered on new post with image.  \n5. Provide a WebSocket endpoint in FastAPI to broadcast newly created posts to connected clients in real time.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library (FastAPI, Pydantic v2, SQLModel, redis-py asyncio, Dramatiq) is used. Use the latest available versions of all libraries as of 2025-05-06.","question_index":87,"modules":["fastapi","pydantic","sqlmodel","redis","dramatiq","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gpt-4o","question":"Scenario: You are building a real-time news aggregation microservice with live client updates.\n\nFunctional Requirements:\n1. Develop a REST API using Litestar (the latest version as of 2025-05-06) leveraging its OpenAPI v3.1 auto-generation feature.\n2. Asynchronously fetch and parse external RSS feeds over HTTP\/3 using HTTPX AsyncClient (the latest), then cache results in aiocache (the latest) with a TTL.\n3. Validate and serialize each article using Pydantic v2 (the latest) functional-style models to ensure data integrity.\n4. Expose a WebSocket endpoint using the websockets library (the latest) with permessage-deflate compression for real-time article broadcasting.\n5. Schedule periodic background tasks with Arq (the latest) to re-poll feeds every 5 minutes.\n\nDeliverables:\n- A list of the chosen libraries and their latest versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":31,"modules":["litestar","httpx","aiocache","pydantic","websockets","arq","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"Scenario: Build a real-time collaborative note-taking microservice integrating REST, database persistence, caching, GraphQL, WebSockets, and background tasks.\n\nFunctional Requirements:\n1. Expose REST endpoints using FastAPI (latest) for creating, updating and retrieving notes; leverage Pydantic v2\u2019s new validation decorators.\n2. Persist notes asynchronously in SQLite via SQLModel (latest) with SQLAlchemy 2.0-style typing and async engine.\n3. Broadcast note creation and updates over WebSockets with FastAPI\u2019s WebSocket support to subscribed clients in real time.\n4. Cache note retrieval results in Redis using aioredis (latest), implement TTL and cache invalidation on updates.\n5. Provide a GraphQL schema and endpoint using Strawberry (latest) to query and filter notes with federation-style resolvers.\n6. Schedule a daily summary email using FastAPI BackgroundTasks and aiosmtplib (latest) for asynchronous SMTP delivery.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- A single Python 3.11+ source file under 150 lines that implements all requirements, with clear comments marking where each external library is used.","question_index":73,"modules":["fastapi","pydantic","sqlmodel","aioredis","strawberry-graphql","uvicorn","aiosmtplib"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gpt-4o","question":"Scenario: You are a senior software engineer building a proof-of-concept Python service that ingests user text, generates embeddings and classifications via an LLM, stores them, and streams nearest-neighbor results in real time.\n\nFunctional Requirements:\n1. Implement a JWT-secured POST \/ingest endpoint using the latest async Python web framework as of 2025-05-06 (e.g., Litestar).  \n2. In \/ingest, accept JSON `{ \"text\": \"...\" }`, call a state-of-the-art LLM (via the latest Transformers or LangChain client) to produce both an embedding vector and a classification label.  \n3. Persist the embedding vector in a vector database using the newest ChromaDB client.  \n4. Log the original text and its classification into a relational database through the latest async ORM (e.g., Prisma Python).  \n5. Expose a GET \/stream endpoint that streams nearest-neighbor matches for incoming embeddings over server-sent events using the newest SSE library (e.g., httpx-sse or framework-native support).  \n6. Target Python 3.11+, include type hints, and keep all code under 150 lines.\n\nDeliverables:\n\u2022 A `requirements.txt` (or equivalent) listing each library with exact versions (latest as of 2025-05-06).  \n\u2022 A single Python module (\u2264150 lines) that meets all requirements, with clear comments marking where and why each external library is used.","question_index":81,"modules":["fastapi","httpx_sse","transformers","chromadb","prisma"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: As a senior software engineer, build a high-performance Python web application integrating real-time streaming, GraphQL, async ORM, JWT security, and caching using the latest third-party libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. HTTP & WebSocket server with the latest Litestar, utilizing its new streaming response API for real-time event delivery.\n2. GraphQL API with the latest Strawberry-GraphQL, defining queries, mutations, and subscription streams for live data updates.\n3. Async database models and CRUD operations using the latest Piccolo ORM\u2019s Python 3.11+ async support and built-in migration tooling.\n4. JWT authentication via the latest python-jose, supporting secure login, token rotation, and injection of user context into requests.\n5. Redis-backed caching with the latest aioredis to cache GraphQL query results and automatically invalidate on data changes.\n6. Trigger real-time client notifications over WebSockets when database events occur, leveraging Litestar\u2019s WebSocket routing.\n\nDeliverables:\n\u2022 A numbered list of the final functional requirements.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":57,"modules":["uvicorn","litestar","piccolo","strawberry","aioredis","python-jose"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: As a senior software engineer, build a high-performance microblogging API using the latest Python libraries for REST, validation, async database operations, caching, background tasks, and real-time streaming.\n\nFunctional Requirements:\n1. Implement a user signup endpoint in FastAPI (>=0.100.0) using Pydantic v2 root_validator for advanced schema validation.  \n2. Create and list posts asynchronously in PostgreSQL via SQLModel\u2019s latest async engine.  \n3. Maintain a cache of the 10 most recent posts in Redis using redis-py (>=5.0) asyncio client, updating on post creation.  \n4. Offload image thumbnail generation to an async Dramatiq (>=2.0) background task triggered on new post with image.  \n5. Provide a WebSocket endpoint in FastAPI to broadcast newly created posts to connected clients in real time.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library (FastAPI, Pydantic v2, SQLModel, redis-py asyncio, Dramatiq) is used. Use the latest available versions of all libraries as of 2025-05-06.","question_index":87,"modules":["fastapi","pydantic","sqlmodel","redis","dramatiq","sqlalchemy","pillow","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario:\nCreate an AI-powered article summarization service that accepts user-submitted text, stores it, generates summaries asynchronously via an AI API, caches results, and streams them back to clients in real time.\n\nFunctional Requirements:\n1. Implement a REST API using FastAPI (latest) with endpoints POST \/articles to submit text and GET \/stream\/{article_id} for server-sent events.\n2. Persist submissions in a database using SQLModel (latest) with an async SQLite or PostgreSQL engine.\n3. Orchestrate a background job queue using Celery (latest) with Redis as broker to process newly submitted articles.\n4. Perform summarization using the OpenAI Python SDK (latest) and its chat completion endpoint.\n5. Cache generated summaries in Redis via aioredis (latest) with a configurable TTL.\n6. Stream summaries to connected clients in real time using server-sent events via sse-starlette (latest).\n\nDeliverables:\n\u2022 A list of all third-party libraries (with versions pinned to the latest as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":64,"modules":["fastapi","sqlmodel","sqlalchemy","openai","celery","redis","sse-starlette","aioredis","pydantic","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Senior software engineer tasked with developing a real-time GraphQL web service that validates incoming event data, persists it to PostgreSQL, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI web framework) as of 2025-05-06 to implement an async HTTP POST endpoint at `\/events`.\n2. Validate incoming JSON payloads with Pydantic v2 (latest) models, including at least one custom field validator.\n3. Persist validated events to PostgreSQL using the latest Prisma Python client (type-safe generated models and migrations).\n4. Expose a GraphQL API with the latest Strawberry GraphQL library, supporting queries for event history and subscriptions for real-time updates.\n5. Configure WebSocket support in Starlite to broadcast new events to all GraphQL subscription clients.\n\nDeliverables:\n- A brief recap of the functional requirements.\n- Complete, runnable Python 3.11+ code under 150 lines that uses the latest versions of each library as of 2025-05-06, with clear comments marking where each external library is imported and utilized.","question_index":22,"modules":["starlite","pydantic","strawberry","prisma"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Design and implement a high-performance real-time chat backend with semantic search and background processing.\n\nFunctional Requirements:\n1. Asynchronous REST API: use FastAPI (latest as of 2025-05-06) to expose user and message endpoints.\n2. HTTP\/3 WebSockets: implement real-time chat over WebSockets with Hypercorn\u2019s HTTP\/3 support.\n3. Async ORM persistence: store users and messages in PostgreSQL via SQLModel (async).\n4. Vector semantic search: index and query messages in Qdrant using the latest qdrant-client.\n5. Background sentiment analysis: enqueue and run analysis jobs with ARQ (async Redis-backed queue).\n6. JWT authentication: secure all endpoints with python-jose for JWT issuance and validation.\n7. Redis caching: cache active WebSocket connections or session data via redis.asyncio.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06  \n\u2022 A single Python 3.11+ file under 150 lines that:\n  \u2013 Is fully runnable  \n  \u2013 Shows clear comments where each external library is used  \n  \u2013 Integrates all above requirements into a cohesive application","question_index":11,"modules":["fastapi","hypercorn","sqlmodel","qdrant-client","python-jose","redis","arq","pydantic","uvicorn","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Create a serverless real-time analytics API that ingests WebSocket events, performs on-the-fly NLP summarization, indexes embeddings, exposes a GraphQL endpoint, and deploys to AWS Lambda.\n\nFunctional Requirements:\n1. Use the latest Litestar (as of 2025-05-06) to build an HTTP\/2 GraphQL endpoint with schema-based typing.\n2. Implement a real-time WebSocket consumer using wsproto or litestar-websockets for event ingestion.\n3. Offload text summarization to an asynchronous background task leveraging the latest Transformers library.\n4. Generate embeddings with the newest OpenAI Embeddings API or sentence-transformers and store them in the latest ChromaDB vector store.\n5. Secure all endpoints with passwordless authentication via the latest fastapi-users or equivalent.\n6. Package and deploy the entire application to AWS Lambda using the latest Mangum adapter.\n\nDeliverables:\n- A numbered list of chosen \u201clatest\u201d library versions (as of 2025-05-06) mapping to each requirement.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":77,"modules":["chromadb","litestar","openai","transformers","mangum"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a real-time collaborative document review service with semantic search and AI summarization.\n\nFunctional Requirements:\n1. HTTP API (HTTP\/3) using the latest starlite (v1.x) to manage documents and user sessions.  \n2. Real-time updates over WebSockets for live editing and presence using starlite\u2019s WebSocket support.  \n3. Semantic indexing and search: on each document save, compute embeddings and store\/retrieve via the latest qdrant-client.  \n4. AI summarization endpoint: call an LLM using the latest llm-tools library to generate concise summaries on demand.  \n5. Background task scheduling with dramatiq (latest) to clean up stale sessions and regenerate indexes.\n\nDeliverables:\n\u2022 Reiterate the numbered requirements.  \n\u2022 A complete, runnable Python 3.11+ code file under 150 lines.  \n\u2022 Use the latest starlite, qdrant-client, llm-tools, and dramatiq libraries available as of 2025-05-06.  \n\u2022 Include clear comments indicating where and why each external library is used.","question_index":55,"modules":["starlite","qdrant_client","dramatiq","llm_tools"],"modulenotfound":["llm_tools"],"modulenotfound_count":1,"module_count":4}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are a senior software engineer building a high-throughput chat summarization microservice in Python 3.11+ using only the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Real-time WebSocket ingestion: implement a `\/ws` endpoint using fastapi-socketio (latest) with session storage in Redis via `redis.asyncio`.\n2. AI summarization: enqueue incoming messages in Arq (latest) and process them with the OpenAI Python SDK (latest) calling GPT-4 Turbo for summaries.\n3. Async data persistence: store raw messages and summaries in PostgreSQL using SQLModel + encode\/databases with the asyncpg driver.\n4. Dynamic admin dashboard: expose `\/admin` rendering Jinja2 templates enhanced with HTMX via fastapi-htmx (latest).\n5. Email alerts: send summary notifications asynchronously using FastAPI-Mail (latest).\n6. Containerization: provide a multi-stage Dockerfile based on `python:3.11-slim`.\n\nDeliverables:\n- A `requirements.txt` listing the exact versions of all third-party libraries (as of May 6, 2025).\n- A single `main.py` (under 150 lines) containing complete, runnable Python 3.11+ code with clear comments indicating where and how each external library is used.","question_index":23,"modules":["fastapi","fastapi-socketio","redis","arq","openai","sqlmodel","databases","asyncpg","jinja2","fastapi-htmx","fastapi-mail","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a high-performance Python microservice that combines cutting-edge web, real-time, AI and observability features into a single ASGI application.\n\nFunctional Requirements:\n1. HTTP\/3 ASGI server with automatic TLS and QUIC support using the latest HTTP\/3 library released within the past 12 months.  \n2. Real-time WebSocket endpoint that streams JSON events with zero-copy backpressure control using the newest streaming library created in the last year.  \n3. Semantic search pipeline: accept text uploads, generate embeddings, and perform vector similarity queries via the latest vector-store client released within 12 months.  \n4. On-device LLM inference endpoint that consumes streamed text segments and returns summaries using the most recent quantized model library published this year.  \n5. Distributed tracing and metrics export using the newest OpenTelemetry OTLP exporter released within the past 12 months.\n\nDeliverables:\n- A requirements list (requirements.txt) specifying the exact, latest versions of each third-party library as of 2025-05-06.  \n- A single Python 3.11+ script under 150 lines that implements all features above, with clear comments marking where each external library is used.","question_index":51,"modules":["httpx","uvicorn","starlette","aioquic","websockets","opentelemetry-api","opentelemetry-sdk","opentelemetry-exporter-otlp","sentence-transformers","faiss-cpu","torch","transformers","accelerate"],"modulenotfound":[],"modulenotfound_count":0,"module_count":13}
{"model":"claude-3-5-haiku-20241022","question":"You are tasked with building a real-time analytics web dashboard that ingests streaming data, processes it, summarizes it with AI, secures user access, and visualizes results dynamically using the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Implement a REST API in Python 3.11+ with FastAPI (HTTP\/3 support via Hypercorn) to accept JSON event streams.\n2. Broadcast processed insights in real time over WebSocket using python-socketio.\n3. Buffer incoming events and generate streaming AI summaries using the latest LangChain and OpenAI Python SDK.\n4. Perform real-time aggregation and lazy analytics on events with Polars.\n5. Produce dynamic Plotly charts of key metrics and serve them as JSON for front-end rendering.\n6. Secure all endpoints and WebSocket connections with JWT-based auth via fastapi-users.\n\nDeliverables:\n\u2022 A numbered list of the above functional requirements.  \n\u2022 A single Python 3.11+ file (under 150 lines) that fulfills all requirements, complete and runnable. Include clear comments indicating where and how each external library is used.","question_index":84,"modules":["asyncio","hypercorn","jwt","langchain","openai","plotly","polars","python-socketio","python-dotenv","fastapi","fastapi-users"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python web service that combines asynchronous REST & GraphQL APIs, real-time notifications, persistent storage, caching, and AI-driven text summarization.\n\nFunctional Requirements:\n1. Implement async HTTP endpoints with FastAPI (latest as of 2025-05-06) using Python 3.11 structural pattern matching and Pydantic v2 dataclass models.\n2. Provide a GraphQL endpoint powered by Strawberry GraphQL v1.0+ with schema federation support.\n3. Persist and query data in PostgreSQL via the latest Tortoise-ORM with asyncio-optimized connection pooling.\n4. Push real-time notifications over WebSockets using python-socketio latest async server mode.\n5. Summarize user-submitted text on demand using the latest llama-cpp-python library with optional GPU offload.\n6. Cache frequent database queries in Redis using aioredis latest cluster-mode support.\n\nDeliverables:\n- A requirements list specifying exact versions of all third-party libraries.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":74,"modules":["fastapi","uvicorn","strawberry-graphql","tortoise-orm","python-socketio","aioredis","llama-cpp-python","pydantic","asyncpg","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are building a high-performance Python backend that ingests user articles, generates AI-powered summaries, stores and caches results, and exposes them via REST, GraphQL, and real-time channels with background processing.\n\nFunctional Requirements:\n1. Accept article submissions over a REST API using FastAPI and validate payloads with Pydantic v2.  \n2. Offload summarization to a background worker with Dramatiq using Redis as a broker.  \n3. Summarize articles via the latest OpenAI Python SDK and cache summaries with aiocache.  \n4. Persist articles and summaries in SQLite using SQLModel\u2019s async ORM features.  \n5. Expose stored summaries through a GraphQL endpoint implemented with Strawberry GraphQL.  \n6. Broadcast new summary notifications to connected clients over WebSockets with python-socketio.  \n\nDeliverables:\n\u2022 List of the latest third-party libraries (as of today) and their install commands.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":63,"modules":["fastapi","uvicorn","pydantic","dramatiq","redis","openai","aiocache","sqlmodel","strawberry-graphql","python-socketio","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are tasked with building a modular Python 3.11+ microservice that delivers real-time chat with semantic search and scheduled maintenance, leveraging only the latest libraries released within the past 12 months.\n\nFunctional Requirements:\n1. Implement HTTP REST endpoints for posting and retrieving chat messages using Litestar (latest release).  \n2. Establish a WebSocket chat channel that broadcasts new messages to all connected clients via the official Litestar WebSocket plugin (latest).  \n3. Persist each message to MongoDB with Beanie ODM (latest) and simultaneously index its vector embedding into Qdrant using qdrant-client (latest).  \n4. Expose a GraphQL query endpoint that performs semantic search over stored messages via strawberry-graphql (latest).  \n5. Schedule a daily background job to purge messages older than 30 days using Dramatiq (latest).\n\nDeliverables:\n- A requirements.txt (or pyproject.toml) listing all dependencies with exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments marking where and how each external library is used.","question_index":76,"modules":["litestar","beanie","motor","qdrant-client","strawberry-graphql","dramatiq","openai","pymongo","python-dotenv"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are tasked with building a real-time collaborative task management web service for enterprise teams.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to expose HTTP endpoints and WebSocket routes, leveraging its new dependency injection and WebSocket lifetime management.\n2. Configure an async PostgreSQL database using the latest Tortoise-ORM with its auto-migration CLI and Pydantic model integration.\n3. Broadcast real-time task updates to connected clients via the latest python-socketio in async mode.\n4. Implement OAuth2 authentication with JWT access and refresh tokens using the latest Authlib, including token rotation.\n5. Cache sessions and publish update events using the latest aioredis async Redis client.\n6. Schedule and execute background jobs (e.g., daily summaries) with the latest APScheduler async support.\n7. Instrument request tracing and logging using the latest OpenTelemetry Python SDK, exporting to console.\n8. Auto-generate interactive API docs using FastAPI\u2019s built-in Swagger UI and Redoc.\n\nDeliverables:\n\u2022 A bullet list of selected libraries (with version numbers).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":15,"modules":["fastapi","tortoise-orm","python-socketio","authlib","aioredis","apscheduler","opentelemetry-api","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"You are a senior software engineer tasked with building a real-time analytics dashboard with AI-driven anomaly alerts using the latest Python libraries as of 2025-05-06.  \n\nFunctional Requirements:  \n1. Implement asynchronous REST API endpoints using the latest FastAPI.  \n2. Add WebSocket-based real-time alert streaming using the latest websockets.  \n3. Persist incoming metrics in PostgreSQL via async ORM operations using the latest SQLModel.  \n4. Cache expensive queries in Redis for high throughput using the latest aioredis.  \n5. Perform streaming anomaly detection on incoming metrics using the latest river library.  \n6. Enable file uploads of detailed reports to S3-compatible storage via the latest minio SDK.  \n\nDeliverables:  \n\u2022 A numbered list of required Python libraries (with versions as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":20,"modules":["fastapi","uvicorn","sqlmodel","asyncpg","aioredis","minio","river","numpy","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: As a senior software engineer, build a modular web application that authenticates users with WebAuthn passkeys, streams real-time stock data, generates AI trading signals, processes payments, and delivers interactive dashboards.\n\nFunctional Requirements:\n1. Implement user authentication via WebAuthn using the latest pywebauthn library (as of 2025-05-06) with hardware security key support.\n2. Ingest and stream real-time stock price data over WebSockets using FastAPI and the latest Starlette HTTP\/2 push features.\n3. Generate AI-driven trading signals leveraging the latest OpenAI Python SDK\u2019s new function-calling interface.\n4. Integrate secure payment processing with the latest Stripe Python SDK supporting Payment Element and webhook handling.\n5. Serve interactive dashboards using the latest Plotly Dash or Reflex library for live data visualization and client-side callbacks.\n6. Add structured logging and Prometheus metrics using structlog and prometheus-client.\n\nDeliverables:\n- A concise mapping of each functional requirement to its implementation approach.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used and using the latest libraries available as of 2025-05-06.","question_index":56,"modules":["pywebauthn","fastapi","openai","stripe","plotly","dash","structlog","prometheus_client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Develop a modern, high-performance AI-powered web service for real-time text processing.\n\nFunctional Requirements:\n1. Build a JSON REST API with Litestar using its latest dependency-injection and ASGI lifespan hooks.\n2. Integrate Tortoise ORM (async) to manage a PostgreSQL \u201cjobs\u201d table with migrations.\n3. Use llama-cpp-python to generate text embeddings or responses via a local LLaMA model.\n4. Expose a WebSocket endpoint with python-socketio (asyncio mode) for live client notifications.\n5. Cache recent embeddings in Redis using redis-py\u2019s asyncio client.\n6. Schedule background tasks (e.g., stale job cleanup) leveraging asyncio\u2019s latest task groups.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":60,"modules":["litestar","tortoise-orm","llama-cpp-python","redis","python-socketio","asyncpg","alembic","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a serverless Python application enabling real-time geolocated chat with sentiment analysis, dynamic map visualization, and PDF session reporting.\n\nFunctional Requirements:\n1. Real\u2010time WebSocket chat using the latest Litestar (released in the last 12 months) with asynchronous message persistence via a modern async ORM.\n2. Interactive geospatial mapping of incoming messages on the client side using the latest Python geospatial visualization library released in the last 12 months.\n3. On-the-fly sentiment analysis of each message using the latest Python sentiment analysis library released in the last 12 months.\n4. PDF generation of the entire chat session\u2014including message timeline, geolocation markers, and sentiment scores\u2014using the latest Python PDF library released in the last 12 months.\n5. Deployment configuration for AWS Lambda using the latest Serverless Framework plugin and AWS Lambda Powertools (all released in the last 12 months).\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":97,"modules":["litestar","sqlalchemy","alembic","geopandas","folium","textblob","reportlab","mangum","aws-lambda-powertools","python-dotenv","aioboto3","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are designing a Python microservice for ingesting user text, storing embeddings, and providing real-time, rate-limited on-device summaries via WebSockets.\n\nFunctional Requirements:\n1. HTTP POST \/ingest  \n   - Validate payload (id: str, text: str) using Pydantic v2 root validators  \n   - Compute embeddings on-device with llama-cpp-python and store (id, embedding, text) in ChromaDB  \n2. HTTP GET \/query  \n   - Accept query text, validate with Pydantic v2  \n   - Compute embedding via llama-cpp-python, retrieve top-5 nearest documents from ChromaDB, return metadata  \n3. WebSocket \/summarize  \n   - Client sends document id  \n   - Fetch text from ChromaDB metadata  \n   - Stream back a summary using llama-cpp-python streaming API  \n   - Enforce token-per-second rate limiting via aiolimiter  \n4. Lifespan events  \n   - On startup, initialize ChromaDB client, Llama model, and aiolimiter  \n   - On shutdown, cleanly close any resources  \n\nDeliverables:\n- requirements.txt listing the latest available versions as of today of:\n  \u2022 starlite  \n  \u2022 pydantic v2  \n  \u2022 chromadb  \n  \u2022 llama-cpp-python  \n  \u2022 aiolimiter  \n- main.py: a single, runnable Python 3.11+ script (under 150 lines) implementing all above, with clear comments marking where each external library is used.","question_index":8,"modules":["starlite","pydantic","chromadb","llama-cpp-python","aiolimiter"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a real-time collaborative messaging microservice integrating graph storage, vector search, JWT authentication, and WebSockets using the latest Python libraries (as of May 6, 2025).\n\nFunctional Requirements:\n1. Authentication: secure all endpoints with JWTs using the latest TokenGuard library.\n2. Message Ingestion: implement POST \/messages to accept user messages; decode JWT, extract user ID.\n3. Graph Storage: store each message and its \u201cauthored_by\u201d relationship in a graph database via the latest GraphitORM.\n4. Vector Indexing & Search: generate text embeddings with the latest Morpher library, store them in its vector store, and expose GET \/search?q={text} to return top-5 similar messages.\n5. Real-Time Updates: broadcast new messages to connected clients over WebSockets using the latest Voila framework.\n\nDeliverables:\n1. A requirements list with each third-party library pinned to its latest version as of May 6, 2025.\n2. A single Python 3.11+ script (\u2264150 lines) implementing all endpoints, with clear comments marking where each external library is used.","question_index":35,"modules":["fastapi","uvicorn","tokenguard","graphitorm","morpher-embeddings","voila-websockets","pydantic","python-jose","asyncpg"],"modulenotfound":["tokenguard","graphitorm","morpher-embeddings","voila-websockets"],"modulenotfound_count":4,"module_count":9}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a high-performance, AI-powered web dashboard for real-time analytics and user collaboration.\n\nFunctional Requirements:\n1. Web API: Use the latest FastAPI (as of 2025-05-06) with async endpoints and the new dependency-injection system.\n2. Real-Time Collaboration: Implement bidirectional WebSocket chat using python-socketio v5.x+ for live user updates.\n3. AI Integration: Leverage the latest llama-cpp-python (or equivalent) to compute embeddings on incoming messages.\n4. Database Persistence: Store users and message history asynchronously with SQLModel v0.7+ (Pydantic v2) on SQLite.\n5. Caching Layer: Cache AI embedding results in Redis via aioredis v3.x+ to minimize redundant computation.\n6. Security: Protect all endpoints with OAuth2 JWT authentication using Authlib v1.2+.\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all third-party libraries as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, with clear inline comments indicating where each external library is used.","question_index":58,"modules":["fastapi","uvicorn","python-socketio","llama-cpp-python","sqlmodel","pydantic","aioredis","authlib","python-jose","passlib"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are a senior software engineer building a real-time sentiment and OCR-enhanced geospatial chat dashboard for remote field teams.\n\nFunctional Requirements:\n1. Use FastAPI (latest version as of 2025-05-06) to implement an async REST API that serves a minimal HTML client.\n2. Implement bi-directional real-time chat via Socket.IO using python-socketio (latest) on the FastAPI server.\n3. Perform on-the-fly sentiment analysis of text messages using Hugging Face\u2019s transformers pipeline (latest) and broadcast sentiment scores.\n4. Accept image uploads from clients, extract embedded text with EasyOCR (latest), and inject the OCR result into the chat stream.\n5. Plot message geolocation metadata on a Folium (latest) map and serve it as an embeddable HTML snippet.\n\nDeliverables:\n- A requirements list enumerating the exact \u201clatest\u201d third-party libraries and versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":2,"modules":["fastapi","uvicorn","python-socketio","python-multipart","transformers","torch","easyocr","folium","geopy","huggingface_hub"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are building a Python microservice that accepts user-uploaded text files, asynchronously generates summaries with a local LLM, and streams live progress to clients via WebSocket.\n\nFunctional Requirements:\n1. Implement a FastAPI app (use the latest FastAPI as of 2025-05-06) with an `\/upload` POST endpoint that accepts plain-text file uploads.  \n2. Upon upload, enqueue a background job using arq (latest version) to process the file asynchronously.  \n3. In the arq worker, load a local LLM via llama-cpp-python (latest version) using its new GPU offloading API to generate a summary.  \n4. Configure python-socketio (latest version) as an ASGI middleware in FastAPI to broadcast progress and final summary events to connected WebSocket clients.\n\nDeliverables:\n- A bullet list of the chosen third-party libraries with their exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":46,"modules":["fastapi","socketio","arq","llama_cpp_python"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are tasked with building a real-time IoT analytics dashboard featuring anomaly detection, edge caching, vector search, and HTTP\/3 streaming.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of May 6, 2025) with HTTP\/3 and ASGI concurrency for API endpoints.\n2. Ingest IoT telemetry via real-time WebSocket streams using the latest websockets library.\n3. Perform transformer-based anomaly detection using the latest HuggingFace transformers and PyTorch 2.x.\n4. Store and query time-series embeddings in a vector database using the latest Pinecone Python client with HNSW indexing.\n5. Cache recent query results at the edge using the latest aioredis client with TTL invalidation.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the \u201clatest\u201d versions of all third-party libraries as of today.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":67,"modules":["fastapi","uvicorn","websockets","torch","transformers","pinecone-client","aioredis","numpy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"You are a senior software engineer tasked with building a modern image-processing web service end-to-end.\n\nFunctional Requirements:\n1. Implement a REST endpoint in Litestar for image uploads that converts incoming images to AVIF using the latest pillow-avif-plugin.\n2. Asynchronously persist image metadata (filename, upload timestamp, AVIF URL, dimensions) in MongoDB via the latest Beanie ODM.\n3. Index and cache metadata in Redis for fast lookup using the latest Redis-OM Python library.\n4. Expose a GraphQL API via the latest Litestar-GraphQL plugin to query images with filtering, pagination, and type validation.\n5. Broadcast real-time upload notifications over WebSocket connections using Litestar\u2019s built-in WebSocket support.\n\nDeliverables:\n- A concise list of the five functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines, using the latest Litestar, Litestar-GraphQL, Beanie, Redis-OM, and pillow-avif-plugin (as of May 6, 2025), with clear comments indicating where each external library is used.","question_index":89,"modules":["litestar","strawberry","beanie","motor","redis-om","pillow-avif-plugin","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are developing a high-performance, real-time AI-powered web service for personalized content recommendations.\n\nFunctional Requirements:\n1. Implement a RESTful POST \/recommend endpoint using the latest Python web framework released within the last 12 months, accepting JSON input and returning JSON responses.\n2. Add a WebSocket \/ws endpoint for streaming live recommendation updates using the newest asyncio-based WebSocket library from the past year.\n3. Integrate semantic search over user profiles by connecting to a vector database via its newest Python client library released within the last 12 months.\n4. Schedule periodic model retraining in a background worker using the most recent Python task scheduler released in the last 12 months.\n5. Store and serve user-uploaded media files on cloud object storage using the latest cloud storage client library published within the past 12 months.\n\nDeliverables:\n\u2022 A requirements list (requirements.txt) specifying the exact library names and versions (all released within the last 12 months).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total, with clear comments marking where and why each external library is used.","question_index":71,"modules":["fastapi","uvicorn","websockets","pinecone-client","celery","redis","boto3","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Develop a Python microservice that supports AI-driven product imagery, real-time stock updates, secure user authentication, and checkout processing.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) app with JWT-based auth using Authlib (latest) and secure password hashing.  \n2. Define async ORM models for Users and Products with SQLModel (latest) on PostgreSQL, including migration setup.  \n3. On product creation, generate an AI image via Diffusers (latest) or Stability-SDK (latest) and expose it via a REST endpoint.  \n4. Provide a WebSocket endpoint for real-time inventory updates using Starlette\u2019s WebSocket support (latest).  \n5. Create a \/checkout POST endpoint integrating Stripe\u2019s Python SDK (latest) with webhook handling to confirm payments.\n\nDeliverables:\n\u2022 A bullet list of the \u201clatest\u201d libraries (with pip install specifiers as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":85,"modules":["fastapi","sqlmodel","authlib","passlib","jose","diffusers","stripe","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a high-performance asynchronous message processing service for real-time sentiment analysis, storage, notification, and search.\n\nFunctional Requirements:\n1. Implement an async HTTP API using the latest FastAPI (as of May 6, 2025) to accept user messages.\n2. Define and persist message models in PostgreSQL via the latest SQLModel ORM.\n3. Schedule and execute sentiment analysis in the background using the latest Dramatiq with RabbitMQ.\n4. After analysis, update the database and push real-time notifications to connected clients over WebSockets.\n5. Index messages and sentiment scores into Elasticsearch using the latest official Python client.\n6. Provide a search endpoint to query stored messages by keyword and sentiment.\n\nDeliverables:\n\u2022 A numbered list of the \u201clatest\u201d third-party libraries (with versions) used for each sub-domain.  \n\u2022 Complete, runnable Python 3.11+ application code under 150 lines, with clear comments indicating where each external library is used.","question_index":48,"modules":["fastapi","sqlmodel","elasticsearch","textblob","dramatiq","asyncpg","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build an end-to-end real-time customer feedback summarization service with live dashboard updates.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled JSON ingest endpoint using Litestar (latest as of 2025-05-06) with async support.\n2. Define a feedback model with Pydantic v2 and persist submissions into SQLite asynchronously via SQLModel (latest).\n3. On each new submission, call the OpenAI Python SDK (latest) asynchronously to generate a brief summary.\n4. Cache each summary in Redis via redis-om (latest) with a 1-hour TTL.\n5. Serve a live dashboard using Reflex (latest) that connects over WebSockets to display incoming summaries in real time.\n6. Use HTTPX (latest) with HTTP\/3 support for any external HTTP calls.\n\nDeliverables:\n\u2022 requirements.txt listing all dependencies pinned to the latest versions as of 2025-05-06  \n\u2022 A single Python 3.11+ script (under 150 lines) that implements the above, with clear comments indicating where and why each external library is used","question_index":61,"modules":["litestar","pydantic","sqlmodel","openai","redis-om","reflex","httpx","sqlalchemy","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-haiku-20241022","question":"You are a senior software engineer tasked with building a Python web application that integrates user authentication, real-time notifications, AI-based recommendations, and payment processing.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (2025-05-06) and Python 3.11 features (e.g. match-case).\n2. Secure all endpoints with OAuth2 JWT authentication via the latest fastapi-users, including email verification flows.\n3. Provide real-time notifications over WebSockets using the latest python-socketio async server.\n4. Handle one-time charges using the latest Stripe Python SDK via the Payment Intents API.\n5. Generate AI-driven product recommendations by invoking gpt-4-turbo through the latest openai library.\n6. Emit structured, context-rich logs for each operation using the latest structlog.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06  \n\u2022 Complete, runnable Python 3.11+ code (under 150 lines) with clear comments marking where each external library is used","question_index":96,"modules":["fastapi","fastapi-users","python-socketio","stripe","openai","structlog","uvicorn","python-jose","pydantic","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Design a minimal real-time chat service combining HTTP endpoints, data validation, async ORM persistence, WebSocket broadcasting, and JWT security using the latest Python libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Starlite to expose POST \/register and GET \/messages endpoints, leveraging its advanced dependency injection system.\n2. Define and serialize request\/response models with the latest Pydantic v2, employing its new `model_serializer` API.\n3. Persist User and Message models to SQLite asynchronously with the latest Ormar ORM, utilizing its built-in async migration feature.\n4. Implement a `\/ws` WebSocket route using the latest websockets library for real-time message broadcasting to connected clients.\n5. Secure both HTTP routes and WebSocket connections with JWT authentication via the latest PyJWT release.\n\nDeliverables:\n- A concise mapping of each numbered requirement to the specific library and feature used.\n- Complete, runnable Python 3.11+ code under 150 lines total, with clear comments indicating where each external library is imported and applied.","question_index":21,"modules":["starlite","pydantic","jwt","ormar","databases","sqlalchemy","uvicorn","aiosqlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Develop an internal Python microservice where authenticated users can submit text to be summarized by an LLM and receive real-time notifications via WebSockets.\n\nFunctional Requirements:\n1. Implement an asynchronous REST API using the latest Starlite framework (2025-05-06) leveraging its dependency injection and lifespan event system.\n2. Integrate OAuth2 Authorization Code Flow for user authentication with Authlib\u2019s async client libraries released in the last 12 months.\n3. Persist users, submissions, and summaries in PostgreSQL using the latest Piccolo ORM with async schema migrations and connection pooling.\n4. Summarize submitted text by calling OpenAI\u2019s GPT-4 Turbo via the newest openai Python SDK and its function-calling feature.\n5. Broadcast summary completion events over WebSockets using a modern Python websockets library (with HTTP\/2 support) released in the past year.\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":82,"modules":["starlite","authlib","piccolo","openai","asyncpg","websockets","pydantic","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Develop a high-performance Python microservice that ingests real-time social media posts via WebSockets, generates AI summaries, stores them in MongoDB Atlas, serves secured HTTP\/3 endpoints, and provides an interactive Streamlit dashboard for sentiment trends.\n\nFunctional Requirements:\n1. Initialize an asynchronous FastAPI app with HTTP\/3 (h2\/h3) and ASGI support.  \n2. Protect all HTTP endpoints with OAuth2 JWT using Authlib\u2019s latest OAuth library.  \n3. Consume a WebSocket stream of JSON posts from wss:\/\/stream.example.com asynchronously.  \n4. Summarize each post using OpenAI\u2019s latest Python SDK leveraging function-calling or embeddings.  \n5. Persist summaries and metadata in MongoDB Atlas via Motor (async driver).  \n6. Expose a secured REST endpoint `\/summaries` supporting pagination and filtering by sentiment.  \n7. Build a Streamlit dashboard that fetches `\/summaries` and visualizes sentiment trends over time with Plotly.\n\nDeliverables:\n- A requirements list naming the latest versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":28,"modules":["fastapi","uvicorn","motor","openai","authlib","streamlit","plotly","python-jose","httpx","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a secure, real-time AI-powered chat web service.\n\nFunctional Requirements:\n1. Expose an async REST API using the latest FastAPI for user registration and login endpoints.  \n2. Implement JWT authentication with rotating refresh tokens via the latest Authlib.  \n3. Persist users and chat messages in SQLite using the latest SQLModel, leveraging its new async support.  \n4. Enable real-time chat notifications over WebSockets using the latest python-socketio and ASGI.  \n5. Integrate AI chat completions using the latest OpenAI Python client in an async background task.\n\nDeliverables:\n1. A requirements.txt listing exact, up-to-date package versions as of today\u2019s date.  \n2. A single Python 3.11+ file (<150 lines) containing complete, runnable code with clear comments marking where each external library is used.","question_index":91,"modules":["fastapi","uvicorn","sqlmodel","authlib","python-socketio","python-jose","passlib","openai","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are a senior software engineer building a modern microblogging backend that supports authenticated posts, AI-generated images, real-time feed updates, and cloud storage integration.\n\nFunctional Requirements:\n1. Implement a REST API using a Python web framework released in the last 12 months (use the latest available as of 2025-05-06).\n2. Persist posts in a SQL database via an async ORM library created in the last 12 months, leveraging advanced async model features.\n3. Broadcast new posts to connected clients over WebSockets using a recently released high-performance async messaging library.\n4. Authenticate users with OAuth2 (Google) using the latest OAuth client library.\n5. Generate post images by calling Stable Diffusion through the most recent Python API client for AI image generation.\n6. Store and serve AI-generated images in AWS S3 using the newest AWS SDK for Python.\n7. Ensure all components run under Python 3.11+ and fit within 150 lines of code.\n\nDeliverables:\n\u2022 A numbered list of the specific libraries (with versions) you chose to satisfy each requirement.  \n\u2022 Complete, runnable Python 3.11+ code (<150 lines) combining all features.  \n\u2022 Clear inline comments marking where and why each external library is used.","question_index":7,"modules":["fastapi","sqlmodel","stability-sdk","authlib","boto3","websockets","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: As a senior software engineer, design a high-performance microservice for real-time document collaboration with secure access, persistence, and cloud integration.  \n\nFunctional Requirements:  \n1. Implement an asynchronous JSON REST API using the latest Python web framework available as of 2025-05-06, supporting pagination and input validation.  \n2. Secure all endpoints with OAuth2 PKCE and JWT using the latest authentication library (released within the last 12 months).  \n3. Enable bidirectional real-time collaboration events over WebSocket using the latest WebSocket library (released within the last 12 months).  \n4. Persist document data in an async NoSQL database (e.g., MongoDB) using the newest driver supporting transactions and change streams.  \n5. Upload and serve document attachments to an S3-compatible object storage using the latest SDK with multipart upload support.  \n6. Cache frequently accessed documents in Redis with TTL and automatic invalidation using the newest Redis client library.  \n\nDeliverables:  \n- A requirements.txt listing \u201cthe latest\u201d versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":80,"modules":["fastapi","motor","pydantic","python-jose","redis","uuid","httpx","websockets","asyncio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-haiku-20241022","question":"Senior Software Engineer: You are developing an intelligent IoT monitoring dashboard that integrates real-time ingestion, streaming anomaly detection, WebSocket updates, dynamic UI rendering, and cloud infrastructure provisioning.\n\nFunctional Requirements:\n1. Build an ASGI web server using the latest ASGI framework (as of 2025-05-06) with HTTP\/2 and WebSocket endpoints for real-time data streaming.\n2. Connect asynchronously to an MQTT broker using the latest Python MQTT client library (as of 2025-05-06), subscribe to sensor topics, and relay messages to the server.\n3. Implement real-time anomaly detection on incoming sensor data using the latest Python anomaly detection library supporting incremental or deep learning.\n4. Serve a live, interactive front-end auto-generated from Python to JavaScript using the latest Python-to-JS UI framework.\n5. Provision and deploy all components via infrastructure-as-code using the latest Python IaC SDK on a major cloud provider.\n\nDeliverables:\n\u2022 Restate the numbered functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code integrating all five requirements, under 150 lines.  \n\u2022 Clear inline comments indicating where and how each external \u201clatest\u201d library (as of 2025-05-06) is imported and used.","question_index":49,"modules":["starlette","asyncio-mqtt","river","pulumi","pulumi-azure-native"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are building a real-time news aggregation microservice with live client updates.\n\nFunctional Requirements:\n1. Develop a REST API using Litestar (the latest version as of 2025-05-06) leveraging its OpenAPI v3.1 auto-generation feature.\n2. Asynchronously fetch and parse external RSS feeds over HTTP\/3 using HTTPX AsyncClient (the latest), then cache results in aiocache (the latest) with a TTL.\n3. Validate and serialize each article using Pydantic v2 (the latest) functional-style models to ensure data integrity.\n4. Expose a WebSocket endpoint using the websockets library (the latest) with permessage-deflate compression for real-time article broadcasting.\n5. Schedule periodic background tasks with Arq (the latest) to re-poll feeds every 5 minutes.\n\nDeliverables:\n- A list of the chosen libraries and their latest versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":31,"modules":["litestar","httpx","pydantic","aiocache","arq","redis","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are tasked with developing a next-generation chat analytics platform that seamlessly integrates HTTP\/3, JWT auth, real-time messaging, vector search, and on-device LLM inference.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) with HTTP\/3 support via Hypercorn to serve all endpoints.\n2. Implement JWT authentication with rotational refresh tokens using fastapi-users (latest).\n3. Provide a real-time WebSocket chat endpoint using fastapi-websocket-pubsub (latest) supporting pub\/sub channels.\n4. On each incoming chat message, generate vector embeddings using llama-cpp-python (latest) with an on-device streaming pipeline, and store them in ChromaDB (latest) with HNSW indexing.\n5. Expose a REST endpoint to query semantic similarity: accept a text query, compute embeddings, perform a ChromaDB search, and return the top-3 similar messages.\n6. Expose a streaming summarization endpoint that returns an on-the-fly summary of recent messages using llama-cpp-python\u2019s streaming completion interface.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements you\u2019ve implemented.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":93,"modules":["uvicorn","chroma","fastapi","llama_cpp","fastapi_users","hypercorn","fastapi_websocket_pubsub"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":7}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are a senior software engineer tasked with building a next-generation real-time collaborative data analytics dashboard. Use the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a FastAPI server with a WebSocket endpoint for live data updates using the latest fastapi[full].\n2. Expose a type-safe GraphQL API at `\/graphql` using Strawberry with async schema stitching.\n3. Integrate AI-driven insights by generating embeddings on incoming data with llama-cpp-python.\n4. Perform vector similarity search over embeddings via the Weaviate Python client\u2019s async API.\n5. Secure all endpoints with passwordless authentication using py_webauthn for WebAuthn flows.\n6. Containerize the application using docker-py to build images and dynamically generate a docker-compose config.\n7. Instrument the app with async Prometheus metrics using prometheus-client.\n\nDeliverables:\n- A list of required libraries (with versions) using the latest releases as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":72,"modules":["fastapi","strawberry-graphql","weaviate","llama_cpp","prometheus_client","py_webauthn","docker","uvicorn"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are a senior engineer building a high-throughput job processing microservice with a modern async Python stack.  \n\nFunctional Requirements:  \n1. Expose a POST \/jobs endpoint using the latest Starlite (as of 2025-05-06) to accept JSON job submissions.  \n2. Validate incoming JSON payloads with the latest Pydantic v2 models.  \n3. Persist job records to PostgreSQL via the latest prisma-client-py async ORM.  \n4. Enqueue and process jobs in the background using the latest Taskiq queue.  \n5. Expose a GET \/jobs\/{id}\/status endpoint to retrieve current job status.  \n6. Ensure all I\/O is non-blocking and use Python 3.11+ async\/await throughout.  \n\nDeliverables:  \n\u2022 A requirements list specifying the exact latest versions of Starlite, Pydantic v2, prisma-client-py, and Taskiq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ source code under 150 lines.  \n\u2022 Clear inline comments marking where each external library is used.","question_index":99,"modules":["starlite","pydantic","prisma","taskiq","taskiq-postgres","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are a senior software engineer tasked with building a serverless real-time IoT analytics service that ingests sensor data, performs streaming aggregation, applies online anomaly detection, and streams results to clients.\n\nFunctional Requirements:\n1. Ingest JSON messages from an MQTT broker (\u201csensors\/+\/data\u201d) using the latest asyncio-based gmqtt library (as of 2025-05-06).  \n2. Stream-process incoming messages with streamz (latest version) to compute 1-minute tumbling-window averages for temperature and humidity.  \n3. Apply online anomaly detection on each windowed aggregation using River\u2019s latest HDBSCAN-based drift detector.  \n4. Persist raw messages and aggregated results into TimescaleDB via the asyncpg driver (latest version).  \n5. Expose a Python 3.11+ FastAPI application with a WebSocket endpoint for live metric and anomaly alert streaming.  \n6. Wrap the FastAPI app for AWS Lambda deployment using the latest mangum adapter.\n\nDeliverables:\n\u2022 A list of third-party dependencies pinned to \u201clatest\u201d versions as of 2025-05-06.  \n\u2022 A single, complete, runnable Python 3.11+ source file under 150 lines.  \n\u2022 Clear inline comments showing where each external library is used.  \n\u2022 The code must integrate all functional requirements and be deployable serverlessly on AWS Lambda.","question_index":38,"modules":["gmqtt","streamz","river","asyncpg","timescaledb_helpers","fastapi","mangum","websockets"],"modulenotfound":["timescaledb_helpers"],"modulenotfound_count":1,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a real-time HTTP\/3-powered AI chat service that streams user messages, generates LLM responses, persists embeddings in a vector DB, and exposes chat history via GraphQL.\n\nFunctional Requirements:\n1. HTTP\/3 streaming: use the latest aioquic (as of 2025-05-06) to handle bidirectional HTTP\/3 connections for chat.\n2. On-device LLM inference: integrate the latest llama-cpp-python for quantized model loading and response generation.\n3. Vector storage: use the latest chromadb Python client to embed each message (via llama-cpp-python\u2019s embed API) and store\/retrieve vectors.\n4. GraphQL API: implement an async GraphQL schema with the latest strawberry to query chat history from ChromaDB.\n5. Async orchestration: ensure end-to-end async I\/O under Python 3.11+, coordinating the QUIC server, LLM, and DB.\n6. Code constraints: keep the complete solution under 150 lines, include clear comments marking where each external library is used.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of aioquic, llama-cpp-python, chromadb, and strawberry (latest as of 2025-05-06)  \n\u2022 A single Python 3.11+ script (<150 lines) with runnable code and comments identifying each external library usage.","question_index":79,"modules":["aioquic","llama-cpp-python","chromadb","strawberry-graphql","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"claude-3-5-haiku-20241022","question":"Scenario  \nDevelop an end-to-end Python web service that provides passwordless WebAuthn authentication, stores and summarizes text inputs via OpenAI, broadcasts real-time summary events, and serves interactive charts.\n\nFunctional Requirements  \n1. Implement passwordless registration and login using the latest \u201cwebauthn\u201d library (FIDO2) with Litestar middleware.  \n2. Create REST endpoints under Litestar (latest version) for submitting text to summarize and retrieving stored summaries.  \n3. Use the latest SQLModel with an async PostgreSQL engine to persist input texts, summaries, timestamps, and user IDs.  \n4. Invoke the latest \u201copenai\u201d Python client to perform GPT-4 Turbo summarization with function-calling.  \n5. Serialize and broadcast new summaries over WebSockets using the latest \u201cmsgspec\u201d for high-performance JSON.  \n6. Serve an HTML page at \u201c\/charts\u201d that embeds an interactive Plotly chart of summary lengths over time.\n\nDeliverables  \n\u2022 A numbered list of all external libraries (with exact latest versions as of 2025-05-06) and their roles.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":13,"modules":["litestar","webauthn","sqlmodel","asyncpg","openai","msgspec","plotly","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a real-time collaborative note-taking web application with authentication, persistence, caching, live updates, and scheduled cleanup.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (as of today) for note creation, retrieval, update, and deletion.\n2. Secure endpoints with JWT authentication using the latest PyJWT.\n3. Persist notes in a relational database via the latest SQLModel ORM, defining Note and User models.\n4. Enable real-time note synchronization between clients using WebSockets with the latest python-socketio.\n5. Cache frequently accessed note metadata in Redis using the latest redis-om to reduce DB load.\n6. Schedule automatic archiving of notes older than 30 days using the latest APScheduler.\n7. Document all endpoints with OpenAPI and include example request\/response schemas.\n\nDeliverables:\n- A numbered list of all third-party libraries (with versions) you\u2019ve chosen.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear comments indicating where each external library is used.","question_index":69,"modules":["fastapi","python-jose","redis-om","python-socketio","apscheduler","sqlmodel"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are a senior software engineer tasked with building a real-time event analytics dashboard that ingests events, stores them, generates AI-driven summaries, caches counts, and streams updates to authenticated clients with full observability.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to implement a POST \/events REST endpoint for event ingestion.\n2. Validate incoming JSON payloads with the latest Pydantic v2 models.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise-ORM.\n4. Cache aggregated event counts in Redis via the latest aioredis.\n5. Generate streaming summaries of event batches with the latest OpenAI Python client.\n6. Stream real-time updates to clients over WebSocket using FastAPI.\n7. Secure all HTTP and WebSocket endpoints with JWT authentication via the latest fastapi-jwt-auth.\n8. Instrument HTTP, database, cache, and AI client calls with the latest OpenTelemetry for distributed tracing.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest library names and versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines integrating all subdomains, with clear comments indicating where each external library is used.","question_index":4,"modules":["fastapi","pydantic","tortoise-orm","aioredis","openai","fastapi-jwt-auth","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","uvicorn","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: As a senior software engineer, develop a Python 3.11+ microservice that manages real-time collaborative whiteboard sessions, persists data, exposes REST and GraphQL APIs, generates session snapshots, integrates external metrics, and secures endpoints with OAuth2.\n\nFunctional Requirements:\n1. Implement session and user management REST endpoints using FastAPI (latest) with async event hooks and Pydantic V2 models.  \n2. Persist all data in PostgreSQL via Tortoise ORM (latest) using model lifecycle events.  \n3. Expose a GraphQL API with Strawberry GraphQL (latest) dataclass resolvers for session and user queries\/mutations.  \n4. Broadcast real-time drawing events using python-socketio (latest) over WebSockets.  \n5. Generate on-demand PNG snapshots of whiteboard state with Pillow-SIMD (latest) leveraging multi-threaded rendering.  \n6. Send session metrics asynchronously to an external analytics service using httpx (latest) with HTTP\/2 support.  \n7. Secure all endpoints with OAuth2 (authorization code + PKCE) using Authlib (latest).  \n\nDeliverables:\n- A numbered list of the functional requirements above.  \n- Complete, runnable Python 3.11+ code (\u2264 150 lines) integrating all requirements.  \n- Inline comments indicating where each external library is used.  \n- Use the latest stable releases of all libraries as of today\u2019s date.","question_index":50,"modules":["uvicorn","fastapi","tortoise","strawberry","socketio","authlib","httpx","pillow"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a real-time collaborative note-taking microservice integrating REST, database persistence, caching, GraphQL, WebSockets, and background tasks.\n\nFunctional Requirements:\n1. Expose REST endpoints using FastAPI (latest) for creating, updating and retrieving notes; leverage Pydantic v2\u2019s new validation decorators.\n2. Persist notes asynchronously in SQLite via SQLModel (latest) with SQLAlchemy 2.0-style typing and async engine.\n3. Broadcast note creation and updates over WebSockets with FastAPI\u2019s WebSocket support to subscribed clients in real time.\n4. Cache note retrieval results in Redis using aioredis (latest), implement TTL and cache invalidation on updates.\n5. Provide a GraphQL schema and endpoint using Strawberry (latest) to query and filter notes with federation-style resolvers.\n6. Schedule a daily summary email using FastAPI BackgroundTasks and aiosmtplib (latest) for asynchronous SMTP delivery.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- A single Python 3.11+ source file under 150 lines that implements all requirements, with clear comments marking where each external library is used.","question_index":73,"modules":["fastapi","sqlmodel","strawberry-graphql","aioredis","aiosmtplib","uvicorn","pydantic","sqlalchemy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Develop a real-time collaborative note-taking web application with AI-powered summarization.\n\nFunctional Requirements:\n1. Implement user authentication via OAuth2 password flow using the latest FastAPI features for secure login.  \n2. Expose a WebSocket endpoint for real-time CRDT-based document state synchronization using the newest FastAPI WebSocket enhancements.  \n3. Persist users and documents in PostgreSQL via the latest Piccolo ORM async API with JSONB indexing for rich querying.  \n4. Cache live presence and session metadata in Redis using aioredis with RedisJSON support for low-latency reads\/writes.  \n5. Summarize document edits on demand using the latest LangChain library and ChatOpenAI (GPT-4) integration.  \n6. Serve a minimal HTML frontend via Jinja2 templates that connects to the WebSocket and displays live updates.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of each dependency as of 2025-05-06.  \n\u2022 main.py: Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":52,"modules":["fastapi","uvicorn","piccolo","pydantic","aioredis","langchain","openai","python-jose","passlib","websockets","sqlalchemy","asyncpg","jinja2","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":14}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are a senior software engineer tasked with building an AI-powered collaborative document annotation API.\n\nFunctional Requirements:\n1. Secure user signup and login with JWT-based authentication using Starlite (latest) and Pydantic v2.\n2. REST endpoint to upload documents and persist metadata in Redis OM Python (latest).\n3. WebSocket route for real-time annotation broadcasting using Starlite\u2019s built-in WebSocket support.\n4. Background summarization of uploaded documents using llama-cpp-python (latest).\n\nDeliverables:\n- A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":1,"modules":["starlite","pydantic","redis-om","llama-cpp-python","uvicorn","python-jose","passlib","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario:\nYou are tasked with developing a lightweight AI-powered semantic search microservice in Python 3.11+.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI framework with HTTP\/3 support) to expose a POST \/search endpoint.  \n2. Validate and parse incoming JSON with the latest Pydantic v2\u2019s strict model parsing and Python pattern-matching features.  \n3. Asynchronously fetch text embeddings from OpenAI\u2019s embedding API using the latest HTTPX client with HTTP\/3.  \n4. Store and query vectors in a GPU-accelerated ChromaDB instance via the latest ChromaDB Python client.  \n5. Return the top 5 semantically related document IDs and similarity scores as a JSON response.\n\nDeliverables:\n\u2022 A numbered list confirming the above functional requirements.  \n\u2022 A single, complete, runnable Python 3.11+ source file (\u2264150 lines) that implements the microservice, with clear comments indicating where each external library is used.","question_index":3,"modules":["starlite","pydantic","httpx","chromadb","openai"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: A fintech startup requires a live transaction monitoring service that ingests, processes, stores, and notifies users of suspicious activity in real time.\n\nFunctional Requirements:\n1. Build an HTTP endpoint to receive and stream-respond to JSON transaction payloads using the latest async web framework released in the last 12 months.\n2. Validate and serialize incoming data with the latest data validation library, enforcing strict type constraints and custom validators.\n3. Persist transactions asynchronously in a relational database using the latest async ORM library compatible with Python 3.11+.\n4. Offload CPU-intensive anomaly detection to a background worker queue using the latest task orchestration library.\n5. Broadcast real-time alerts to connected clients via WebSockets using the latest real-time communication library.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of the latest libraries (as of May 6, 2025)  \n\u2022 A single Python 3.11+ file under 150 lines, complete and runnable, with clear comments indicating where each external library is used.","question_index":39,"modules":["fastapi","pydantic","sqlalchemy","celery","redis","websockets","scikit-learn","asyncpg","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: As a senior software engineer, you must develop a real-time analytics dashboard backend that requires user authentication, data ingestion, live updates, background enrichment via an external API, and a web-based dashboard.\n\nFunctional Requirements:\n1. Implement user registration and login endpoints using JWT authentication with asymmetric keys via FastAPI-JWT-Auth (latest).\n2. Create a POST \/data endpoint to ingest JSON payloads and store them asynchronously in SQLite using SQLModel (latest).\n3. Broadcast each new record to connected WebSocket clients at \/ws using the latest websockets library, leveraging async broadcast.\n4. Schedule a background job every minute with arq (latest) to enrich stored records by streaming JSON from a third-party API via httpx (latest).\n5. Serve a simple HTML dashboard at GET \/dashboard that opens a WebSocket to \/ws and displays incoming records in real time.\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":75,"modules":["fastapi","fastapi-jwt-auth","sqlmodel","websockets","httpx","arq","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a microservice that accepts text input, asynchronously generates summaries, persists data, and provides real-time metrics.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI 3, HTTP\/3 support) to implement POST \/summaries accepting JSON { \"text\": \"\u2026\" }, enqueue a background job via Arq, and return a job ID.\n2. Persist requests and summaries in SQLite asynchronously with the latest SQLModel (Pydantic V2 model support).\n3. Offload summarization jobs to Redis using the latest Arq, invoking the latest LangChain to call OpenAI for a concise summary.\n4. Expose GET \/summaries\/{id} to retrieve stored summaries from the database.\n5. Instrument request counts and job durations with the latest prometheus_client, exposing metrics on GET \/metrics.\n\nDeliverables:\n- A bulleted list of chosen \u201clatest\u201d libraries with exact version numbers.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear inline comments indicating where each external library is used.\n- Self-contained implementation: after installing dependencies, code runs without further setup.","question_index":95,"modules":["starlite","sqlmodel","arq","langchain","openai","prometheus_client","uvicorn","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You\u2019re building a high-performance async task management API using the latest libraries in web framework, async ORM, caching, background processing, and observability as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement CRUD REST endpoints for a Task model using the latest starlite.\n2. Persist tasks in Postgres asynchronously with the latest Tortoise-ORM.\n3. Enqueue a background job on task creation using the latest Dramatiq with Redis broker.\n4. Cache GET \/tasks\/{id} responses in Redis via the latest redis.asyncio.\n5. Instrument all HTTP and database calls with OpenTelemetry using the latest opentelemetry-instrumentation-starlite.\n\nDeliverables:\n- A requirements.txt listing each external dependency with explicit versions.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":34,"modules":["starlite","tortoise-orm","dramatiq","redis","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-starlite","asyncpg","pydantic","uvicorn"],"modulenotfound":["opentelemetry-instrumentation-starlite"],"modulenotfound_count":1,"module_count":10}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are a senior software engineer tasked with building a real-time analytics microservice that combines REST, GraphQL, persistent storage, and background processing using the latest Python ecosystem tools as of 2025-05-06.\n\nFunctional Requirements:\n1. Expose HTTP and WebSocket endpoints using the latest Litestar ASGI framework with annotated routing and streaming support.\n2. Implement a GraphQL API using the latest Strawberry GraphQL code-first schema builder with asyncio support.\n3. Persist and query analytics data in PostgreSQL via the latest Piccolo-ORM\u2019s async-first models and automatic migrations.\n4. Offload long-running analytics jobs to a Redis-backed async queue using the latest Arq library.\n5. Send WebSocket notifications to clients when background jobs complete.\n\nDeliverables:\n\u2022 A requirements list (pip install commands) specifying the latest versions of Litestar, Strawberry, Piccolo-ORM, and Arq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":65,"modules":["litestar","strawberry-graphql","piccolo","arq","redis","asyncpg","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are a senior software engineer building a proof-of-concept Python service that ingests user text, generates embeddings and classifications via an LLM, stores them, and streams nearest-neighbor results in real time.\n\nFunctional Requirements:\n1. Implement a JWT-secured POST \/ingest endpoint using the latest async Python web framework as of 2025-05-06 (e.g., Litestar).  \n2. In \/ingest, accept JSON `{ \"text\": \"...\" }`, call a state-of-the-art LLM (via the latest Transformers or LangChain client) to produce both an embedding vector and a classification label.  \n3. Persist the embedding vector in a vector database using the newest ChromaDB client.  \n4. Log the original text and its classification into a relational database through the latest async ORM (e.g., Prisma Python).  \n5. Expose a GET \/stream endpoint that streams nearest-neighbor matches for incoming embeddings over server-sent events using the newest SSE library (e.g., httpx-sse or framework-native support).  \n6. Target Python 3.11+, include type hints, and keep all code under 150 lines.\n\nDeliverables:\n\u2022 A `requirements.txt` (or equivalent) listing each library with exact versions (latest as of 2025-05-06).  \n\u2022 A single Python module (\u2264150 lines) that meets all requirements, with clear comments marking where and why each external library is used.","question_index":81,"modules":["litestar","openai","chromadb","sqlalchemy","alembic","httpx","pydantic","PyJWT","asyncpg","typing-extensions"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Senior Software Engineer, you are tasked with building a prototype real-time collaborative note-taking service using only the latest Python libraries released in the last 12 months.\n\nFunctional Requirements:\n1. Develop an asynchronous REST API for note CRUD operations using the latest Litestar framework.\n2. Persist notes in a SQL database via a newly released async ORM library.\n3. Cache frequent note queries in Redis with a recently published async Redis client.\n4. Enable real-time collaboration via WebSockets using a modern Python WebSocket library launched within the last year.\n5. Expose an AI-powered summarization endpoint using the latest OpenAI Python client.\n6. Render an interactive HTML dashboard showing active collaborators with a new Python-to-JS visualization tool.\n7. Instrument distributed tracing and structured logging using a cutting-edge observability library released in the past 12 months.\n\nDeliverables:\n- requirements.txt listing only libraries created in the last 12 months with exact versions.\n- A single Python 3.11+ file under 150 lines containing the complete, runnable application, with clear comments indicating where each external library is used.","question_index":41,"modules":["litestar","sqlalchemy","redis","websockets","openai","structlog","opentelemetry-api","pydantic","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are a senior engineer tasked with building a secure, high-throughput microservice for customer order ingestion, persistence, background processing, and caching.\n\nFunctional Requirements:\n1. Use FastAPI (latest as of 2025-05-06) with HTTP\/3 support to implement POST \/orders accepting JSON order data.\n2. Define an asynchronous Order model with SQLModel (latest) and connect to an async database engine (SQLite or PostgreSQL).\n3. Enqueue and process new orders via Arq (latest) background worker.\n4. Implement OAuth2 with JWT issuance and protected endpoints using Authlib (latest).\n5. Cache GET \/orders\/{id} responses in Redis using aiocache (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments marking where each external library is used.","question_index":68,"modules":["uvicorn","fastapi","sqlmodel","authlib","pydantic","redis","arq","aiocache"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a real-time chat analysis service that ingests user messages, performs on-device LLM summarization and sentiment analysis, caches results, persists chat history, and broadcasts updates via WebSockets.\n\nFunctional Requirements:\n1. Expose a POST \/messages endpoint using FastAPI (latest) to receive JSON payloads with \u201cuser_id\u201d and \u201ctext\u201d.\n2. Perform sentiment analysis and summarization on each message using llama-cpp-python (latest) in an async background task.\n3. Cache sentiment results for identical texts in Redis via redis-py (latest) to avoid recomputation.\n4. Persist messages, sentiments, and summaries in PostgreSQL using SQLModel (latest version).\n5. Provide a WebSocket \/ws\/{user_id} endpoint (FastAPI) that broadcasts new summaries and sentiments to connected clients in real time.\n6. Secure all endpoints with OAuth2 password flow via Authlib (latest).\n\nDeliverables:\n\u2022 Restate the numbered requirements above.  \n\u2022 Provide complete, runnable Python 3.11+ code integrating the latest versions of FastAPI, llama-cpp-python, redis-py, SQLModel, and Authlib, under 150 lines total.  \n\u2022 Include clear comments indicating where and how each external library is used.","question_index":32,"modules":["fastapi","sqlmodel","llama_cpp","redis","authlib","pydantic","uvicorn"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":7}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a real-time collaborative document editor service with AI-powered summarization, OAuth2 access control, WebSocket syncing, Redis caching, and NoSQL persistence.\n\nFunctional Requirements:\n1. Use the latest async Python web framework (as of 2025-05-06) to define RESTful CRUD endpoints for documents.\n2. Implement OAuth2 authorization with the latest Python OAuth library released within the last 12 months for secure user login and token issuance.\n3. Establish bi-directional real-time document synchronization over WebSockets using the newest real-time communication library (with built-in pub\/sub).\n4. Integrate the latest AI inference library published in the past year to generate live summaries of incremental document edits.\n5. Employ the latest Redis client library (released within the last year) to cache active sessions and coordinate pub\/sub channels for syncing.\n6. Persist document state and version history in a NoSQL database using the newest async database client released in the last 12 months.\n\nDeliverables:\n1. A numbered list of the chosen \u201clatest\u201d libraries (with a one-line description of their role).\n2. Complete, runnable Python 3.11+ code under 150 lines, including clear comments indicating where and why each external library is used.","question_index":86,"modules":["fastapi","authlib","litestar","langchain","redis","motor"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Design and implement a Python microservice that exposes HTTP endpoints, real-time WebSocket notifications, persistent storage, caching, scheduled tasks, and email alerts using the latest third-party libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use FastAPI (latest) with HTTP\/2 support to expose async REST endpoints for user registration and notification retrieval.\n2. Use SQLModel (latest) as an async ORM for PostgreSQL, defining typed models for users and notifications.\n3. Implement WebSocket broadcasting via FastAPI\u2019s WebSocket support to push new notifications to connected clients.\n4. Leverage Aioredis (latest) Redis Streams to cache session state and stream notification events.\n5. Schedule periodic database cleanup and cache eviction using APScheduler (latest) with its asyncio integration.\n6. Send email alerts for critical notifications using Aiosmtplib (latest) with STARTTLS.\n7. Perform external health-check API calls using HTTPX (latest) with async retries and timeouts.\n\nDeliverables:\n- A requirements list naming each library and its latest version as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, including clear comments that indicate where each external library is used.","question_index":44,"modules":["fastapi","uvicorn","sqlmodel","aioredis","httpx","apscheduler","aiosmtplib","asyncpg","pydantic","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are a senior software engineer building a real-time social media monitoring application that ingests tweets, analyzes sentiment, classifies images, displays live charts, and deploys as a serverless microservice using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Ingest tweets in real time from Twitter API v2 using the latest HTTPX async streaming client.\n2. Perform NLP sentiment analysis on tweet text with the latest Hugging Face Transformers pipeline.\n3. Classify embedded images using the latest Ultralytics YOLOv8 model.\n4. Render an interactive, live-updating dashboard using the latest Plotly Dash framework.\n5. Scaffold serverless deployment with the latest AWS Lambda Powertools for Python.\n\nDeliverables:\n- A requirements list of all external dependencies.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":98,"modules":["httpx","transformers","torch","ultralytics","plotly","dash","tweepy","aws-lambda-powertools"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are a senior software engineer tasked with developing an end-to-end Python web application that leverages the latest libraries to showcase advanced API development, ORM integration, security, background processing, and real-time features.\n\nFunctional Requirements:\n1. Asynchronous REST API: implement CRUD endpoints for a `Task` model using FastAPI (latest version as of 2025-05-06).\n2. Data Modeling and Persistence: define `Task` ORM models with SQLModel (latest version) and connect to a SQLite or PostgreSQL database asynchronously.\n3. Security: secure all API endpoints with OAuth2 password flow using Authlib (latest version) to obtain and validate JWT tokens.\n4. Background Processing: create an asynchronous task queue with Arq (latest version) to process tasks in the background.\n5. Real-Time Notifications: establish a WebSocket endpoint using the websockets library (latest version) to push real-time task status updates.\n\nDeliverables:\n- The numbered list of functional requirements (as above).\n- Complete, runnable Python 3.11+ code under 150 lines. Use the latest versions of FastAPI, SQLModel, Authlib, Arq, and websockets as of 2025-05-06. Include clear comments indicating where and how each external library is used.","question_index":24,"modules":["fastapi","sqlmodel","authlib","arq","websockets","uvicorn","aiosqlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a lightweight feedback API that records user comments, analyzes sentiment in real time, stores results, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (as of 2025-05-06) to implement asynchronous REST endpoints for submitting feedback and querying stored analytics.\n2. Use the latest SQLModel to define async ORM models and persist incoming feedback in an SQLite database.\n3. Use the latest PyWebSocketX to broadcast each new sentiment result over an ASGI WebSocket endpoint.\n4. Use the latest VulcanMind inference library to perform on-the-fly sentiment analysis of submitted feedback.\n5. Implement structured logging and error handling throughout the request, inference, database, and WebSocket flows.\n\nDeliverables:\n- A requirements list with exact version pins for Starlite, SQLModel, PyWebSocketX, and VulcanMind (latest as of 2025-05-06).\n- Complete, runnable Python 3.11+ code (\u2264150 lines) that integrates all four libraries with clear comments marking where each is used.","question_index":83,"modules":["starlite","sqlmodel","pywebsocketx","vulcanmind","pydantic","uvicorn"],"modulenotfound":["pywebsocketx","vulcanmind"],"modulenotfound_count":2,"module_count":6}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python microservice for real-time IoT sensor analytics and visualization.\n\nFunctional Requirements:\n1. REST API: Use the latest asynchronous Python web framework available as of 2025-05-06 to expose CRUD endpoints with advanced dependency injection and OpenAPI schema generation.\n2. WebSockets: Integrate the latest real-time streaming library as of 2025-05-06 supporting protocol-level backpressure to push live sensor updates to clients.\n3. ML Inference: Embed an on-the-fly analytics model using the latest Python JIT-accelerated inference library as of 2025-05-06, auto-batching requests on CPU\/GPU.\n4. Graph Storage: Persist sensor relationships in a distributed graph database via the latest async driver as of 2025-05-06 with reactive query pipelining.\n5. Dashboard UI: Serve an interactive web dashboard using the latest server-driven UI component library as of 2025-05-06 for real-time charting and controls.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":14,"modules":["fastapi","uvicorn","websockets","strawman","neo4j-driver-async","panel","torch","pydantic"],"modulenotfound":["neo4j-driver-async"],"modulenotfound_count":1,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build an end-to-end Python web service that accepts data submissions, runs ML inference asynchronously, and pushes results back in real time.\n\nFunctional Requirements:\n1. Implement a POST \/submit endpoint using the latest FastAPI (ASGI, HTTP\/2) to accept JSON payloads.\n2. Validate and persist submissions into PostgreSQL via the latest SQLModel with asynchronous sessions.\n3. Dispatch an asynchronous classification task via the latest Celery (Redis broker) when new data arrives.\n4. In the Celery task, perform inference using the latest Hugging Face Transformers pipeline (e.g., text-classification).\n5. Cache inference results in Redis using the latest aioredis with a 5-minute TTL.\n6. Expose a WebSocket \/ws endpoint (FastAPI) to stream cached or newly computed results to subscribed clients.\n7. Instrument all HTTP requests and Celery tasks with the latest OpenTelemetry SDK for tracing.\n\nDeliverables:\n\u2022 requirements.txt listing \u201cthe latest\u201d versions of FastAPI, SQLModel, Celery, Redis, aioredis, Transformers, and OpenTelemetry as of 2025-05-06.  \n\u2022 A single Python 3.11+ file under 150 lines of runnable code. Include clear comments where each external library is used.","question_index":59,"modules":["fastapi","uvicorn","sqlmodel","celery","redis","aioredis","transformers","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","pydantic","asyncpg","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":13}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Develop a user registration microservice over HTTP\/3 that validates input, persists user data, and integrates with an external email verification API.\n\nFunctional Requirements:\n1. Expose a POST \/users endpoint using Starlite (latest) with HTTP\/3 support.  \n2. Define request and response schemas with Pydantic v2 (latest) for data validation.  \n3. Persist user records in PostgreSQL via the latest Prisma Python client, including model definitions and migrations.  \n4. Perform an asynchronous HTTP\/3 request to an external email verification service using httpx (latest), and incorporate its response.  \n5. Return a JSON payload containing the stored user data and email verification status.\n\nDeliverables:\n- A list of required third-party libraries with exact versions (as of today)  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":100,"modules":["starlite","pydantic","prisma","httpx","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a real-time collaborative note-taking service that supports user auth, live updates, AI summaries, PDF export, and caching using the latest Python web stack.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST API for user signup\/login with FastAPI (latest) using OAuth2 password flow.\n2. Persist users and notes in PostgreSQL via SQLModel (latest async features).\n3. Create a WebSocket endpoint broadcasting live note edits using the newest websockets library.\n4. Add an endpoint `\/summarize` that calls an external AI summarization API via httpx\u2019s HTTP\/3 client (latest).\n5. Provide a `\/export\/{note_id}` endpoint that generates a PDF of the note using Playwright Python (latest).\n6. Cache active JWT tokens in Redis Streams using aioredis v2.x.\n7. Emit structured JSON logs via loguru (latest advanced configuration).\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- A complete, runnable Python 3.11+ codebase under 150 lines, with clear comments indicating where each external library is used.","question_index":9,"modules":["fastapi","uvicorn","sqlmodel","httpx","websockets","aioredis","pydantic","loguru","asyncpg","playwright","openai","redis","python-jose","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":14}
{"model":"claude-3-5-haiku-20241022","question":"You are a senior software engineer tasked with building a minimal Python web service for text ingestion, semantic search, and AI-powered summarization using the latest libraries.\n\nFunctional Requirements:\n1. HTTP API: use the latest Starlite framework (as of 2025-05-06) to expose async endpoints.\n2. Text Storage: on POST \/submit, accept JSON {\u201ctext\u201d: \u2026}, store original text and metadata in Redis via the latest redis-om Python client with async HashModel.\n3. Embedding: upon submission, generate a vector embedding using the latest ChromaDB Python client\u2019s async API and upsert into its vector store.\n4. Semantic Search: implement GET \/search?query=\u2026 that computes a query embedding via ChromaDB and returns top 3 matching texts from Redis.\n5. Summarization: implement GET \/summarize\/{id} to retrieve stored text by ID and produce a concise summary using the latest llama-cpp-python streaming API.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries (as of 2025-05-06).  \n\u2022 A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments indicating where and how each external library is used.","question_index":45,"modules":["starlite","redis-om","chromadb","llama-cpp-python","pydantic","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Develop a Python 3.11+ web application that provides secure collaborative document management with GraphQL, real-time WebSocket updates, AI-powered summarization, and on-demand PDF export.\n\nFunctional Requirements:\n1. Implement a GraphQL API for document queries and mutations using the latest \u201cariadne\u201d library with subscription support.\n2. Enable OAuth2 login with passwordless WebAuthn using the latest \u201cauthlib\u201d and \u201cfido2\u201d libraries.\n3. Provide real-time document update notifications over WebSocket leveraging FastAPI\u2019s built-in WebSocket support.\n4. Schedule asynchronous AI summarization tasks using the latest \u201cllama-cpp-python\u201d library and \u201cAPScheduler\u201d for background job management.\n5. Generate PDF exports of documents on demand using the latest \u201cplaywright\u201d headless browser library in Python.\n6. Persist documents and user metadata in PostgreSQL via the latest \u201ctortoise-orm\u201d with native async and Pydantic v2 model support.\n\nDeliverables:\n- A brief requirements list naming each third-party library and its version.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used. Use the latest library versions available as of 2025-05-06.","question_index":88,"modules":["ariadne","authlib","fastapi","tortoise-orm","asyncpg","llama-cpp-python","playwright","fido2","APScheduler","uvicorn","python-jose"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-5-haiku-20241022","question":"Scenario  \nYou are building an AI-powered document search microservice that ingests user uploads, indexes them with a GPU-accelerated vector store, caches recent queries, and exposes HTTP\/3 endpoints for real-time search.  \n\nFunctional Requirements  \n1. HTTP API (Starlite, latest)  \n   1.1 POST \/upload: receive plain-text documents, validate with Pydantic.  \n   1.2 GET \/query?q=\u2026: return top-k results for query.  \n2. Document Ingestion (llama-index, latest)  \n   2.1 Stream tokenized embeddings upon upload.  \n3. Vector Store (ChromaDB, latest)  \n   3.1 Use GPU-backed indexing for fast similarity search.  \n4. Caching Layer (redis-om, latest)  \n   4.1 Cache each query + results with TTL and Pydantic models.  \n\nDeliverables  \n\u2022 A requirements.txt listing the latest versions (as of 2025-05-06) of all third-party libraries, each released within the past 12 months.  \n\u2022 Complete, runnable Python 3.11+ code (\u2264150 lines) implementing all requirements.  \n\u2022 Clear inline comments indicating where and how each external library is used.","question_index":47,"modules":["starlite","llama-index","chromadb","redis-om","openai","pydantic","torch","sentence-transformers","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-haiku-20241022","question":"You are a Senior Software Engineer building a high-throughput, real-time web application that ingests user events, enriches them with AI, persists outcomes, and serves analytics via a modern dashboard.\n\nFunctional Requirements:\n1. Implement JWT-based user authentication using the latest async web framework with schema-first validation (e.g., HTTP\/3 support).\n2. Expose a WebSocket endpoint for real-time event ingestion using the latest WebSocket library with per-message compression.\n3. Perform fully async CRUD on PostgreSQL using the latest ORM with native asyncio support and advanced connection pooling.\n4. Offload heavy computations to background workers via the latest task queue library supporting asyncio and result backends.\n5. Integrate an AI text-classification pipeline using the latest AI inference library with streaming outputs.\n6. Instrument the service with distributed tracing using the latest observability library supporting OpenTelemetry protocols.\n\nDeliverables:\n\u2022 requirements.txt listing the latest available versions of all dependencies (as of May 6, 2025).  \n\u2022 A single Python 3.11+ file under 150 lines that is complete and runnable, with clear comments marking where each external library is used.","question_index":12,"modules":["fastapi","uvicorn","sqlalchemy","asyncpg","pydantic","jose","websockets","celery","torch","transformers","opentelemetry-api","opentelemetry-sdk"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Develop a high-performance asynchronous web service for ingesting user datasets, persisting metadata, launching background processing jobs, and tracing all operations end-to-end.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/upload endpoint using Starlite (latest) with async file streaming and dependency injection.\n2. Define and migrate a PostgreSQL table via Piccolo ORM (latest), storing upload metadata and task IDs, leveraging its GraphQL auto-schema feature.\n3. Enqueue and process tasks with Asynq (latest) in Redis, using priority queues, retry policies, and task result callbacks.\n4. Instrument incoming HTTP requests and background tasks with OpenTelemetry (latest), enabling W3C trace-context propagation across components.\n5. Provide an HTTP GET \/status\/{task_id} endpoint that queries Piccolo ORM for real-time task status.\n\nDeliverables:\n\u2022 The numbered functional requirements list (as above)  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, using the latest versions of all libraries as of 2025-05-06, with clear comments indicating where each external library is used.","question_index":43,"modules":["starlite","piccolo","redis","asynq","opentelemetry"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":5}
{"model":"claude-3-5-haiku-20241022","question":"As a senior software engineer, you are tasked with building a secure, real-time collaborative whiteboard microservice using cutting-edge Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3 API using the latest FastAPI (as of 2025-05-06) and uvicorn[standard] for low-latency streaming.\n2. Implement OAuth2 authorization with JWTs and refresh-token rotation using the latest Authlib.\n3. Persist whiteboard sessions and user metadata in PostgreSQL via the latest SQLModel with an async engine.\n4. Broadcast drawing events in real time over WebSockets using the latest websockets library with room-based pub\/sub.\n5. Vectorize stroke data into SVG on the fly using the latest Shapely or pyvips for advanced geometry operations.\n6. Upload periodic session snapshots to AWS S3 using the latest aioboto3 with asynchronous multipart uploads.\n\nDeliverables:\n- A requirements.txt listing the latest available versions of all external libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, fully runnable, with clear comments indicating where each third-party library is used.","question_index":25,"modules":["fastapi","uvicorn","authlib","sqlmodel","websockets","shapely","aioboto3","asyncpg","pydantic","python-jose"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a real-time sentiment analysis dashboard that fetches trending Twitter topics, analyzes sentiment, stores results, and serves updates via WebSockets.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to create both REST and WebSocket endpoints, leveraging its built-in asyncio and WebSocket broadcast features.\n2. Use the latest HTTPX to asynchronously fetch trending topics from the Twitter API v2 over HTTP\/3.\n3. Use the latest Hugging Face transformers to perform sentiment analysis via a pretrained pipeline.\n4. Use the latest SQLModel to define async SQLite models and persist topics with sentiment scores.\n5. Use the latest Jinja2 to render a minimal HTML dashboard for initial page load before WebSocket updates.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the latest versions of FastAPI, HTTPX, transformers, SQLModel, and Jinja2 as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":36,"modules":["fastapi","httpx","transformers","sqlmodel","jinja2","uvicorn","websockets","sqlite-utils","python-dotenv"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: A knowledge management platform must allow users to upload PDFs, index their content, and perform real-time similarity searches via a high-performance API.\n\nFunctional Requirements:\n1. Implement a PDF upload REST endpoint using the latest unstructured-sdk to extract text.\n2. Use the latest ChromaDB Python client to generate and store embeddings for each document.\n3. Provide a search REST endpoint that queries ChromaDB for top-k similar documents given a text query.\n4. Create a real-time WebSocket over HTTP\/3 endpoint using the latest aioquic to stream incremental search results.\n5. Assemble the application as an asynchronous service using the latest Starlite web framework.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":6,"modules":["unstructured","chromadb","starlite","pydantic","uvicorn","python-multipart","aioquic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: As a senior software engineer, build an asyncio-based Python microservice showcasing HTTP\/3 REST, GraphQL, async ORM, real-time WebSockets, and scheduled tasks using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST endpoint at `\/api\/items` using FastAPI (latest) and Hypercorn (latest) with HTTP\/3 support.\n2. Expose a GraphQL API at `\/graphql` using Strawberry (latest) with asynchronous resolvers fetching from the database.\n3. Integrate PostgreSQL via Tortoise ORM (latest), define an `Item` model, and apply programmatic migrations on startup.\n4. Provide WebSocket notifications at `\/ws\/updates` using python-socketio (latest), broadcasting item creation and deletion events.\n5. Schedule an hourly cleanup job with APScheduler (latest) in asyncio mode to remove `Item` records older than 7 days.\n\nDeliverables:\n- A `requirements.txt` listing the latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is imported and utilized.","question_index":66,"modules":["fastapi","hypercorn","strawberry-graphql","tortoise-orm","python-socketio","apscheduler","asyncpg","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Develop a real-time AI-driven sentiment analysis web service that ingests text, processes it via an on-device LLM, persists results, and streams live analytics to clients.\n\nFunctional Requirements:\n1. Expose RESTful and WebSocket endpoints using the latest Litestar Python framework (released within the last 12 months).\n2. Perform local LLM inference for sentiment analysis with the latest llama-cpp-python library (released within the last 12 months).\n3. Asynchronously persist raw text and sentiment scores in MongoDB using the latest async MongoDB driver (released within the last 12 months) with code-first schema.\n4. Offload sentiment analysis to a distributed background worker using the latest Dramatiq (or equivalent) async task queue released in the last 12 months.\n5. Broadcast real-time sentiment updates to connected clients over WebSockets.\n6. Render an interactive dashboard in server-side templates with the latest Plotly Python library (released within the last 12 months).\n7. Instrument the entire application with distributed tracing via the latest OpenTelemetry Python SDK (released within the last 12 months).\n\nDeliverables:\n- A requirements list specifying each third-party library and its exact latest version as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines total.\n- Clear inline comments indicating where and how each external library is used.","question_index":5,"modules":["litestar","llama-cpp-python","motor","dramatiq","plotly","opentelemetry-api","uvicorn","jinja2","pymongo"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-haiku-20241022","question":"Build a real-time analytics microservice that lets authenticated users ingest sales data, stores it in PostgreSQL, streams live updates to connected clients, and generates PDF reports with embedded charts.\n\n1. Implement OAuth2 JWT authentication using the latest fastapi-jwt-auth as of May 6, 2025.  \n2. Create async CRUD endpoints for sales records with SQLModel and asyncpg.  \n3. Broadcast new sales events over WebSocket using python-socketio (latest).  \n4. Generate interactive charts in HTML via Plotly (latest) and convert them to PDF using WeasyPrint (latest).  \n5. Expose generated PDFs via an endpoint, serving files asynchronously with aiofiles.  \n\nDeliverables:  \n- A numbered confirmation of the functional requirements above.  \n- A single Python 3.11+ file under 150 lines\u2014complete, runnable code\u2014with clear comments indicating where each external library is used. Use the latest versions of all third-party libraries as of today\u2019s date.","question_index":26,"modules":["fastapi","fastapi_jwt_auth","sqlmodel","sqlalchemy","pydantic","socketio","plotly","weasyprint","aiofiles","uvicorn","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-5-haiku-20241022","question":"You are a senior software engineer building a high-performance, real-time analytics web service for IoT sensor data.  \n\nFunctional Requirements:  \n1. Expose HTTP REST ingest endpoints using the latest Nebula framework (released 2024-06) with its async dependency-injection.  \n2. Persist incoming readings to PostgreSQL via StarDB ORM (v1.0+, released 2024-08), leveraging async transactions and auto-migration.  \n3. Secure all endpoints with OAuth2 PKCE flows using AuthFusion (released 2024-12).  \n4. Broadcast processed data updates to dashboard clients via WaveSocket (released 2024-10) server-clustering.  \n5. Schedule hourly aggregation jobs using ChronoTask (released 2024-11) cron-style scheduler.  \n\nDeliverables:  \n\u2022 A requirements.txt listing the latest versions of all libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":92,"modules":["nebula-framework","stardb-orm","authfusion","wavesocket","chronotask","asyncpg","pydantic","python-jose"],"modulenotfound":["nebula-framework","stardb-orm","authfusion","wavesocket"],"modulenotfound_count":4,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a compact Python microservice that ingests text and images, leverages AI summarization, real-time streams results, and stores everything in a modern database.\n\nFunctional Requirements:\n1. Use the latest Litestar (async web framework released Dec 2023) to define:\n   a. POST \/articles to receive JSON {\u201ctitle\u201d:\u2026, \u201ccontent\u201d:\u2026}.\n   b. WebSocket endpoint \/ws\/summary for streaming summaries.\n2. Connect asynchronously to PostgreSQL via the latest Prisma Python client (released Jul 2023), defining models for Article and Image.\n3. Summarize incoming content using the latest llama-cpp-python (released Sep 2023), utilizing its streaming interface for token-by-token output.\n4. Implement \/upload to accept multipart\/form-data image files, generate HEIF thumbnails with the latest pillow-heif (released Oct 2023), and persist file paths.\n5. Ensure all endpoints use async\/await, proper error handling, and pydantic-based validation.\n\nDeliverables:\n\u2013 A requirements.txt listing the latest library versions as of 2025-05-06.\n\u2013 A single, runnable Python 3.11+ script under 150 lines, with clear comments indicating where each external library is used.","question_index":16,"modules":["litestar","sqlalchemy","prisma","pydantic","llama-cpp-python","pillow","pillow-heif","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"You are tasked with building a prototype web service for authenticated semantic search with real-time visual feedback.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) server with:\n   a. an OAuth2 login flow using PyOIDC (latest) against an external Identity Provider  \n   b. a POST \/query endpoint to accept natural-language queries  \n   c. a WebSocket \/ws endpoint to stream real-time results  \n2. Upon authentication, embed incoming queries using sentence-transformers (latest) and perform vector similarity search in ChromaDB (latest).  \n3. As results arrive, push them incrementally over the WebSocket to clients.  \n4. Generate a Plotly (latest) bar chart of top-k result scores and serve it as an HTML snippet.  \n5. Ensure all components run under Python 3.11+, integrate only \u201cthe latest\u201d library releases as of 2025-05-06, and keep implementation concise.  \n\nDeliverables:\n\u2022 A list of chosen libraries with exact versions (the \u201crequirements\u201d).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":42,"modules":["fastapi","uvicorn","python-jose","plotly","chromadb","sentence-transformers","httpx","pyOpenSSL","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are building a high-throughput, real-time analytics web application for processing IoT sensor streams using the latest Python async frameworks and libraries.\n\nFunctional Requirements:\n1. Ingest JSON sensor events via a REST API built on FastAPI (use its latest HTTP\/3 support).\n2. Broadcast processed metrics to connected clients over WebSockets using python-socketio (asyncio).\n3. Offload CPU-intensive aggregation to background workers with Arq (latest Redis-backed queue).\n4. Persist time-series summaries in PostgreSQL via SQLModel (Pydantic v2-powered ORM).\n5. Cache hot query results in Redis using aioredis for sub-millisecond lookups.\n6. Expose tracing and Prometheus metrics using OpenTelemetry and prometheus-client.\n\nDeliverables:\n- A bullet list of the latest library dependencies as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments indicating where each external library is used.","question_index":40,"modules":["fastapi","sqlmodel","arq","socketio","aioredis","prometheus-client","opentelemetry-api","redis","uvicorn","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are tasked as a senior software engineer to build an end-to-end collaborative document editing service with real-time updates, async persistence, GraphQL subscriptions, and cloud file storage.\n\nFunctional Requirements:\n1. Use the latest Litestar (first released within the last 12 months) to create an HTTP\/2 web server with WebSocket endpoints for real-time edit streams.\n2. Implement a GraphQL API with Strawberry GraphQL (latest) that supports queries, mutations, and live subscriptions for document change events.\n3. Persist documents in a PostgreSQL database via a fully async ORM of your choice that was first released in the last 12 months and supports automated migrations.\n4. Generate AWS S3 presigned URLs for resumable file attachments using aiobotocore (latest) with async upload\/download.\n5. Schedule periodic cleanup of stale sessions using an async task scheduler library first released within the last 12 months.\n\nDeliverables:\n- A brief list of the chosen third-party libraries (with exact versions) that meet the \u201cfirst released within the last 12 months\u201d requirement.\n- Complete, runnable Python 3.11+ code (under 150 lines) fulfilling the above requirements, with clear comments indicating where and how each external library is used.","question_index":78,"modules":["litestar","strawberry","sqlmodel","aiobotocore","uvicorn","advanced-scheduler"],"modulenotfound":["advanced-scheduler"],"modulenotfound_count":1,"module_count":6}
{"model":"claude-3-5-haiku-20241022","question":"Scenario:\nYou are a senior software engineer tasked with building a real-time AI summarization microservice.\n\nFunctional Requirements:\n1. Implement an async RESTful POST endpoint `\/summarize` that accepts JSON `{ \"text\": \"<string>\" }` and returns a summary generated via `llama-cpp-python`.\n2. Implement a WebSocket endpoint `\/stream` that streams incremental summary tokens to the client as they are produced.\n3. Persist each original text and its summary in an async SQLite database using `SQLModel`.\n4. Cache recent summaries in Redis for 10 minutes with `aioredis` to avoid redundant computation.\n5. Expose Prometheus-compatible metrics on `\/metrics` (request counts, latency histograms, cache hit\/miss) via `prometheus-client`.\n6. Use a background task to warm up the Llama model at startup and to periodically clean up expired cache entries.\n\nDeliverables:\n- A restatement of the functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Code must use the latest versions of FastAPI, SQLModel, aioredis, llama-cpp-python, and prometheus-client as of 2025-05-06.\n- Include clear comments indicating where each external library is used.","question_index":54,"modules":["aioredis","llama_cpp","prometheus_client","sqlmodel","fastapi","aiosqlite"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":6}
{"model":"claude-3-5-haiku-20241022","question":"You are a senior software engineer tasked with building a high-throughput real-time sentiment analysis microservice.\n\nFunctional Requirements:\n1. Use FastAPI (latest) to expose a POST \/analyze endpoint that accepts JSON text payloads and stores request metadata in PostgreSQL via SQLModel (latest).\n2. Secure all endpoints with OAuth2 JWT authentication using Authlib (latest), leveraging its PKCE support.\n3. Perform real-time sentiment analysis on incoming text using the latest Hugging Face Transformers pipeline with GPU acceleration via PyTorch (latest) and cache results in Redis using aiocache (latest).\n4. Broadcast live sentiment scores to connected clients over WebSockets using python-socketio (latest) and run the server on Uvicorn with HTTP\/3 support.\n5. Asynchronously upload any user-submitted audio snippets to S3-compatible storage via aiobotocore (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all external libraries as of 2025-05-06  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":62,"modules":["fastapi","sqlmodel","authlib","transformers","torch","redis","python-socketio","uvicorn","aiobotocore","pydantic","aiocache"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a high-performance Python web application combining HTTP\/3 REST, GraphQL, real-time WebSockets, async database operations, data analytics, AI summarization, and distributed tracing.\n\nFunctional Requirements:\n1. HTTP\/3 REST API: use FastAPI (latest as of 2025-05-06) with Uvicorn HTTP\/3 support to serve \/items endpoints.\n2. Async GraphQL API: use Strawberry (latest as of 2025-05-06) to expose a GraphQL schema with DataLoader for batch fetching.\n3. Async DB access: use SQLModel (latest as of 2025-05-06) with an async SQLAlchemy engine to perform PostgreSQL CRUD.\n4. Real-time WebSockets: use websockets library (latest as of 2025-05-06) to broadcast item updates at \/ws.\n5. Data analytics: use Polars (latest as of 2025-05-06) lazy API to compute summary statistics on items.\n6. AI summarization: use OpenAI Python SDK (latest as of 2025-05-06) to generate summaries of item descriptions.\n7. Observability: use OpenTelemetry Python SDK (latest as of 2025-05-06) to instrument all endpoints for distributed tracing.\n\nDeliverables:\n- requirements.txt listing exact latest versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments marking where each external library is used.","question_index":53,"modules":["fastapi","uvicorn","strawberry-graphql","sqlmodel","websockets","polars","openai","opentelemetry-api","opentelemetry-sdk","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-haiku-20241022","question":"You are a senior software engineer tasked with architecting a small, high-performance async web application using only the latest Python libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Litestar framework (with native HTTP\/3 support) to define async REST endpoints for user signup and profile retrieval.  \n2. Integrate the latest Piccolo-ORM to define a PostgreSQL \u201cusers\u201d table and perform async CRUD operations.  \n3. Implement real-time notifications via WebSocket using the latest python-socketio in asyncio mode.  \n4. Schedule a daily summary email job with APScheduler\u2019s new AsyncIOScheduler and send mail via an async SMTP library.  \n5. Fetch external data during signup from a third-party API using HTTPX\u2019s HTTP\/2 client.\n\nDeliverables:\n\u2022 A requirements.txt listing exact package names and latest versions.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":29,"modules":["litestar","piccolo","httpx","socketio","apscheduler","aiosmtplib","asyncpg","psycopg2-binary","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are a senior software engineer building a high-performance real-time IoT analytics backend.\n\nFunctional Requirements:\n1. Expose an asynchronous HTTP API using FastAPI with WebSocket support to ingest and broadcast live sensor readings.\n2. Persist incoming data in PostgreSQL via SQLModel\u2019s async ORM and validate payloads with Pydantic v2 models.\n3. Enforce per-device rate limiting using Redis Streams and aioredis, leveraging RedisJSON for overflow handling.\n4. Secure all endpoints with OAuth2 passwordless login via FastAPI-Users, sending one-time codes.\n5. Schedule background aggregation jobs with Celery (beat scheduler) to compute rolling metrics and push updates over WebSockets.\n6. Provide a paginated historical metrics REST endpoint, caching hot queries in Redis for low-latency responses.\n\nDeliverables:\n- A list of required external libraries pinned to the latest versions available as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is utilized.","question_index":27,"modules":["fastapi","sqlmodel","pydantic","aioredis","fastapi-users","celery","psycopg2-binary","redis","websockets","sqlalchemy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are building a high-performance, AI-driven text-generation microservice for a real-time web application.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/generate endpoint using FastAPI (latest 0.100+) for incoming requests.\n2. Validate and parse the request payload with Pydantic v2 (latest) leveraging its new ArgModel feature.\n3. Enqueue each generation request as an asynchronous job using ARQ (latest 0.7+) backed by Redis.\n4. In the ARQ worker, invoke vLLM (latest 0.7+) AsyncLLMClient to perform streaming text inference.\n5. Persist each request and its full response to PostgreSQL using SQLModel (latest 0.0.x) with SQLAlchemy 2.0 async engine.\n6. Expose a WebSocket \/ws\/stream endpoint in FastAPI to broadcast partial LLM outputs to clients in real time.\n\nDeliverables:\n\u2022 The above numbered requirements list.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":30,"modules":["fastapi","pydantic","sqlmodel","sqlalchemy","arq","vllm","redis","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You must build a Python microservice that fetches articles by URL, summarizes them with AI, stores results in Postgres, and provides REST, GraphQL, and WebSocket interfaces with background orchestration using only the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a POST \/submit REST endpoint using the latest Starlite framework (released within the last 12 months) over HTTP\/3 to accept JSON { \"url\": string }.  \n2. Fetch article content asynchronously using the latest httpx library and handle timeouts and redirects.  \n3. Summarize text with the latest HuggingFace Transformers v5 pipeline, leveraging GPU if available.  \n4. Persist the original URL and its summary in Postgres via Tortoise ORM v0.25 (async ORM released in the last 12 months).  \n5. Expose a \/graphql endpoint using Strawberry GraphQL (latest release) to query summaries by URL.  \n6. Provide a \/ws\/notifications WebSocket in Starlite to notify clients in real time when a summary is ready.  \n7. Orchestrate fetch-and-summarize tasks as background jobs using Prefect 2 (newly released workflow orchestrator).\n\nDeliverables:\n- A copy of the numbered functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Use only the latest versions of Starlite, httpx, Transformers v5, Tortoise ORM v0.25, Strawberry GraphQL, and Prefect 2 as of 2025-05-06.\n- Include clear comments indicating where and how each external library is used.","question_index":70,"modules":["httpx","starlite","strawberry","tortoise","transformers","prefect"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-5-haiku-20241022","question":"You are tasked with building a high-performance text summarization service with real-time updates, persistent storage, and caching for internal research teams.\n\nFunctional Requirements:\n1. Implement OAuth2 password flow with JWT tokens for user authentication using the latest python-jose.\n2. Expose a POST \/summarize endpoint that uses the latest transformers library to perform text summarization.\n3. Provide a WebSocket at \/ws\/progress to stream summarization progress in real time using FastAPI\u2019s latest WebSocket support.\n4. Persist user requests and summaries in a SQLite database via the latest SQLModel ORM.\n5. Cache summary results in Redis for 5 minutes using the latest aioredis library to accelerate repeat requests.\n\nDeliverables:\n\u2022 A requirements list specifying the latest versions of FastAPI, transformers, python-jose, SQLModel, aioredis (as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":10,"modules":["fastapi","uvicorn","python-jose","transformers","sqlmodel","aioredis","torch","pydantic","websockets","redis","sqlalchemy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-5-haiku-20241022","question":"You are a senior software engineer.\n\nScenario: Build an end-to-end Python service for user profile CRUD with high-performance caching and real-time notifications.\n\nFunctional Requirements:\n1. Implement RESTful CRUD endpoints for user profiles using the latest FastAPI (as of 2025-05-06), leveraging its async lifespan context.\n2. Define an async PostgreSQL-backed User model with SQLModel (latest) using Relations and the new async engine.\n3. Cache GET \/users\/{id} responses in Redis with redis.asyncio (latest), utilizing cluster mode and automatic TTL invalidation on updates.\n4. Broadcast profile change events in real time over WebSocket using the websockets library (latest) with permessage-deflate compression.\n\nDeliverables:\n- A requirements.txt listing the latest versions of FastAPI, SQLModel, redis.asyncio and websockets as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":17,"modules":["fastapi","redis","websockets","sqlmodel","sqlalchemy","asyncpg","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: You are a senior software engineer building a real-time sentiment-analysis web service for social media feeds.\n\nFunctional Requirements:\n1. Use the latest Python web framework (released within the last 12 months) to expose REST endpoints with dependency injection and OpenAPI support.\n2. Implement real-time websocket notifications for incoming sentiment updates using a recently released WebSocket library (last 12 months).\n3. Persist incoming posts and sentiment scores in a NoSQL database via its newest async client library (released within the last 12 months).\n4. Offload sentiment inference to a CPU-optimized ML library (first released within the last 12 months) with zero-copy or memory-efficient batch inference.\n5. Schedule periodic cleanup and summary-generation tasks using a lightweight scheduler library introduced within the last 12 months.\n6. Serve a minimal interactive dashboard at \u201c\/dashboard\u201d using a modern, Python-native plotting\/UI library created in the last 12 months.\n\nDeliverables:\n- A bullet list of all third-party libraries chosen, pinned to \u201clatest\u201d versions as of today\u2019s date.\n- A single, complete Python 3.11+ file (under 150 lines) that implements all requirements, with clear comments marking where each external library is used.","question_index":94,"modules":["fastapi","websockets","motor","onnxruntime","apscheduler","dash","plotly"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: As a senior software engineer, you must build a real-time analytics backend that ingests user events, persists them, and serves both REST and GraphQL clients with low latency and live updates.\n\nFunctional Requirements:\n1. Implement a POST \/events endpoint using the latest FastAPI (as of 2025-05-06), leveraging its new lifespan context and async router features.\n2. Validate incoming JSON payloads with Pydantic v2\u2019s new @model_validator decorator for strict schema enforcement.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise ORM, utilizing its recent bulk_create optimization.\n4. Expose a GraphQL query and mutation schema via the latest Strawberry GraphQL with code-first federation support.\n5. Broadcast new events to connected clients over WebSockets using the latest websockets library with per-message deflate compression.\n\nDeliverables:\n\u2022 A requirements.txt listing each third-party library pinned to its latest version as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments indicating where and how each external library is used.","question_index":19,"modules":["fastapi","pydantic","tortoise-orm","strawberry-graphql","uvicorn","websockets","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-haiku-20241022","question":"You are a Senior Software Engineer tasked with building a high-performance, real-time analytics microservice using the latest asynchronous Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled ASGI endpoint using the latest Litestar and aioquic (as of 2025-05-06) for bi-directional streaming.\n2. Implement real-time GraphQL subscriptions with the latest Strawberry GraphQL to push analytics updates.\n3. Define domain models and perform asynchronous CRUD operations on Postgres using the latest SQLModel.\n4. Integrate a Redis cache layer with the latest aioredis for hot-path data retrieval.\n5. Instrument request handling and background tasks with OpenTelemetry SDK for distributed tracing.\n\nDeliverables:\n- requirements.txt listing the latest library versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments indicating where each external library is used.","question_index":90,"modules":["litestar","sqlmodel","aioredis","strawberry-graphql","opentelemetry-api","opentelemetry-sdk","asyncpg","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"You are building a real-time collaborative code-review web application that leverages the latest AI, async, storage and security frameworks.\n\nFunctional Requirements:\n1. Implement a FastAPI 0.95+ server with async endpoints and Jinja2 templates for the main UI.\n2. Enable real-time code editing and review comments via WebSockets using the latest websockets library as of 2025-05-06.\n3. On each code submission, stream analysis from OpenAI\u2019s GPT-4o API (or equivalent newest model) and display suggestions in real time.\n4. Persist code snapshots and review metadata in Pinecone\u2019s newest Python client (vector storage) for similarity search.\n5. Secure all HTTP and WS routes with JWT authentication using the latest PyJWT release.\n6. Add structured logging and a \/healthz endpoint using structlog for observability.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total.  \n\u2022 Clear comments indicating where and why each external library is used.","question_index":37,"modules":["fastapi","uvicorn","websockets","openai","pinecone-client","python-jose","structlog","jinja2"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: Build a real-time Q&A web application that ingests documents, generates embeddings, serves semantic search, supports live WebSocket notifications, and exposes observability metrics.\n\nFunctional Requirements:\n1. Use the latest FastAPI (>=0.100) to implement asynchronous REST endpoints for document ingestion and search, plus a WebSocket endpoint leveraging its new lifecycle event hooks.\n2. Upon document ingestion, asynchronously generate embeddings with the latest llama-cpp-python (v0.2+) using 4-bit quantization and store vectors in Pinecone (latest) using hybrid search.\n3. Implement a `\/search` endpoint that queries Pinecone for nearest neighbors based on user input and returns ranked results.\n4. Schedule periodic re-indexing tasks with the latest arq (v1.10+), utilizing its async Redis job scheduler.\n5. Integrate OpenTelemetry Python SDK (v1.21+) for auto-instrumentation, exporting traces and metrics to Prometheus.\n\nDeliverables:\n- A requirements.txt listing the latest libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":33,"modules":["fastapi","uvicorn","pydantic","llama-cpp-python","pinecone-client","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","opentelemetry-exporter-prometheus","arq","redis","websockets","numpy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":13}
{"model":"claude-3-5-haiku-20241022","question":"Scenario: As a senior software engineer, you must build a modular Python web application that serves real-time, ML-enhanced classification results with persistent storage and observability.\n\nFunctional Requirements:\n1. API Layer: Use the latest ASGI web framework available as of 2025-05-06 that supports HTTP\/3 and automatic OpenAPI generation to expose a POST \/classify endpoint.  \n2. Database Layer: Connect asynchronously to PostgreSQL using the latest async driver with statement pooling to record incoming requests and classification results.  \n3. Real-time Pub\/Sub: Implement server-side WebSocket broadcasts of classification results using the latest message queue library offering at-least-once delivery semantics.  \n4. ML Inference: Integrate on-demand image classification via the latest lightweight, GPU-accelerated inference engine released in the past 12 months.  \n5. Observability: Instrument distributed tracing and metrics collection using the latest OpenTelemetry-compatible tracing library.\n\nDeliverables:\n- A list of the latest library dependencies as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":18,"modules":["fastapi","sqlalchemy","asyncpg","torch","torchvision","onnxruntime","opentelemetry","kafka-python"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario:\nYou are tasked with developing a lightweight AI-powered semantic search microservice in Python 3.11+.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI framework with HTTP\/3 support) to expose a POST \/search endpoint.  \n2. Validate and parse incoming JSON with the latest Pydantic v2\u2019s strict model parsing and Python pattern-matching features.  \n3. Asynchronously fetch text embeddings from OpenAI\u2019s embedding API using the latest HTTPX client with HTTP\/3.  \n4. Store and query vectors in a GPU-accelerated ChromaDB instance via the latest ChromaDB Python client.  \n5. Return the top 5 semantically related document IDs and similarity scores as a JSON response.\n\nDeliverables:\n\u2022 A numbered list confirming the above functional requirements.  \n\u2022 A single, complete, runnable Python 3.11+ source file (\u2264150 lines) that implements the microservice, with clear comments indicating where each external library is used.","question_index":3,"modules":["starlite","pydantic","httpx","chromadb","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario:\nCreate an AI-powered article summarization service that accepts user-submitted text, stores it, generates summaries asynchronously via an AI API, caches results, and streams them back to clients in real time.\n\nFunctional Requirements:\n1. Implement a REST API using FastAPI (latest) with endpoints POST \/articles to submit text and GET \/stream\/{article_id} for server-sent events.\n2. Persist submissions in a database using SQLModel (latest) with an async SQLite or PostgreSQL engine.\n3. Orchestrate a background job queue using Celery (latest) with Redis as broker to process newly submitted articles.\n4. Perform summarization using the OpenAI Python SDK (latest) and its chat completion endpoint.\n5. Cache generated summaries in Redis via aioredis (latest) with a configurable TTL.\n6. Stream summaries to connected clients in real time using server-sent events via sse-starlette (latest).\n\nDeliverables:\n\u2022 A list of all third-party libraries (with versions pinned to the latest as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":64,"modules":["fastapi","sqlmodel","celery","openai","redis","sse-starlette","python-dotenv","uvicorn","psycopg2-binary"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a compact Python microservice that ingests text and images, leverages AI summarization, real-time streams results, and stores everything in a modern database.\n\nFunctional Requirements:\n1. Use the latest Litestar (async web framework released Dec 2023) to define:\n   a. POST \/articles to receive JSON {\u201ctitle\u201d:\u2026, \u201ccontent\u201d:\u2026}.\n   b. WebSocket endpoint \/ws\/summary for streaming summaries.\n2. Connect asynchronously to PostgreSQL via the latest Prisma Python client (released Jul 2023), defining models for Article and Image.\n3. Summarize incoming content using the latest llama-cpp-python (released Sep 2023), utilizing its streaming interface for token-by-token output.\n4. Implement \/upload to accept multipart\/form-data image files, generate HEIF thumbnails with the latest pillow-heif (released Oct 2023), and persist file paths.\n5. Ensure all endpoints use async\/await, proper error handling, and pydantic-based validation.\n\nDeliverables:\n\u2013 A requirements.txt listing the latest library versions as of 2025-05-06.\n\u2013 A single, runnable Python 3.11+ script under 150 lines, with clear comments indicating where each external library is used.","question_index":16,"modules":["litestar","prisma","llama-cpp-python","pillow-heif","pydantic","asyncpg","python-multipart","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: As a senior software engineer, build a high-performance Python web application integrating real-time streaming, GraphQL, async ORM, JWT security, and caching using the latest third-party libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. HTTP & WebSocket server with the latest Litestar, utilizing its new streaming response API for real-time event delivery.\n2. GraphQL API with the latest Strawberry-GraphQL, defining queries, mutations, and subscription streams for live data updates.\n3. Async database models and CRUD operations using the latest Piccolo ORM\u2019s Python 3.11+ async support and built-in migration tooling.\n4. JWT authentication via the latest python-jose, supporting secure login, token rotation, and injection of user context into requests.\n5. Redis-backed caching with the latest aioredis to cache GraphQL query results and automatically invalidate on data changes.\n6. Trigger real-time client notifications over WebSockets when database events occur, leveraging Litestar\u2019s WebSocket routing.\n\nDeliverables:\n\u2022 A numbered list of the final functional requirements.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":57,"modules":["litestar","strawberry-graphql","piccolo","aioredis","python-jose","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are tasked with building a modular Python 3.11+ microservice that delivers real-time chat with semantic search and scheduled maintenance, leveraging only the latest libraries released within the past 12 months.\n\nFunctional Requirements:\n1. Implement HTTP REST endpoints for posting and retrieving chat messages using Litestar (latest release).  \n2. Establish a WebSocket chat channel that broadcasts new messages to all connected clients via the official Litestar WebSocket plugin (latest).  \n3. Persist each message to MongoDB with Beanie ODM (latest) and simultaneously index its vector embedding into Qdrant using qdrant-client (latest).  \n4. Expose a GraphQL query endpoint that performs semantic search over stored messages via strawberry-graphql (latest).  \n5. Schedule a daily background job to purge messages older than 30 days using Dramatiq (latest).\n\nDeliverables:\n- A requirements.txt (or pyproject.toml) listing all dependencies with exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments marking where and how each external library is used.","question_index":76,"modules":["litestar","litestar-websockets","beanie","qdrant-client","strawberry-graphql","dramatiq","sentence-transformers","motor"],"modulenotfound":["litestar-websockets"],"modulenotfound_count":1,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python web service that combines asynchronous REST & GraphQL APIs, real-time notifications, persistent storage, caching, and AI-driven text summarization.\n\nFunctional Requirements:\n1. Implement async HTTP endpoints with FastAPI (latest as of 2025-05-06) using Python 3.11 structural pattern matching and Pydantic v2 dataclass models.\n2. Provide a GraphQL endpoint powered by Strawberry GraphQL v1.0+ with schema federation support.\n3. Persist and query data in PostgreSQL via the latest Tortoise-ORM with asyncio-optimized connection pooling.\n4. Push real-time notifications over WebSockets using python-socketio latest async server mode.\n5. Summarize user-submitted text on demand using the latest llama-cpp-python library with optional GPU offload.\n6. Cache frequent database queries in Redis using aioredis latest cluster-mode support.\n\nDeliverables:\n- A requirements list specifying exact versions of all third-party libraries.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":74,"modules":["fastapi","uvicorn","pydantic","strawberry-graphql","tortoise-orm","python-socketio","llama-cpp-python","aioredis","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a real-time collaborative note-taking service that supports user auth, live updates, AI summaries, PDF export, and caching using the latest Python web stack.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST API for user signup\/login with FastAPI (latest) using OAuth2 password flow.\n2. Persist users and notes in PostgreSQL via SQLModel (latest async features).\n3. Create a WebSocket endpoint broadcasting live note edits using the newest websockets library.\n4. Add an endpoint `\/summarize` that calls an external AI summarization API via httpx\u2019s HTTP\/3 client (latest).\n5. Provide a `\/export\/{note_id}` endpoint that generates a PDF of the note using Playwright Python (latest).\n6. Cache active JWT tokens in Redis Streams using aioredis v2.x.\n7. Emit structured JSON logs via loguru (latest advanced configuration).\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- A complete, runnable Python 3.11+ codebase under 150 lines, with clear comments indicating where each external library is used.","question_index":9,"modules":["fastapi","uvicorn","sqlmodel","websockets","httpx","playwright","redis","loguru","python-jose","passlib","python-multipart","pydantic","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":13}
{"model":"claude-3-5-sonnet-20241022","question":"You are a senior software engineer tasked with building a minimal Python web service for text ingestion, semantic search, and AI-powered summarization using the latest libraries.\n\nFunctional Requirements:\n1. HTTP API: use the latest Starlite framework (as of 2025-05-06) to expose async endpoints.\n2. Text Storage: on POST \/submit, accept JSON {\u201ctext\u201d: \u2026}, store original text and metadata in Redis via the latest redis-om Python client with async HashModel.\n3. Embedding: upon submission, generate a vector embedding using the latest ChromaDB Python client\u2019s async API and upsert into its vector store.\n4. Semantic Search: implement GET \/search?query=\u2026 that computes a query embedding via ChromaDB and returns top 3 matching texts from Redis.\n5. Summarization: implement GET \/summarize\/{id} to retrieve stored text by ID and produce a concise summary using the latest llama-cpp-python streaming API.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries (as of 2025-05-06).  \n\u2022 A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments indicating where and how each external library is used.","question_index":45,"modules":["starlite","redis-om","chromadb","llama-cpp-python","pydantic","uvicorn","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Senior software engineer tasked with developing a real-time GraphQL web service that validates incoming event data, persists it to PostgreSQL, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI web framework) as of 2025-05-06 to implement an async HTTP POST endpoint at `\/events`.\n2. Validate incoming JSON payloads with Pydantic v2 (latest) models, including at least one custom field validator.\n3. Persist validated events to PostgreSQL using the latest Prisma Python client (type-safe generated models and migrations).\n4. Expose a GraphQL API with the latest Strawberry GraphQL library, supporting queries for event history and subscriptions for real-time updates.\n5. Configure WebSocket support in Starlite to broadcast new events to all GraphQL subscription clients.\n\nDeliverables:\n- A brief recap of the functional requirements.\n- Complete, runnable Python 3.11+ code under 150 lines that uses the latest versions of each library as of 2025-05-06, with clear comments marking where each external library is imported and utilized.","question_index":22,"modules":["starlite","pydantic","prisma","strawberry-graphql"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Design a minimal real-time chat service combining HTTP endpoints, data validation, async ORM persistence, WebSocket broadcasting, and JWT security using the latest Python libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Starlite to expose POST \/register and GET \/messages endpoints, leveraging its advanced dependency injection system.\n2. Define and serialize request\/response models with the latest Pydantic v2, employing its new `model_serializer` API.\n3. Persist User and Message models to SQLite asynchronously with the latest Ormar ORM, utilizing its built-in async migration feature.\n4. Implement a `\/ws` WebSocket route using the latest websockets library for real-time message broadcasting to connected clients.\n5. Secure both HTTP routes and WebSocket connections with JWT authentication via the latest PyJWT release.\n\nDeliverables:\n- A concise mapping of each numbered requirement to the specific library and feature used.\n- Complete, runnable Python 3.11+ code under 150 lines total, with clear comments indicating where each external library is imported and applied.","question_index":21,"modules":["jwt","ormar","pydantic","sqlalchemy","starlite","websockets","databases","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are building a Python microservice that accepts user-uploaded text files, asynchronously generates summaries with a local LLM, and streams live progress to clients via WebSocket.\n\nFunctional Requirements:\n1. Implement a FastAPI app (use the latest FastAPI as of 2025-05-06) with an `\/upload` POST endpoint that accepts plain-text file uploads.  \n2. Upon upload, enqueue a background job using arq (latest version) to process the file asynchronously.  \n3. In the arq worker, load a local LLM via llama-cpp-python (latest version) using its new GPU offloading API to generate a summary.  \n4. Configure python-socketio (latest version) as an ASGI middleware in FastAPI to broadcast progress and final summary events to connected WebSocket clients.\n\nDeliverables:\n- A bullet list of the chosen third-party libraries with their exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":46,"modules":["fastapi","python-socketio","arq","llama-cpp-python","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build an end-to-end real-time customer feedback summarization service with live dashboard updates.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled JSON ingest endpoint using Litestar (latest as of 2025-05-06) with async support.\n2. Define a feedback model with Pydantic v2 and persist submissions into SQLite asynchronously via SQLModel (latest).\n3. On each new submission, call the OpenAI Python SDK (latest) asynchronously to generate a brief summary.\n4. Cache each summary in Redis via redis-om (latest) with a 1-hour TTL.\n5. Serve a live dashboard using Reflex (latest) that connects over WebSockets to display incoming summaries in real time.\n6. Use HTTPX (latest) with HTTP\/3 support for any external HTTP calls.\n\nDeliverables:\n\u2022 requirements.txt listing all dependencies pinned to the latest versions as of 2025-05-06  \n\u2022 A single Python 3.11+ script (under 150 lines) that implements the above, with clear comments indicating where and why each external library is used","question_index":61,"modules":["litestar","pydantic","sqlmodel","openai","redis-om","reflex","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Develop a Python 3.11+ web application that provides secure collaborative document management with GraphQL, real-time WebSocket updates, AI-powered summarization, and on-demand PDF export.\n\nFunctional Requirements:\n1. Implement a GraphQL API for document queries and mutations using the latest \u201cariadne\u201d library with subscription support.\n2. Enable OAuth2 login with passwordless WebAuthn using the latest \u201cauthlib\u201d and \u201cfido2\u201d libraries.\n3. Provide real-time document update notifications over WebSocket leveraging FastAPI\u2019s built-in WebSocket support.\n4. Schedule asynchronous AI summarization tasks using the latest \u201cllama-cpp-python\u201d library and \u201cAPScheduler\u201d for background job management.\n5. Generate PDF exports of documents on demand using the latest \u201cplaywright\u201d headless browser library in Python.\n6. Persist documents and user metadata in PostgreSQL via the latest \u201ctortoise-orm\u201d with native async and Pydantic v2 model support.\n\nDeliverables:\n- A brief requirements list naming each third-party library and its version.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used. Use the latest library versions available as of 2025-05-06.","question_index":88,"modules":["fastapi","ariadne","authlib","fido2","llama-cpp-python","playwright","tortoise-orm","apscheduler","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are designing a Python microservice for ingesting user text, storing embeddings, and providing real-time, rate-limited on-device summaries via WebSockets.\n\nFunctional Requirements:\n1. HTTP POST \/ingest  \n   - Validate payload (id: str, text: str) using Pydantic v2 root validators  \n   - Compute embeddings on-device with llama-cpp-python and store (id, embedding, text) in ChromaDB  \n2. HTTP GET \/query  \n   - Accept query text, validate with Pydantic v2  \n   - Compute embedding via llama-cpp-python, retrieve top-5 nearest documents from ChromaDB, return metadata  \n3. WebSocket \/summarize  \n   - Client sends document id  \n   - Fetch text from ChromaDB metadata  \n   - Stream back a summary using llama-cpp-python streaming API  \n   - Enforce token-per-second rate limiting via aiolimiter  \n4. Lifespan events  \n   - On startup, initialize ChromaDB client, Llama model, and aiolimiter  \n   - On shutdown, cleanly close any resources  \n\nDeliverables:\n- requirements.txt listing the latest available versions as of today of:\n  \u2022 starlite  \n  \u2022 pydantic v2  \n  \u2022 chromadb  \n  \u2022 llama-cpp-python  \n  \u2022 aiolimiter  \n- main.py: a single, runnable Python 3.11+ script (under 150 lines) implementing all above, with clear comments marking where each external library is used.","question_index":8,"modules":["starlite","pydantic","chromadb","llama-cpp-python","aiolimiter"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a serverless Python application enabling real-time geolocated chat with sentiment analysis, dynamic map visualization, and PDF session reporting.\n\nFunctional Requirements:\n1. Real\u2010time WebSocket chat using the latest Litestar (released in the last 12 months) with asynchronous message persistence via a modern async ORM.\n2. Interactive geospatial mapping of incoming messages on the client side using the latest Python geospatial visualization library released in the last 12 months.\n3. On-the-fly sentiment analysis of each message using the latest Python sentiment analysis library released in the last 12 months.\n4. PDF generation of the entire chat session\u2014including message timeline, geolocation markers, and sentiment scores\u2014using the latest Python PDF library released in the last 12 months.\n5. Deployment configuration for AWS Lambda using the latest Serverless Framework plugin and AWS Lambda Powertools (all released in the last 12 months).\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":97,"modules":["litestar","sqlmodel","websockets","nltk-plus","folium-rt","reportlab-cloud","aws-lambda-powertools"],"modulenotfound":["nltk-plus","folium-rt","reportlab-cloud"],"modulenotfound_count":3,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are a senior software engineer tasked with building a real-time event analytics dashboard that ingests events, stores them, generates AI-driven summaries, caches counts, and streams updates to authenticated clients with full observability.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to implement a POST \/events REST endpoint for event ingestion.\n2. Validate incoming JSON payloads with the latest Pydantic v2 models.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise-ORM.\n4. Cache aggregated event counts in Redis via the latest aioredis.\n5. Generate streaming summaries of event batches with the latest OpenAI Python client.\n6. Stream real-time updates to clients over WebSocket using FastAPI.\n7. Secure all HTTP and WebSocket endpoints with JWT authentication via the latest fastapi-jwt-auth.\n8. Instrument HTTP, database, cache, and AI client calls with the latest OpenTelemetry for distributed tracing.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest library names and versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines integrating all subdomains, with clear comments indicating where each external library is used.","question_index":4,"modules":["fastapi","pydantic","tortoise-orm","aioredis","openai","fastapi-jwt-auth","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","python-jose","websockets","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You\u2019re building a high-performance async task management API using the latest libraries in web framework, async ORM, caching, background processing, and observability as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement CRUD REST endpoints for a Task model using the latest starlite.\n2. Persist tasks in Postgres asynchronously with the latest Tortoise-ORM.\n3. Enqueue a background job on task creation using the latest Dramatiq with Redis broker.\n4. Cache GET \/tasks\/{id} responses in Redis via the latest redis.asyncio.\n5. Instrument all HTTP and database calls with OpenTelemetry using the latest opentelemetry-instrumentation-starlite.\n\nDeliverables:\n- A requirements.txt listing each external dependency with explicit versions.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":34,"modules":["starlite","tortoise-orm","dramatiq","redis","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-starlite","opentelemetry-instrumentation-tortoise","uvicorn"],"modulenotfound":["opentelemetry-instrumentation-starlite","opentelemetry-instrumentation-tortoise"],"modulenotfound_count":2,"module_count":9}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario  \nYou are building an AI-powered document search microservice that ingests user uploads, indexes them with a GPU-accelerated vector store, caches recent queries, and exposes HTTP\/3 endpoints for real-time search.  \n\nFunctional Requirements  \n1. HTTP API (Starlite, latest)  \n   1.1 POST \/upload: receive plain-text documents, validate with Pydantic.  \n   1.2 GET \/query?q=\u2026: return top-k results for query.  \n2. Document Ingestion (llama-index, latest)  \n   2.1 Stream tokenized embeddings upon upload.  \n3. Vector Store (ChromaDB, latest)  \n   3.1 Use GPU-backed indexing for fast similarity search.  \n4. Caching Layer (redis-om, latest)  \n   4.1 Cache each query + results with TTL and Pydantic models.  \n\nDeliverables  \n\u2022 A requirements.txt listing the latest versions (as of 2025-05-06) of all third-party libraries, each released within the past 12 months.  \n\u2022 Complete, runnable Python 3.11+ code (\u2264150 lines) implementing all requirements.  \n\u2022 Clear inline comments indicating where and how each external library is used.","question_index":47,"modules":["starlite","llama-index","chromadb","redis-om","pydantic","httpx","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are a senior engineer building a high-throughput job processing microservice with a modern async Python stack.  \n\nFunctional Requirements:  \n1. Expose a POST \/jobs endpoint using the latest Starlite (as of 2025-05-06) to accept JSON job submissions.  \n2. Validate incoming JSON payloads with the latest Pydantic v2 models.  \n3. Persist job records to PostgreSQL via the latest prisma-client-py async ORM.  \n4. Enqueue and process jobs in the background using the latest Taskiq queue.  \n5. Expose a GET \/jobs\/{id}\/status endpoint to retrieve current job status.  \n6. Ensure all I\/O is non-blocking and use Python 3.11+ async\/await throughout.  \n\nDeliverables:  \n\u2022 A requirements list specifying the exact latest versions of Starlite, Pydantic v2, prisma-client-py, and Taskiq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ source code under 150 lines.  \n\u2022 Clear inline comments marking where each external library is used.","question_index":99,"modules":["starlite","pydantic","prisma-client-py","taskiq","asyncpg","uvicorn"],"modulenotfound":["prisma-client-py"],"modulenotfound_count":1,"module_count":6}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a secure, real-time AI-powered chat web service.\n\nFunctional Requirements:\n1. Expose an async REST API using the latest FastAPI for user registration and login endpoints.  \n2. Implement JWT authentication with rotating refresh tokens via the latest Authlib.  \n3. Persist users and chat messages in SQLite using the latest SQLModel, leveraging its new async support.  \n4. Enable real-time chat notifications over WebSockets using the latest python-socketio and ASGI.  \n5. Integrate AI chat completions using the latest OpenAI Python client in an async background task.\n\nDeliverables:\n1. A requirements.txt listing exact, up-to-date package versions as of today\u2019s date.  \n2. A single Python 3.11+ file (<150 lines) containing complete, runnable code with clear comments marking where each external library is used.","question_index":91,"modules":["fastapi","python-jose","authlib","sqlmodel","python-socketio","openai","uvicorn","bcrypt"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are a senior software engineer tasked with building an AI-powered collaborative document annotation API.\n\nFunctional Requirements:\n1. Secure user signup and login with JWT-based authentication using Starlite (latest) and Pydantic v2.\n2. REST endpoint to upload documents and persist metadata in Redis OM Python (latest).\n3. WebSocket route for real-time annotation broadcasting using Starlite\u2019s built-in WebSocket support.\n4. Background summarization of uploaded documents using llama-cpp-python (latest).\n\nDeliverables:\n- A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":1,"modules":["starlite","pydantic","redis-om","llama-cpp-python","python-jose","python-multipart","bcrypt"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are a senior engineer tasked with building a secure, high-throughput microservice for customer order ingestion, persistence, background processing, and caching.\n\nFunctional Requirements:\n1. Use FastAPI (latest as of 2025-05-06) with HTTP\/3 support to implement POST \/orders accepting JSON order data.\n2. Define an asynchronous Order model with SQLModel (latest) and connect to an async database engine (SQLite or PostgreSQL).\n3. Enqueue and process new orders via Arq (latest) background worker.\n4. Implement OAuth2 with JWT issuance and protected endpoints using Authlib (latest).\n5. Cache GET \/orders\/{id} responses in Redis using aiocache (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments marking where each external library is used.","question_index":68,"modules":["fastapi","sqlmodel","asyncpg","arq","authlib","aiocache","python-jose","python-multipart","redis","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build an end-to-end Python web service that accepts data submissions, runs ML inference asynchronously, and pushes results back in real time.\n\nFunctional Requirements:\n1. Implement a POST \/submit endpoint using the latest FastAPI (ASGI, HTTP\/2) to accept JSON payloads.\n2. Validate and persist submissions into PostgreSQL via the latest SQLModel with asynchronous sessions.\n3. Dispatch an asynchronous classification task via the latest Celery (Redis broker) when new data arrives.\n4. In the Celery task, perform inference using the latest Hugging Face Transformers pipeline (e.g., text-classification).\n5. Cache inference results in Redis using the latest aioredis with a 5-minute TTL.\n6. Expose a WebSocket \/ws endpoint (FastAPI) to stream cached or newly computed results to subscribed clients.\n7. Instrument all HTTP requests and Celery tasks with the latest OpenTelemetry SDK for tracing.\n\nDeliverables:\n\u2022 requirements.txt listing \u201cthe latest\u201d versions of FastAPI, SQLModel, Celery, Redis, aioredis, Transformers, and OpenTelemetry as of 2025-05-06.  \n\u2022 A single Python 3.11+ file under 150 lines of runnable code. Include clear comments where each external library is used.","question_index":59,"modules":["fastapi","sqlmodel","celery","redis","aioredis","transformers","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-celery","uvicorn","psycopg2-binary"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are tasked with building a real-time IoT analytics dashboard featuring anomaly detection, edge caching, vector search, and HTTP\/3 streaming.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of May 6, 2025) with HTTP\/3 and ASGI concurrency for API endpoints.\n2. Ingest IoT telemetry via real-time WebSocket streams using the latest websockets library.\n3. Perform transformer-based anomaly detection using the latest HuggingFace transformers and PyTorch 2.x.\n4. Store and query time-series embeddings in a vector database using the latest Pinecone Python client with HNSW indexing.\n5. Cache recent query results at the edge using the latest aioredis client with TTL invalidation.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the \u201clatest\u201d versions of all third-party libraries as of today.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":67,"modules":["fastapi","uvicorn","websockets","torch","transformers","pinecone-client","redis","numpy","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a high-performance, AI-powered web dashboard for real-time analytics and user collaboration.\n\nFunctional Requirements:\n1. Web API: Use the latest FastAPI (as of 2025-05-06) with async endpoints and the new dependency-injection system.\n2. Real-Time Collaboration: Implement bidirectional WebSocket chat using python-socketio v5.x+ for live user updates.\n3. AI Integration: Leverage the latest llama-cpp-python (or equivalent) to compute embeddings on incoming messages.\n4. Database Persistence: Store users and message history asynchronously with SQLModel v0.7+ (Pydantic v2) on SQLite.\n5. Caching Layer: Cache AI embedding results in Redis via aioredis v3.x+ to minimize redundant computation.\n6. Security: Protect all endpoints with OAuth2 JWT authentication using Authlib v1.2+.\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all third-party libraries as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, with clear inline comments indicating where each external library is used.","question_index":58,"modules":["fastapi","python-socketio","llama-cpp-python","sqlmodel","authlib","redis","uvicorn","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"You are a senior software engineer tasked with building a modern image-processing web service end-to-end.\n\nFunctional Requirements:\n1. Implement a REST endpoint in Litestar for image uploads that converts incoming images to AVIF using the latest pillow-avif-plugin.\n2. Asynchronously persist image metadata (filename, upload timestamp, AVIF URL, dimensions) in MongoDB via the latest Beanie ODM.\n3. Index and cache metadata in Redis for fast lookup using the latest Redis-OM Python library.\n4. Expose a GraphQL API via the latest Litestar-GraphQL plugin to query images with filtering, pagination, and type validation.\n5. Broadcast real-time upload notifications over WebSocket connections using Litestar\u2019s built-in WebSocket support.\n\nDeliverables:\n- A concise list of the five functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines, using the latest Litestar, Litestar-GraphQL, Beanie, Redis-OM, and pillow-avif-plugin (as of May 6, 2025), with clear comments indicating where each external library is used.","question_index":89,"modules":["beanie","litestar","litestar-graphql","motor","pillow","pillow-avif","redis-om","strawberry-graphql"],"modulenotfound":["litestar-graphql","pillow-avif"],"modulenotfound_count":2,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"You are a Senior Software Engineer building a high-throughput, real-time web application that ingests user events, enriches them with AI, persists outcomes, and serves analytics via a modern dashboard.\n\nFunctional Requirements:\n1. Implement JWT-based user authentication using the latest async web framework with schema-first validation (e.g., HTTP\/3 support).\n2. Expose a WebSocket endpoint for real-time event ingestion using the latest WebSocket library with per-message compression.\n3. Perform fully async CRUD on PostgreSQL using the latest ORM with native asyncio support and advanced connection pooling.\n4. Offload heavy computations to background workers via the latest task queue library supporting asyncio and result backends.\n5. Integrate an AI text-classification pipeline using the latest AI inference library with streaming outputs.\n6. Instrument the service with distributed tracing using the latest observability library supporting OpenTelemetry protocols.\n\nDeliverables:\n\u2022 requirements.txt listing the latest available versions of all dependencies (as of May 6, 2025).  \n\u2022 A single Python 3.11+ file under 150 lines that is complete and runnable, with clear comments marking where each external library is used.","question_index":12,"modules":["fastapi","python-jose","sqlalchemy","asyncpg","celery","redis","transformers","torch","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","python-multipart","pydantic","websockets","jwt"],"modulenotfound":[],"modulenotfound_count":0,"module_count":15}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Develop a high-performance asynchronous web service for ingesting user datasets, persisting metadata, launching background processing jobs, and tracing all operations end-to-end.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/upload endpoint using Starlite (latest) with async file streaming and dependency injection.\n2. Define and migrate a PostgreSQL table via Piccolo ORM (latest), storing upload metadata and task IDs, leveraging its GraphQL auto-schema feature.\n3. Enqueue and process tasks with Asynq (latest) in Redis, using priority queues, retry policies, and task result callbacks.\n4. Instrument incoming HTTP requests and background tasks with OpenTelemetry (latest), enabling W3C trace-context propagation across components.\n5. Provide an HTTP GET \/status\/{task_id} endpoint that queries Piccolo ORM for real-time task status.\n\nDeliverables:\n\u2022 The numbered functional requirements list (as above)  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, using the latest versions of all libraries as of 2025-05-06, with clear comments indicating where each external library is used.","question_index":43,"modules":["starlite","piccolo","asynq","opentelemetry-sdk","opentelemetry-instrumentation-starlite","opentelemetry-instrumentation-asynq","redis"],"modulenotfound":["opentelemetry-instrumentation-starlite","opentelemetry-instrumentation-asynq"],"modulenotfound_count":2,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"As a senior software engineer, you are tasked with building a secure, real-time collaborative whiteboard microservice using cutting-edge Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3 API using the latest FastAPI (as of 2025-05-06) and uvicorn[standard] for low-latency streaming.\n2. Implement OAuth2 authorization with JWTs and refresh-token rotation using the latest Authlib.\n3. Persist whiteboard sessions and user metadata in PostgreSQL via the latest SQLModel with an async engine.\n4. Broadcast drawing events in real time over WebSockets using the latest websockets library with room-based pub\/sub.\n5. Vectorize stroke data into SVG on the fly using the latest Shapely or pyvips for advanced geometry operations.\n6. Upload periodic session snapshots to AWS S3 using the latest aioboto3 with asynchronous multipart uploads.\n\nDeliverables:\n- A requirements.txt listing the latest available versions of all external libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, fully runnable, with clear comments indicating where each third-party library is used.","question_index":25,"modules":["fastapi","uvicorn","authlib","sqlmodel","websockets","shapely","aioboto3","python-jose","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a real-time sentiment analysis dashboard that fetches trending Twitter topics, analyzes sentiment, stores results, and serves updates via WebSockets.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to create both REST and WebSocket endpoints, leveraging its built-in asyncio and WebSocket broadcast features.\n2. Use the latest HTTPX to asynchronously fetch trending topics from the Twitter API v2 over HTTP\/3.\n3. Use the latest Hugging Face transformers to perform sentiment analysis via a pretrained pipeline.\n4. Use the latest SQLModel to define async SQLite models and persist topics with sentiment scores.\n5. Use the latest Jinja2 to render a minimal HTML dashboard for initial page load before WebSocket updates.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the latest versions of FastAPI, HTTPX, transformers, SQLModel, and Jinja2 as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":36,"modules":["fastapi","httpx","transformers","sqlmodel","jinja2","uvicorn","python-dotenv"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Senior Software Engineer: You are developing an intelligent IoT monitoring dashboard that integrates real-time ingestion, streaming anomaly detection, WebSocket updates, dynamic UI rendering, and cloud infrastructure provisioning.\n\nFunctional Requirements:\n1. Build an ASGI web server using the latest ASGI framework (as of 2025-05-06) with HTTP\/2 and WebSocket endpoints for real-time data streaming.\n2. Connect asynchronously to an MQTT broker using the latest Python MQTT client library (as of 2025-05-06), subscribe to sensor topics, and relay messages to the server.\n3. Implement real-time anomaly detection on incoming sensor data using the latest Python anomaly detection library supporting incremental or deep learning.\n4. Serve a live, interactive front-end auto-generated from Python to JavaScript using the latest Python-to-JS UI framework.\n5. Provision and deploy all components via infrastructure-as-code using the latest Python IaC SDK on a major cloud provider.\n\nDeliverables:\n\u2022 Restate the numbered functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code integrating all five requirements, under 150 lines.  \n\u2022 Clear inline comments indicating where and how each external \u201clatest\u201d library (as of 2025-05-06) is imported and used.","question_index":49,"modules":["litestar","asyncmqtt","rapids","pywebui","pulumi","pulumi-aws","uvicorn"],"modulenotfound":["asyncmqtt"],"modulenotfound_count":1,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a real-time collaborative note-taking web application with authentication, persistence, caching, live updates, and scheduled cleanup.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (as of today) for note creation, retrieval, update, and deletion.\n2. Secure endpoints with JWT authentication using the latest PyJWT.\n3. Persist notes in a relational database via the latest SQLModel ORM, defining Note and User models.\n4. Enable real-time note synchronization between clients using WebSockets with the latest python-socketio.\n5. Cache frequently accessed note metadata in Redis using the latest redis-om to reduce DB load.\n6. Schedule automatic archiving of notes older than 30 days using the latest APScheduler.\n7. Document all endpoints with OpenAPI and include example request\/response schemas.\n\nDeliverables:\n- A numbered list of all third-party libraries (with versions) you\u2019ve chosen.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear comments indicating where each external library is used.","question_index":69,"modules":["fastapi","pyjwt","sqlmodel","python-socketio","redis-om","apscheduler","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a high-performance asynchronous message processing service for real-time sentiment analysis, storage, notification, and search.\n\nFunctional Requirements:\n1. Implement an async HTTP API using the latest FastAPI (as of May 6, 2025) to accept user messages.\n2. Define and persist message models in PostgreSQL via the latest SQLModel ORM.\n3. Schedule and execute sentiment analysis in the background using the latest Dramatiq with RabbitMQ.\n4. After analysis, update the database and push real-time notifications to connected clients over WebSockets.\n5. Index messages and sentiment scores into Elasticsearch using the latest official Python client.\n6. Provide a search endpoint to query stored messages by keyword and sentiment.\n\nDeliverables:\n\u2022 A numbered list of the \u201clatest\u201d third-party libraries (with versions) used for each sub-domain.  \n\u2022 Complete, runnable Python 3.11+ application code under 150 lines, with clear comments indicating where each external library is used.","question_index":48,"modules":["fastapi","sqlmodel","dramatiq","elasticsearch","websockets","psycopg2-binary","textblob","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a microservice that accepts text input, asynchronously generates summaries, persists data, and provides real-time metrics.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI 3, HTTP\/3 support) to implement POST \/summaries accepting JSON { \"text\": \"\u2026\" }, enqueue a background job via Arq, and return a job ID.\n2. Persist requests and summaries in SQLite asynchronously with the latest SQLModel (Pydantic V2 model support).\n3. Offload summarization jobs to Redis using the latest Arq, invoking the latest LangChain to call OpenAI for a concise summary.\n4. Expose GET \/summaries\/{id} to retrieve stored summaries from the database.\n5. Instrument request counts and job durations with the latest prometheus_client, exposing metrics on GET \/metrics.\n\nDeliverables:\n- A bulleted list of chosen \u201clatest\u201d libraries with exact version numbers.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear inline comments indicating where each external library is used.\n- Self-contained implementation: after installing dependencies, code runs without further setup.","question_index":95,"modules":["starlite","sqlmodel","arq","langchain","prometheus_client","openai","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Create a serverless real-time analytics API that ingests WebSocket events, performs on-the-fly NLP summarization, indexes embeddings, exposes a GraphQL endpoint, and deploys to AWS Lambda.\n\nFunctional Requirements:\n1. Use the latest Litestar (as of 2025-05-06) to build an HTTP\/2 GraphQL endpoint with schema-based typing.\n2. Implement a real-time WebSocket consumer using wsproto or litestar-websockets for event ingestion.\n3. Offload text summarization to an asynchronous background task leveraging the latest Transformers library.\n4. Generate embeddings with the newest OpenAI Embeddings API or sentence-transformers and store them in the latest ChromaDB vector store.\n5. Secure all endpoints with passwordless authentication via the latest fastapi-users or equivalent.\n6. Package and deploy the entire application to AWS Lambda using the latest Mangum adapter.\n\nDeliverables:\n- A numbered list of chosen \u201clatest\u201d library versions (as of 2025-05-06) mapping to each requirement.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":77,"modules":["litestar","mangum","transformers","sentence-transformers","chromadb","strawberry-graphql"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Develop a modern, high-performance AI-powered web service for real-time text processing.\n\nFunctional Requirements:\n1. Build a JSON REST API with Litestar using its latest dependency-injection and ASGI lifespan hooks.\n2. Integrate Tortoise ORM (async) to manage a PostgreSQL \u201cjobs\u201d table with migrations.\n3. Use llama-cpp-python to generate text embeddings or responses via a local LLaMA model.\n4. Expose a WebSocket endpoint with python-socketio (asyncio mode) for live client notifications.\n5. Cache recent embeddings in Redis using redis-py\u2019s asyncio client.\n6. Schedule background tasks (e.g., stale job cleanup) leveraging asyncio\u2019s latest task groups.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":60,"modules":["litestar","tortoise-orm","llama-cpp-python","python-socketio","redis","aerich","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are a senior software engineer building a real-time social media monitoring application that ingests tweets, analyzes sentiment, classifies images, displays live charts, and deploys as a serverless microservice using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Ingest tweets in real time from Twitter API v2 using the latest HTTPX async streaming client.\n2. Perform NLP sentiment analysis on tweet text with the latest Hugging Face Transformers pipeline.\n3. Classify embedded images using the latest Ultralytics YOLOv8 model.\n4. Render an interactive, live-updating dashboard using the latest Plotly Dash framework.\n5. Scaffold serverless deployment with the latest AWS Lambda Powertools for Python.\n\nDeliverables:\n- A requirements list of all external dependencies.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":98,"modules":["httpx","transformers","torch","ultralytics","dash","plotly","aws-lambda-powertools"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: A knowledge management platform must allow users to upload PDFs, index their content, and perform real-time similarity searches via a high-performance API.\n\nFunctional Requirements:\n1. Implement a PDF upload REST endpoint using the latest unstructured-sdk to extract text.\n2. Use the latest ChromaDB Python client to generate and store embeddings for each document.\n3. Provide a search REST endpoint that queries ChromaDB for top-k similar documents given a text query.\n4. Create a real-time WebSocket over HTTP\/3 endpoint using the latest aioquic to stream incremental search results.\n5. Assemble the application as an asynchronous service using the latest Starlite web framework.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":6,"modules":["starlite","chromadb","unstructured","aioquic","pydantic","python-multipart","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: As a senior software engineer, build an asyncio-based Python microservice showcasing HTTP\/3 REST, GraphQL, async ORM, real-time WebSockets, and scheduled tasks using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST endpoint at `\/api\/items` using FastAPI (latest) and Hypercorn (latest) with HTTP\/3 support.\n2. Expose a GraphQL API at `\/graphql` using Strawberry (latest) with asynchronous resolvers fetching from the database.\n3. Integrate PostgreSQL via Tortoise ORM (latest), define an `Item` model, and apply programmatic migrations on startup.\n4. Provide WebSocket notifications at `\/ws\/updates` using python-socketio (latest), broadcasting item creation and deletion events.\n5. Schedule an hourly cleanup job with APScheduler (latest) in asyncio mode to remove `Item` records older than 7 days.\n\nDeliverables:\n- A `requirements.txt` listing the latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is imported and utilized.","question_index":66,"modules":["fastapi","hypercorn","strawberry-graphql","tortoise-orm","python-socketio","apscheduler","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Develop a real-time AI-driven sentiment analysis web service that ingests text, processes it via an on-device LLM, persists results, and streams live analytics to clients.\n\nFunctional Requirements:\n1. Expose RESTful and WebSocket endpoints using the latest Litestar Python framework (released within the last 12 months).\n2. Perform local LLM inference for sentiment analysis with the latest llama-cpp-python library (released within the last 12 months).\n3. Asynchronously persist raw text and sentiment scores in MongoDB using the latest async MongoDB driver (released within the last 12 months) with code-first schema.\n4. Offload sentiment analysis to a distributed background worker using the latest Dramatiq (or equivalent) async task queue released in the last 12 months.\n5. Broadcast real-time sentiment updates to connected clients over WebSockets.\n6. Render an interactive dashboard in server-side templates with the latest Plotly Python library (released within the last 12 months).\n7. Instrument the entire application with distributed tracing via the latest OpenTelemetry Python SDK (released within the last 12 months).\n\nDeliverables:\n- A requirements list specifying each third-party library and its exact latest version as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines total.\n- Clear inline comments indicating where and how each external library is used.","question_index":5,"modules":["litestar","llama-cpp-python","motor","dramatiq","opentelemetry-instrumentation-litestar","plotly","pydantic"],"modulenotfound":["opentelemetry-instrumentation-litestar"],"modulenotfound_count":1,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"You are a senior software engineer tasked with building a real-time analytics dashboard with AI-driven anomaly alerts using the latest Python libraries as of 2025-05-06.  \n\nFunctional Requirements:  \n1. Implement asynchronous REST API endpoints using the latest FastAPI.  \n2. Add WebSocket-based real-time alert streaming using the latest websockets.  \n3. Persist incoming metrics in PostgreSQL via async ORM operations using the latest SQLModel.  \n4. Cache expensive queries in Redis for high throughput using the latest aioredis.  \n5. Perform streaming anomaly detection on incoming metrics using the latest river library.  \n6. Enable file uploads of detailed reports to S3-compatible storage via the latest minio SDK.  \n\nDeliverables:  \n\u2022 A numbered list of required Python libraries (with versions as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":20,"modules":["fastapi","websockets","sqlmodel","aioredis","river","minio","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Build a real-time analytics microservice that lets authenticated users ingest sales data, stores it in PostgreSQL, streams live updates to connected clients, and generates PDF reports with embedded charts.\n\n1. Implement OAuth2 JWT authentication using the latest fastapi-jwt-auth as of May 6, 2025.  \n2. Create async CRUD endpoints for sales records with SQLModel and asyncpg.  \n3. Broadcast new sales events over WebSocket using python-socketio (latest).  \n4. Generate interactive charts in HTML via Plotly (latest) and convert them to PDF using WeasyPrint (latest).  \n5. Expose generated PDFs via an endpoint, serving files asynchronously with aiofiles.  \n\nDeliverables:  \n- A numbered confirmation of the functional requirements above.  \n- A single Python 3.11+ file under 150 lines\u2014complete, runnable code\u2014with clear comments indicating where each external library is used. Use the latest versions of all third-party libraries as of today\u2019s date.","question_index":26,"modules":["fastapi","sqlmodel","databases","python-socketio","plotly","weasyprint","aiofiles","pyjwt","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are tasked with building a real-time collaborative task management web service for enterprise teams.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to expose HTTP endpoints and WebSocket routes, leveraging its new dependency injection and WebSocket lifetime management.\n2. Configure an async PostgreSQL database using the latest Tortoise-ORM with its auto-migration CLI and Pydantic model integration.\n3. Broadcast real-time task updates to connected clients via the latest python-socketio in async mode.\n4. Implement OAuth2 authentication with JWT access and refresh tokens using the latest Authlib, including token rotation.\n5. Cache sessions and publish update events using the latest aioredis async Redis client.\n6. Schedule and execute background jobs (e.g., daily summaries) with the latest APScheduler async support.\n7. Instrument request tracing and logging using the latest OpenTelemetry Python SDK, exporting to console.\n8. Auto-generate interactive API docs using FastAPI\u2019s built-in Swagger UI and Redoc.\n\nDeliverables:\n\u2022 A bullet list of selected libraries (with version numbers).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":15,"modules":["fastapi","python-socketio","tortoise-orm","authlib","redis","apscheduler","opentelemetry-sdk","uvicorn","asyncpg","pyjwt"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are building a high-performance Python backend that ingests user articles, generates AI-powered summaries, stores and caches results, and exposes them via REST, GraphQL, and real-time channels with background processing.\n\nFunctional Requirements:\n1. Accept article submissions over a REST API using FastAPI and validate payloads with Pydantic v2.  \n2. Offload summarization to a background worker with Dramatiq using Redis as a broker.  \n3. Summarize articles via the latest OpenAI Python SDK and cache summaries with aiocache.  \n4. Persist articles and summaries in SQLite using SQLModel\u2019s async ORM features.  \n5. Expose stored summaries through a GraphQL endpoint implemented with Strawberry GraphQL.  \n6. Broadcast new summary notifications to connected clients over WebSockets with python-socketio.  \n\nDeliverables:\n\u2022 List of the latest third-party libraries (as of today) and their install commands.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":63,"modules":["fastapi","pydantic","dramatiq","openai","aiocache","sqlmodel","strawberry-graphql","python-socketio","redis","aiosqlite","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Develop an internal Python microservice where authenticated users can submit text to be summarized by an LLM and receive real-time notifications via WebSockets.\n\nFunctional Requirements:\n1. Implement an asynchronous REST API using the latest Starlite framework (2025-05-06) leveraging its dependency injection and lifespan event system.\n2. Integrate OAuth2 Authorization Code Flow for user authentication with Authlib\u2019s async client libraries released in the last 12 months.\n3. Persist users, submissions, and summaries in PostgreSQL using the latest Piccolo ORM with async schema migrations and connection pooling.\n4. Summarize submitted text by calling OpenAI\u2019s GPT-4 Turbo via the newest openai Python SDK and its function-calling feature.\n5. Broadcast summary completion events over WebSockets using a modern Python websockets library (with HTTP\/2 support) released in the past year.\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":82,"modules":["starlite","authlib","piccolo","openai","websockets","python-jose","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"You are tasked with building a real-time analytics web dashboard that ingests streaming data, processes it, summarizes it with AI, secures user access, and visualizes results dynamically using the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Implement a REST API in Python 3.11+ with FastAPI (HTTP\/3 support via Hypercorn) to accept JSON event streams.\n2. Broadcast processed insights in real time over WebSocket using python-socketio.\n3. Buffer incoming events and generate streaming AI summaries using the latest LangChain and OpenAI Python SDK.\n4. Perform real-time aggregation and lazy analytics on events with Polars.\n5. Produce dynamic Plotly charts of key metrics and serve them as JSON for front-end rendering.\n6. Secure all endpoints and WebSocket connections with JWT-based auth via fastapi-users.\n\nDeliverables:\n\u2022 A numbered list of the above functional requirements.  \n\u2022 A single Python 3.11+ file (under 150 lines) that fulfills all requirements, complete and runnable. Include clear comments indicating where and how each external library is used.","question_index":84,"modules":["fastapi","fastapi-users","hypercorn","langchain","polars","plotly","python-socketio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are a senior software engineer tasked with building a next-generation real-time collaborative data analytics dashboard. Use the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a FastAPI server with a WebSocket endpoint for live data updates using the latest fastapi[full].\n2. Expose a type-safe GraphQL API at `\/graphql` using Strawberry with async schema stitching.\n3. Integrate AI-driven insights by generating embeddings on incoming data with llama-cpp-python.\n4. Perform vector similarity search over embeddings via the Weaviate Python client\u2019s async API.\n5. Secure all endpoints with passwordless authentication using py_webauthn for WebAuthn flows.\n6. Containerize the application using docker-py to build images and dynamically generate a docker-compose config.\n7. Instrument the app with async Prometheus metrics using prometheus-client.\n\nDeliverables:\n- A list of required libraries (with versions) using the latest releases as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":72,"modules":["fastapi","strawberry-graphql","llama-cpp-python","weaviate-client","py-webauthn","docker","prometheus-client","uvicorn","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Develop a real-time collaborative note-taking web application with AI-powered summarization.\n\nFunctional Requirements:\n1. Implement user authentication via OAuth2 password flow using the latest FastAPI features for secure login.  \n2. Expose a WebSocket endpoint for real-time CRDT-based document state synchronization using the newest FastAPI WebSocket enhancements.  \n3. Persist users and documents in PostgreSQL via the latest Piccolo ORM async API with JSONB indexing for rich querying.  \n4. Cache live presence and session metadata in Redis using aioredis with RedisJSON support for low-latency reads\/writes.  \n5. Summarize document edits on demand using the latest LangChain library and ChatOpenAI (GPT-4) integration.  \n6. Serve a minimal HTML frontend via Jinja2 templates that connects to the WebSocket and displays live updates.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of each dependency as of 2025-05-06.  \n\u2022 main.py: Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":52,"modules":["fastapi","python-jose","piccolo","aioredis","rejson","langchain","openai","jinja2","uvicorn","websockets","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Senior Software Engineer, you are tasked with building a prototype real-time collaborative note-taking service using only the latest Python libraries released in the last 12 months.\n\nFunctional Requirements:\n1. Develop an asynchronous REST API for note CRUD operations using the latest Litestar framework.\n2. Persist notes in a SQL database via a newly released async ORM library.\n3. Cache frequent note queries in Redis with a recently published async Redis client.\n4. Enable real-time collaboration via WebSockets using a modern Python WebSocket library launched within the last year.\n5. Expose an AI-powered summarization endpoint using the latest OpenAI Python client.\n6. Render an interactive HTML dashboard showing active collaborators with a new Python-to-JS visualization tool.\n7. Instrument distributed tracing and structured logging using a cutting-edge observability library released in the past 12 months.\n\nDeliverables:\n- requirements.txt listing only libraries created in the last 12 months with exact versions.\n- A single Python 3.11+ file under 150 lines containing the complete, runnable application, with clear comments indicating where each external library is used.","question_index":41,"modules":["litestar","edgedb","redis-om","websockets","openai","pywebio","opentelemetry-api","structlog","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a lightweight feedback API that records user comments, analyzes sentiment in real time, stores results, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (as of 2025-05-06) to implement asynchronous REST endpoints for submitting feedback and querying stored analytics.\n2. Use the latest SQLModel to define async ORM models and persist incoming feedback in an SQLite database.\n3. Use the latest PyWebSocketX to broadcast each new sentiment result over an ASGI WebSocket endpoint.\n4. Use the latest VulcanMind inference library to perform on-the-fly sentiment analysis of submitted feedback.\n5. Implement structured logging and error handling throughout the request, inference, database, and WebSocket flows.\n\nDeliverables:\n- A requirements list with exact version pins for Starlite, SQLModel, PyWebSocketX, and VulcanMind (latest as of 2025-05-06).\n- Complete, runnable Python 3.11+ code (\u2264150 lines) that integrates all four libraries with clear comments marking where each is used.","question_index":83,"modules":["starlette","fastapi","sqlmodel","websockets","transformers","python-multipart","loguru","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"You are a senior software engineer building a high-performance, real-time analytics web service for IoT sensor data.  \n\nFunctional Requirements:  \n1. Expose HTTP REST ingest endpoints using the latest Nebula framework (released 2024-06) with its async dependency-injection.  \n2. Persist incoming readings to PostgreSQL via StarDB ORM (v1.0+, released 2024-08), leveraging async transactions and auto-migration.  \n3. Secure all endpoints with OAuth2 PKCE flows using AuthFusion (released 2024-12).  \n4. Broadcast processed data updates to dashboard clients via WaveSocket (released 2024-10) server-clustering.  \n5. Schedule hourly aggregation jobs using ChronoTask (released 2024-11) cron-style scheduler.  \n\nDeliverables:  \n\u2022 A requirements.txt listing the latest versions of all libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":92,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are a senior software engineer building a high-throughput chat summarization microservice in Python 3.11+ using only the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Real-time WebSocket ingestion: implement a `\/ws` endpoint using fastapi-socketio (latest) with session storage in Redis via `redis.asyncio`.\n2. AI summarization: enqueue incoming messages in Arq (latest) and process them with the OpenAI Python SDK (latest) calling GPT-4 Turbo for summaries.\n3. Async data persistence: store raw messages and summaries in PostgreSQL using SQLModel + encode\/databases with the asyncpg driver.\n4. Dynamic admin dashboard: expose `\/admin` rendering Jinja2 templates enhanced with HTMX via fastapi-htmx (latest).\n5. Email alerts: send summary notifications asynchronously using FastAPI-Mail (latest).\n6. Containerization: provide a multi-stage Dockerfile based on `python:3.11-slim`.\n\nDeliverables:\n- A `requirements.txt` listing the exact versions of all third-party libraries (as of May 6, 2025).\n- A single `main.py` (under 150 lines) containing complete, runnable Python 3.11+ code with clear comments indicating where and how each external library is used.","question_index":23,"modules":["fastapi","fastapi-socketio","fastapi-htmx","python-socketio","redis","arq","openai","sqlmodel","databases","fastapi-mail","jinja2","python-dotenv","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":13}
{"model":"claude-3-5-sonnet-20241022","question":"You are tasked with building a prototype web service for authenticated semantic search with real-time visual feedback.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) server with:\n   a. an OAuth2 login flow using PyOIDC (latest) against an external Identity Provider  \n   b. a POST \/query endpoint to accept natural-language queries  \n   c. a WebSocket \/ws endpoint to stream real-time results  \n2. Upon authentication, embed incoming queries using sentence-transformers (latest) and perform vector similarity search in ChromaDB (latest).  \n3. As results arrive, push them incrementally over the WebSocket to clients.  \n4. Generate a Plotly (latest) bar chart of top-k result scores and serve it as an HTML snippet.  \n5. Ensure all components run under Python 3.11+, integrate only \u201cthe latest\u201d library releases as of 2025-05-06, and keep implementation concise.  \n\nDeliverables:\n\u2022 A list of chosen libraries with exact versions (the \u201crequirements\u201d).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":42,"modules":["fastapi","pyoidc","sentence-transformers","chromadb","plotly","websockets","uvicorn","python-multipart","python-jose"],"modulenotfound":["pyoidc"],"modulenotfound_count":1,"module_count":9}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are building a high-throughput, real-time analytics web application for processing IoT sensor streams using the latest Python async frameworks and libraries.\n\nFunctional Requirements:\n1. Ingest JSON sensor events via a REST API built on FastAPI (use its latest HTTP\/3 support).\n2. Broadcast processed metrics to connected clients over WebSockets using python-socketio (asyncio).\n3. Offload CPU-intensive aggregation to background workers with Arq (latest Redis-backed queue).\n4. Persist time-series summaries in PostgreSQL via SQLModel (Pydantic v2-powered ORM).\n5. Cache hot query results in Redis using aioredis for sub-millisecond lookups.\n6. Expose tracing and Prometheus metrics using OpenTelemetry and prometheus-client.\n\nDeliverables:\n- A bullet list of the latest library dependencies as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments indicating where each external library is used.","question_index":40,"modules":["fastapi","uvicorn","python-socketio","arq","sqlmodel","aioredis","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","prometheus-client","psycopg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-5-sonnet-20241022","question":"You are a senior software engineer tasked with building a Python web application that integrates user authentication, real-time notifications, AI-based recommendations, and payment processing.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (2025-05-06) and Python 3.11 features (e.g. match-case).\n2. Secure all endpoints with OAuth2 JWT authentication via the latest fastapi-users, including email verification flows.\n3. Provide real-time notifications over WebSockets using the latest python-socketio async server.\n4. Handle one-time charges using the latest Stripe Python SDK via the Payment Intents API.\n5. Generate AI-driven product recommendations by invoking gpt-4-turbo through the latest openai library.\n6. Emit structured, context-rich logs for each operation using the latest structlog.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06  \n\u2022 Complete, runnable Python 3.11+ code (under 150 lines) with clear comments marking where each external library is used","question_index":96,"modules":["fastapi","fastapi-users","python-socketio","stripe","openai","structlog","uvicorn","python-jose"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: As a senior software engineer, design a high-performance microservice for real-time document collaboration with secure access, persistence, and cloud integration.  \n\nFunctional Requirements:  \n1. Implement an asynchronous JSON REST API using the latest Python web framework available as of 2025-05-06, supporting pagination and input validation.  \n2. Secure all endpoints with OAuth2 PKCE and JWT using the latest authentication library (released within the last 12 months).  \n3. Enable bidirectional real-time collaboration events over WebSocket using the latest WebSocket library (released within the last 12 months).  \n4. Persist document data in an async NoSQL database (e.g., MongoDB) using the newest driver supporting transactions and change streams.  \n5. Upload and serve document attachments to an S3-compatible object storage using the latest SDK with multipart upload support.  \n6. Cache frequently accessed documents in Redis with TTL and automatic invalidation using the newest Redis client library.  \n\nDeliverables:  \n- A requirements.txt listing \u201cthe latest\u201d versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":80,"modules":["fastapi","motor","redis","boto3","pyjwt","websockets","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a high-performance Python microservice that combines cutting-edge web, real-time, AI and observability features into a single ASGI application.\n\nFunctional Requirements:\n1. HTTP\/3 ASGI server with automatic TLS and QUIC support using the latest HTTP\/3 library released within the past 12 months.  \n2. Real-time WebSocket endpoint that streams JSON events with zero-copy backpressure control using the newest streaming library created in the last year.  \n3. Semantic search pipeline: accept text uploads, generate embeddings, and perform vector similarity queries via the latest vector-store client released within 12 months.  \n4. On-device LLM inference endpoint that consumes streamed text segments and returns summaries using the most recent quantized model library published this year.  \n5. Distributed tracing and metrics export using the newest OpenTelemetry OTLP exporter released within the past 12 months.\n\nDeliverables:\n- A requirements list (requirements.txt) specifying the exact, latest versions of each third-party library as of 2025-05-06.  \n- A single Python 3.11+ script under 150 lines that implements all features above, with clear comments marking where each external library is used.","question_index":51,"modules":["hypercorn","fastapi","websockets","qdrant-client","sentence-transformers","ctransformers","opentelemetry-api","opentelemetry-sdk","opentelemetry-exporter-otlp"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario  \nDevelop an end-to-end Python web service that provides passwordless WebAuthn authentication, stores and summarizes text inputs via OpenAI, broadcasts real-time summary events, and serves interactive charts.\n\nFunctional Requirements  \n1. Implement passwordless registration and login using the latest \u201cwebauthn\u201d library (FIDO2) with Litestar middleware.  \n2. Create REST endpoints under Litestar (latest version) for submitting text to summarize and retrieving stored summaries.  \n3. Use the latest SQLModel with an async PostgreSQL engine to persist input texts, summaries, timestamps, and user IDs.  \n4. Invoke the latest \u201copenai\u201d Python client to perform GPT-4 Turbo summarization with function-calling.  \n5. Serialize and broadcast new summaries over WebSockets using the latest \u201cmsgspec\u201d for high-performance JSON.  \n6. Serve an HTML page at \u201c\/charts\u201d that embeds an interactive Plotly chart of summary lengths over time.\n\nDeliverables  \n\u2022 A numbered list of all external libraries (with exact latest versions as of 2025-05-06) and their roles.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":13,"modules":["litestar","webauthn","sqlmodel","asyncpg","openai","msgspec","plotly","python-jose","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: As a senior software engineer, you must develop a real-time analytics dashboard backend that requires user authentication, data ingestion, live updates, background enrichment via an external API, and a web-based dashboard.\n\nFunctional Requirements:\n1. Implement user registration and login endpoints using JWT authentication with asymmetric keys via FastAPI-JWT-Auth (latest).\n2. Create a POST \/data endpoint to ingest JSON payloads and store them asynchronously in SQLite using SQLModel (latest).\n3. Broadcast each new record to connected WebSocket clients at \/ws using the latest websockets library, leveraging async broadcast.\n4. Schedule a background job every minute with arq (latest) to enrich stored records by streaming JSON from a third-party API via httpx (latest).\n5. Serve a simple HTML dashboard at GET \/dashboard that opens a WebSocket to \/ws and displays incoming records in real time.\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":75,"modules":["fastapi","fastapi-jwt-auth","sqlmodel","arq","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Design and implement a Python microservice that exposes HTTP endpoints, real-time WebSocket notifications, persistent storage, caching, scheduled tasks, and email alerts using the latest third-party libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use FastAPI (latest) with HTTP\/2 support to expose async REST endpoints for user registration and notification retrieval.\n2. Use SQLModel (latest) as an async ORM for PostgreSQL, defining typed models for users and notifications.\n3. Implement WebSocket broadcasting via FastAPI\u2019s WebSocket support to push new notifications to connected clients.\n4. Leverage Aioredis (latest) Redis Streams to cache session state and stream notification events.\n5. Schedule periodic database cleanup and cache eviction using APScheduler (latest) with its asyncio integration.\n6. Send email alerts for critical notifications using Aiosmtplib (latest) with STARTTLS.\n7. Perform external health-check API calls using HTTPX (latest) with async retries and timeouts.\n\nDeliverables:\n- A requirements list naming each library and its latest version as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, including clear comments that indicate where each external library is used.","question_index":44,"modules":["fastapi","sqlmodel","websockets","redis","apscheduler","aiosmtplib","httpx","uvicorn","asyncpg","python-jose"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are tasked as a senior software engineer to build an end-to-end collaborative document editing service with real-time updates, async persistence, GraphQL subscriptions, and cloud file storage.\n\nFunctional Requirements:\n1. Use the latest Litestar (first released within the last 12 months) to create an HTTP\/2 web server with WebSocket endpoints for real-time edit streams.\n2. Implement a GraphQL API with Strawberry GraphQL (latest) that supports queries, mutations, and live subscriptions for document change events.\n3. Persist documents in a PostgreSQL database via a fully async ORM of your choice that was first released in the last 12 months and supports automated migrations.\n4. Generate AWS S3 presigned URLs for resumable file attachments using aiobotocore (latest) with async upload\/download.\n5. Schedule periodic cleanup of stale sessions using an async task scheduler library first released within the last 12 months.\n\nDeliverables:\n- A brief list of the chosen third-party libraries (with exact versions) that meet the \u201cfirst released within the last 12 months\u201d requirement.\n- Complete, runnable Python 3.11+ code (under 150 lines) fulfilling the above requirements, with clear comments indicating where and how each external library is used.","question_index":78,"modules":["litestar","strawberry-graphql","aiobotocore","rocketry","sqlalchemy","asyncpg","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario:\nYou are a senior software engineer tasked with building a real-time AI summarization microservice.\n\nFunctional Requirements:\n1. Implement an async RESTful POST endpoint `\/summarize` that accepts JSON `{ \"text\": \"<string>\" }` and returns a summary generated via `llama-cpp-python`.\n2. Implement a WebSocket endpoint `\/stream` that streams incremental summary tokens to the client as they are produced.\n3. Persist each original text and its summary in an async SQLite database using `SQLModel`.\n4. Cache recent summaries in Redis for 10 minutes with `aioredis` to avoid redundant computation.\n5. Expose Prometheus-compatible metrics on `\/metrics` (request counts, latency histograms, cache hit\/miss) via `prometheus-client`.\n6. Use a background task to warm up the Llama model at startup and to periodically clean up expired cache entries.\n\nDeliverables:\n- A restatement of the functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Code must use the latest versions of FastAPI, SQLModel, aioredis, llama-cpp-python, and prometheus-client as of 2025-05-06.\n- Include clear comments indicating where each external library is used.","question_index":54,"modules":["fastapi","sqlmodel","llama-cpp-python","prometheus-client","redis","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-5-sonnet-20241022","question":"You are a senior software engineer tasked with building a high-throughput real-time sentiment analysis microservice.\n\nFunctional Requirements:\n1. Use FastAPI (latest) to expose a POST \/analyze endpoint that accepts JSON text payloads and stores request metadata in PostgreSQL via SQLModel (latest).\n2. Secure all endpoints with OAuth2 JWT authentication using Authlib (latest), leveraging its PKCE support.\n3. Perform real-time sentiment analysis on incoming text using the latest Hugging Face Transformers pipeline with GPU acceleration via PyTorch (latest) and cache results in Redis using aiocache (latest).\n4. Broadcast live sentiment scores to connected clients over WebSockets using python-socketio (latest) and run the server on Uvicorn with HTTP\/3 support.\n5. Asynchronously upload any user-submitted audio snippets to S3-compatible storage via aiobotocore (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all external libraries as of 2025-05-06  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":62,"modules":["fastapi","sqlmodel","authlib","transformers","torch","aiocache","python-socketio","uvicorn","aiobotocore","python-jose","redis","asyncpg","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":13}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a high-performance Python web application combining HTTP\/3 REST, GraphQL, real-time WebSockets, async database operations, data analytics, AI summarization, and distributed tracing.\n\nFunctional Requirements:\n1. HTTP\/3 REST API: use FastAPI (latest as of 2025-05-06) with Uvicorn HTTP\/3 support to serve \/items endpoints.\n2. Async GraphQL API: use Strawberry (latest as of 2025-05-06) to expose a GraphQL schema with DataLoader for batch fetching.\n3. Async DB access: use SQLModel (latest as of 2025-05-06) with an async SQLAlchemy engine to perform PostgreSQL CRUD.\n4. Real-time WebSockets: use websockets library (latest as of 2025-05-06) to broadcast item updates at \/ws.\n5. Data analytics: use Polars (latest as of 2025-05-06) lazy API to compute summary statistics on items.\n6. AI summarization: use OpenAI Python SDK (latest as of 2025-05-06) to generate summaries of item descriptions.\n7. Observability: use OpenTelemetry Python SDK (latest as of 2025-05-06) to instrument all endpoints for distributed tracing.\n\nDeliverables:\n- requirements.txt listing exact latest versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments marking where each external library is used.","question_index":53,"modules":["fastapi","uvicorn","strawberry-graphql","sqlmodel","websockets","polars","openai","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are a senior software engineer building a real-time sentiment and OCR-enhanced geospatial chat dashboard for remote field teams.\n\nFunctional Requirements:\n1. Use FastAPI (latest version as of 2025-05-06) to implement an async REST API that serves a minimal HTML client.\n2. Implement bi-directional real-time chat via Socket.IO using python-socketio (latest) on the FastAPI server.\n3. Perform on-the-fly sentiment analysis of text messages using Hugging Face\u2019s transformers pipeline (latest) and broadcast sentiment scores.\n4. Accept image uploads from clients, extract embedded text with EasyOCR (latest), and inject the OCR result into the chat stream.\n5. Plot message geolocation metadata on a Folium (latest) map and serve it as an embeddable HTML snippet.\n\nDeliverables:\n- A requirements list enumerating the exact \u201clatest\u201d third-party libraries and versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":2,"modules":["fastapi","python-socketio","transformers","easyocr","folium","uvicorn","python-multipart","aiofiles","jinja2"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Develop a Python microservice that supports AI-driven product imagery, real-time stock updates, secure user authentication, and checkout processing.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) app with JWT-based auth using Authlib (latest) and secure password hashing.  \n2. Define async ORM models for Users and Products with SQLModel (latest) on PostgreSQL, including migration setup.  \n3. On product creation, generate an AI image via Diffusers (latest) or Stability-SDK (latest) and expose it via a REST endpoint.  \n4. Provide a WebSocket endpoint for real-time inventory updates using Starlette\u2019s WebSocket support (latest).  \n5. Create a \/checkout POST endpoint integrating Stripe\u2019s Python SDK (latest) with webhook handling to confirm payments.\n\nDeliverables:\n\u2022 A bullet list of the \u201clatest\u201d libraries (with pip install specifiers as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":85,"modules":["fastapi","authlib","sqlmodel","diffusers","stripe","alembic","asyncpg","python-jose","passlib","pydantic","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are developing a high-performance, real-time AI-powered web service for personalized content recommendations.\n\nFunctional Requirements:\n1. Implement a RESTful POST \/recommend endpoint using the latest Python web framework released within the last 12 months, accepting JSON input and returning JSON responses.\n2. Add a WebSocket \/ws endpoint for streaming live recommendation updates using the newest asyncio-based WebSocket library from the past year.\n3. Integrate semantic search over user profiles by connecting to a vector database via its newest Python client library released within the last 12 months.\n4. Schedule periodic model retraining in a background worker using the most recent Python task scheduler released in the last 12 months.\n5. Store and serve user-uploaded media files on cloud object storage using the latest cloud storage client library published within the past 12 months.\n\nDeliverables:\n\u2022 A requirements list (requirements.txt) specifying the exact library names and versions (all released within the last 12 months).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total, with clear comments marking where and why each external library is used.","question_index":71,"modules":["fastapi","litestar","redis-stack","apscheduler","minio","python-multipart","pydantic","uvicorn","websockets"],"modulenotfound":["redis-stack"],"modulenotfound_count":1,"module_count":9}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Develop a high-performance Python microservice that ingests real-time social media posts via WebSockets, generates AI summaries, stores them in MongoDB Atlas, serves secured HTTP\/3 endpoints, and provides an interactive Streamlit dashboard for sentiment trends.\n\nFunctional Requirements:\n1. Initialize an asynchronous FastAPI app with HTTP\/3 (h2\/h3) and ASGI support.  \n2. Protect all HTTP endpoints with OAuth2 JWT using Authlib\u2019s latest OAuth library.  \n3. Consume a WebSocket stream of JSON posts from wss:\/\/stream.example.com asynchronously.  \n4. Summarize each post using OpenAI\u2019s latest Python SDK leveraging function-calling or embeddings.  \n5. Persist summaries and metadata in MongoDB Atlas via Motor (async driver).  \n6. Expose a secured REST endpoint `\/summaries` supporting pagination and filtering by sentiment.  \n7. Build a Streamlit dashboard that fetches `\/summaries` and visualizes sentiment trends over time with Plotly.\n\nDeliverables:\n- A requirements list naming the latest versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":28,"modules":["fastapi","uvicorn","aioquic","authlib","python-jose","websockets","openai","motor","streamlit","plotly","python-dotenv"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a real-time HTTP\/3-powered AI chat service that streams user messages, generates LLM responses, persists embeddings in a vector DB, and exposes chat history via GraphQL.\n\nFunctional Requirements:\n1. HTTP\/3 streaming: use the latest aioquic (as of 2025-05-06) to handle bidirectional HTTP\/3 connections for chat.\n2. On-device LLM inference: integrate the latest llama-cpp-python for quantized model loading and response generation.\n3. Vector storage: use the latest chromadb Python client to embed each message (via llama-cpp-python\u2019s embed API) and store\/retrieve vectors.\n4. GraphQL API: implement an async GraphQL schema with the latest strawberry to query chat history from ChromaDB.\n5. Async orchestration: ensure end-to-end async I\/O under Python 3.11+, coordinating the QUIC server, LLM, and DB.\n6. Code constraints: keep the complete solution under 150 lines, include clear comments marking where each external library is used.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of aioquic, llama-cpp-python, chromadb, and strawberry (latest as of 2025-05-06)  \n\u2022 A single Python 3.11+ script (<150 lines) with runnable code and comments identifying each external library usage.","question_index":79,"modules":["aioquic","llama-cpp-python","chromadb","strawberry-graphql"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Design and implement a high-performance real-time chat backend with semantic search and background processing.\n\nFunctional Requirements:\n1. Asynchronous REST API: use FastAPI (latest as of 2025-05-06) to expose user and message endpoints.\n2. HTTP\/3 WebSockets: implement real-time chat over WebSockets with Hypercorn\u2019s HTTP\/3 support.\n3. Async ORM persistence: store users and messages in PostgreSQL via SQLModel (async).\n4. Vector semantic search: index and query messages in Qdrant using the latest qdrant-client.\n5. Background sentiment analysis: enqueue and run analysis jobs with ARQ (async Redis-backed queue).\n6. JWT authentication: secure all endpoints with python-jose for JWT issuance and validation.\n7. Redis caching: cache active WebSocket connections or session data via redis.asyncio.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06  \n\u2022 A single Python 3.11+ file under 150 lines that:\n  \u2013 Is fully runnable  \n  \u2013 Shows clear comments where each external library is used  \n  \u2013 Integrates all above requirements into a cohesive application","question_index":11,"modules":["fastapi","hypercorn","sqlmodel","qdrant-client","python-jose","redis","arq","asyncpg","sentence-transformers"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: A fintech startup requires a live transaction monitoring service that ingests, processes, stores, and notifies users of suspicious activity in real time.\n\nFunctional Requirements:\n1. Build an HTTP endpoint to receive and stream-respond to JSON transaction payloads using the latest async web framework released in the last 12 months.\n2. Validate and serialize incoming data with the latest data validation library, enforcing strict type constraints and custom validators.\n3. Persist transactions asynchronously in a relational database using the latest async ORM library compatible with Python 3.11+.\n4. Offload CPU-intensive anomaly detection to a background worker queue using the latest task orchestration library.\n5. Broadcast real-time alerts to connected clients via WebSockets using the latest real-time communication library.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of the latest libraries (as of May 6, 2025)  \n\u2022 A single Python 3.11+ file under 150 lines, complete and runnable, with clear comments indicating where each external library is used.","question_index":39,"modules":["fastapi","pydantic","sqlmodel","celery","redis","websockets","databases","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a real-time collaborative document editor service with AI-powered summarization, OAuth2 access control, WebSocket syncing, Redis caching, and NoSQL persistence.\n\nFunctional Requirements:\n1. Use the latest async Python web framework (as of 2025-05-06) to define RESTful CRUD endpoints for documents.\n2. Implement OAuth2 authorization with the latest Python OAuth library released within the last 12 months for secure user login and token issuance.\n3. Establish bi-directional real-time document synchronization over WebSockets using the newest real-time communication library (with built-in pub\/sub).\n4. Integrate the latest AI inference library published in the past year to generate live summaries of incremental document edits.\n5. Employ the latest Redis client library (released within the last year) to cache active sessions and coordinate pub\/sub channels for syncing.\n6. Persist document state and version history in a NoSQL database using the newest async database client released in the last 12 months.\n\nDeliverables:\n1. A numbered list of the chosen \u201clatest\u201d libraries (with a one-line description of their role).\n2. Complete, runnable Python 3.11+ code under 150 lines, including clear comments indicating where and why each external library is used.","question_index":86,"modules":["fastapi","auth0-python","python-socketio","inferkit","redis","motor","uvicorn"],"modulenotfound":["inferkit"],"modulenotfound_count":1,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Develop a user registration microservice over HTTP\/3 that validates input, persists user data, and integrates with an external email verification API.\n\nFunctional Requirements:\n1. Expose a POST \/users endpoint using Starlite (latest) with HTTP\/3 support.  \n2. Define request and response schemas with Pydantic v2 (latest) for data validation.  \n3. Persist user records in PostgreSQL via the latest Prisma Python client, including model definitions and migrations.  \n4. Perform an asynchronous HTTP\/3 request to an external email verification service using httpx (latest), and incorporate its response.  \n5. Return a JSON payload containing the stored user data and email verification status.\n\nDeliverables:\n- A list of required third-party libraries with exact versions (as of today)  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":100,"modules":["starlite","pydantic","prisma","httpx","aioquic","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: As a senior software engineer, build a modular web application that authenticates users with WebAuthn passkeys, streams real-time stock data, generates AI trading signals, processes payments, and delivers interactive dashboards.\n\nFunctional Requirements:\n1. Implement user authentication via WebAuthn using the latest pywebauthn library (as of 2025-05-06) with hardware security key support.\n2. Ingest and stream real-time stock price data over WebSockets using FastAPI and the latest Starlette HTTP\/2 push features.\n3. Generate AI-driven trading signals leveraging the latest OpenAI Python SDK\u2019s new function-calling interface.\n4. Integrate secure payment processing with the latest Stripe Python SDK supporting Payment Element and webhook handling.\n5. Serve interactive dashboards using the latest Plotly Dash or Reflex library for live data visualization and client-side callbacks.\n6. Add structured logging and Prometheus metrics using structlog and prometheus-client.\n\nDeliverables:\n- A concise mapping of each functional requirement to its implementation approach.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used and using the latest libraries available as of 2025-05-06.","question_index":56,"modules":["fastapi","starlette","webauthn","stripe","openai","prometheus-client","structlog","plotly","reflex-dev","uvicorn"],"modulenotfound":["reflex-dev"],"modulenotfound_count":1,"module_count":10}
{"model":"claude-3-5-sonnet-20241022","question":"You are a senior software engineer tasked with architecting a small, high-performance async web application using only the latest Python libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Litestar framework (with native HTTP\/3 support) to define async REST endpoints for user signup and profile retrieval.  \n2. Integrate the latest Piccolo-ORM to define a PostgreSQL \u201cusers\u201d table and perform async CRUD operations.  \n3. Implement real-time notifications via WebSocket using the latest python-socketio in asyncio mode.  \n4. Schedule a daily summary email job with APScheduler\u2019s new AsyncIOScheduler and send mail via an async SMTP library.  \n5. Fetch external data during signup from a third-party API using HTTPX\u2019s HTTP\/2 client.\n\nDeliverables:\n\u2022 A requirements.txt listing exact package names and latest versions.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":29,"modules":["litestar","piccolo","python-socketio","apscheduler","httpx","aiosmtplib","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are a senior software engineer building a high-performance real-time IoT analytics backend.\n\nFunctional Requirements:\n1. Expose an asynchronous HTTP API using FastAPI with WebSocket support to ingest and broadcast live sensor readings.\n2. Persist incoming data in PostgreSQL via SQLModel\u2019s async ORM and validate payloads with Pydantic v2 models.\n3. Enforce per-device rate limiting using Redis Streams and aioredis, leveraging RedisJSON for overflow handling.\n4. Secure all endpoints with OAuth2 passwordless login via FastAPI-Users, sending one-time codes.\n5. Schedule background aggregation jobs with Celery (beat scheduler) to compute rolling metrics and push updates over WebSockets.\n6. Provide a paginated historical metrics REST endpoint, caching hot queries in Redis for low-latency responses.\n\nDeliverables:\n- A list of required external libraries pinned to the latest versions available as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is utilized.","question_index":27,"modules":["fastapi","uvicorn","sqlmodel","pydantic","aioredis","rejson","fastapi-users","celery","asyncpg","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a real-time collaborative document review service with semantic search and AI summarization.\n\nFunctional Requirements:\n1. HTTP API (HTTP\/3) using the latest starlite (v1.x) to manage documents and user sessions.  \n2. Real-time updates over WebSockets for live editing and presence using starlite\u2019s WebSocket support.  \n3. Semantic indexing and search: on each document save, compute embeddings and store\/retrieve via the latest qdrant-client.  \n4. AI summarization endpoint: call an LLM using the latest llm-tools library to generate concise summaries on demand.  \n5. Background task scheduling with dramatiq (latest) to clean up stale sessions and regenerate indexes.\n\nDeliverables:\n\u2022 Reiterate the numbered requirements.  \n\u2022 A complete, runnable Python 3.11+ code file under 150 lines.  \n\u2022 Use the latest starlite, qdrant-client, llm-tools, and dramatiq libraries available as of 2025-05-06.  \n\u2022 Include clear comments indicating where and why each external library is used.","question_index":55,"modules":["dramatiq","llm_tools","pydantic","qdrant_client","starlite"],"modulenotfound":["llm_tools"],"modulenotfound_count":1,"module_count":5}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are building a high-performance, AI-driven text-generation microservice for a real-time web application.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/generate endpoint using FastAPI (latest 0.100+) for incoming requests.\n2. Validate and parse the request payload with Pydantic v2 (latest) leveraging its new ArgModel feature.\n3. Enqueue each generation request as an asynchronous job using ARQ (latest 0.7+) backed by Redis.\n4. In the ARQ worker, invoke vLLM (latest 0.7+) AsyncLLMClient to perform streaming text inference.\n5. Persist each request and its full response to PostgreSQL using SQLModel (latest 0.0.x) with SQLAlchemy 2.0 async engine.\n6. Expose a WebSocket \/ws\/stream endpoint in FastAPI to broadcast partial LLM outputs to clients in real time.\n\nDeliverables:\n\u2022 The above numbered requirements list.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":30,"modules":["fastapi","redis","arq","sqlmodel","vllm","uvicorn","asyncpg","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You must build a Python microservice that fetches articles by URL, summarizes them with AI, stores results in Postgres, and provides REST, GraphQL, and WebSocket interfaces with background orchestration using only the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a POST \/submit REST endpoint using the latest Starlite framework (released within the last 12 months) over HTTP\/3 to accept JSON { \"url\": string }.  \n2. Fetch article content asynchronously using the latest httpx library and handle timeouts and redirects.  \n3. Summarize text with the latest HuggingFace Transformers v5 pipeline, leveraging GPU if available.  \n4. Persist the original URL and its summary in Postgres via Tortoise ORM v0.25 (async ORM released in the last 12 months).  \n5. Expose a \/graphql endpoint using Strawberry GraphQL (latest release) to query summaries by URL.  \n6. Provide a \/ws\/notifications WebSocket in Starlite to notify clients in real time when a summary is ready.  \n7. Orchestrate fetch-and-summarize tasks as background jobs using Prefect 2 (newly released workflow orchestrator).\n\nDeliverables:\n- A copy of the numbered functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Use only the latest versions of Starlite, httpx, Transformers v5, Tortoise ORM v0.25, Strawberry GraphQL, and Prefect 2 as of 2025-05-06.\n- Include clear comments indicating where and how each external library is used.","question_index":70,"modules":["starlite","tortoise-orm","prefect","transformers","strawberry-graphql","httpx","uvicorn","torch","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are tasked with developing a next-generation chat analytics platform that seamlessly integrates HTTP\/3, JWT auth, real-time messaging, vector search, and on-device LLM inference.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) with HTTP\/3 support via Hypercorn to serve all endpoints.\n2. Implement JWT authentication with rotational refresh tokens using fastapi-users (latest).\n3. Provide a real-time WebSocket chat endpoint using fastapi-websocket-pubsub (latest) supporting pub\/sub channels.\n4. On each incoming chat message, generate vector embeddings using llama-cpp-python (latest) with an on-device streaming pipeline, and store them in ChromaDB (latest) with HNSW indexing.\n5. Expose a REST endpoint to query semantic similarity: accept a text query, compute embeddings, perform a ChromaDB search, and return the top-3 similar messages.\n6. Expose a streaming summarization endpoint that returns an on-the-fly summary of recent messages using llama-cpp-python\u2019s streaming completion interface.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements you\u2019ve implemented.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":93,"modules":["fastapi","fastapi-users","sqlalchemy","llama-cpp","chromadb","pydantic","aiosqlite","hypercorn"],"modulenotfound":["llama-cpp"],"modulenotfound_count":1,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: As a senior software engineer, develop a Python 3.11+ microservice that manages real-time collaborative whiteboard sessions, persists data, exposes REST and GraphQL APIs, generates session snapshots, integrates external metrics, and secures endpoints with OAuth2.\n\nFunctional Requirements:\n1. Implement session and user management REST endpoints using FastAPI (latest) with async event hooks and Pydantic V2 models.  \n2. Persist all data in PostgreSQL via Tortoise ORM (latest) using model lifecycle events.  \n3. Expose a GraphQL API with Strawberry GraphQL (latest) dataclass resolvers for session and user queries\/mutations.  \n4. Broadcast real-time drawing events using python-socketio (latest) over WebSockets.  \n5. Generate on-demand PNG snapshots of whiteboard state with Pillow-SIMD (latest) leveraging multi-threaded rendering.  \n6. Send session metrics asynchronously to an external analytics service using httpx (latest) with HTTP\/2 support.  \n7. Secure all endpoints with OAuth2 (authorization code + PKCE) using Authlib (latest).  \n\nDeliverables:\n- A numbered list of the functional requirements above.  \n- Complete, runnable Python 3.11+ code (\u2264 150 lines) integrating all requirements.  \n- Inline comments indicating where each external library is used.  \n- Use the latest stable releases of all libraries as of today\u2019s date.","question_index":50,"modules":["fastapi","strawberry-graphql","tortoise-orm","pillow-simd","authlib","python-socketio","httpx","uvicorn","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are a senior software engineer tasked with building a real-time analytics microservice that combines REST, GraphQL, persistent storage, and background processing using the latest Python ecosystem tools as of 2025-05-06.\n\nFunctional Requirements:\n1. Expose HTTP and WebSocket endpoints using the latest Litestar ASGI framework with annotated routing and streaming support.\n2. Implement a GraphQL API using the latest Strawberry GraphQL code-first schema builder with asyncio support.\n3. Persist and query analytics data in PostgreSQL via the latest Piccolo-ORM\u2019s async-first models and automatic migrations.\n4. Offload long-running analytics jobs to a Redis-backed async queue using the latest Arq library.\n5. Send WebSocket notifications to clients when background jobs complete.\n\nDeliverables:\n\u2022 A requirements list (pip install commands) specifying the latest versions of Litestar, Strawberry, Piccolo-ORM, and Arq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":65,"modules":["litestar","strawberry-graphql","piccolo","arq","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are a senior software engineer tasked with developing an end-to-end Python web application that leverages the latest libraries to showcase advanced API development, ORM integration, security, background processing, and real-time features.\n\nFunctional Requirements:\n1. Asynchronous REST API: implement CRUD endpoints for a `Task` model using FastAPI (latest version as of 2025-05-06).\n2. Data Modeling and Persistence: define `Task` ORM models with SQLModel (latest version) and connect to a SQLite or PostgreSQL database asynchronously.\n3. Security: secure all API endpoints with OAuth2 password flow using Authlib (latest version) to obtain and validate JWT tokens.\n4. Background Processing: create an asynchronous task queue with Arq (latest version) to process tasks in the background.\n5. Real-Time Notifications: establish a WebSocket endpoint using the websockets library (latest version) to push real-time task status updates.\n\nDeliverables:\n- The numbered list of functional requirements (as above).\n- Complete, runnable Python 3.11+ code under 150 lines. Use the latest versions of FastAPI, SQLModel, Authlib, Arq, and websockets as of 2025-05-06. Include clear comments indicating where and how each external library is used.","question_index":24,"modules":["fastapi","sqlmodel","arq","databases","pyjwt","uvicorn","aiosqlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a real-time collaborative messaging microservice integrating graph storage, vector search, JWT authentication, and WebSockets using the latest Python libraries (as of May 6, 2025).\n\nFunctional Requirements:\n1. Authentication: secure all endpoints with JWTs using the latest TokenGuard library.\n2. Message Ingestion: implement POST \/messages to accept user messages; decode JWT, extract user ID.\n3. Graph Storage: store each message and its \u201cauthored_by\u201d relationship in a graph database via the latest GraphitORM.\n4. Vector Indexing & Search: generate text embeddings with the latest Morpher library, store them in its vector store, and expose GET \/search?q={text} to return top-5 similar messages.\n5. Real-Time Updates: broadcast new messages to connected clients over WebSockets using the latest Voila framework.\n\nDeliverables:\n1. A requirements list with each third-party library pinned to its latest version as of May 6, 2025.\n2. A single Python 3.11+ script (\u2264150 lines) implementing all endpoints, with clear comments marking where each external library is used.","question_index":35,"modules":["fastapi","tokenguard","graphitorm","morpher-ai","voila-ws","uvicorn"],"modulenotfound":["tokenguard","graphitorm","morpher-ai","voila-ws"],"modulenotfound_count":4,"module_count":6}
{"model":"claude-3-5-sonnet-20241022","question":"You are tasked with building a high-performance text summarization service with real-time updates, persistent storage, and caching for internal research teams.\n\nFunctional Requirements:\n1. Implement OAuth2 password flow with JWT tokens for user authentication using the latest python-jose.\n2. Expose a POST \/summarize endpoint that uses the latest transformers library to perform text summarization.\n3. Provide a WebSocket at \/ws\/progress to stream summarization progress in real time using FastAPI\u2019s latest WebSocket support.\n4. Persist user requests and summaries in a SQLite database via the latest SQLModel ORM.\n5. Cache summary results in Redis for 5 minutes using the latest aioredis library to accelerate repeat requests.\n\nDeliverables:\n\u2022 A requirements list specifying the latest versions of FastAPI, transformers, python-jose, SQLModel, aioredis (as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":10,"modules":["fastapi","python-jose","transformers","sqlmodel","redis","passlib","python-multipart","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"You are a senior software engineer.\n\nScenario: Build an end-to-end Python service for user profile CRUD with high-performance caching and real-time notifications.\n\nFunctional Requirements:\n1. Implement RESTful CRUD endpoints for user profiles using the latest FastAPI (as of 2025-05-06), leveraging its async lifespan context.\n2. Define an async PostgreSQL-backed User model with SQLModel (latest) using Relations and the new async engine.\n3. Cache GET \/users\/{id} responses in Redis with redis.asyncio (latest), utilizing cluster mode and automatic TTL invalidation on updates.\n4. Broadcast profile change events in real time over WebSocket using the websockets library (latest) with permessage-deflate compression.\n\nDeliverables:\n- A requirements.txt listing the latest versions of FastAPI, SQLModel, redis.asyncio and websockets as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":17,"modules":["fastapi","sqlmodel","redis-asyncio","websockets","uvicorn","asyncpg"],"modulenotfound":["redis-asyncio"],"modulenotfound_count":1,"module_count":6}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are a senior software engineer building a real-time sentiment-analysis web service for social media feeds.\n\nFunctional Requirements:\n1. Use the latest Python web framework (released within the last 12 months) to expose REST endpoints with dependency injection and OpenAPI support.\n2. Implement real-time websocket notifications for incoming sentiment updates using a recently released WebSocket library (last 12 months).\n3. Persist incoming posts and sentiment scores in a NoSQL database via its newest async client library (released within the last 12 months).\n4. Offload sentiment inference to a CPU-optimized ML library (first released within the last 12 months) with zero-copy or memory-efficient batch inference.\n5. Schedule periodic cleanup and summary-generation tasks using a lightweight scheduler library introduced within the last 12 months.\n6. Serve a minimal interactive dashboard at \u201c\/dashboard\u201d using a modern, Python-native plotting\/UI library created in the last 12 months.\n\nDeliverables:\n- A bullet list of all third-party libraries chosen, pinned to \u201clatest\u201d versions as of today\u2019s date.\n- A single, complete Python 3.11+ file (under 150 lines) that implements all requirements, with clear comments marking where each external library is used.","question_index":94,"modules":["fastapi","litestar","motor","torchmlir","apscheduler","nicegui","pydantic","uvicorn"],"modulenotfound":["torchmlir"],"modulenotfound_count":1,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: As a senior software engineer, you must build a real-time analytics backend that ingests user events, persists them, and serves both REST and GraphQL clients with low latency and live updates.\n\nFunctional Requirements:\n1. Implement a POST \/events endpoint using the latest FastAPI (as of 2025-05-06), leveraging its new lifespan context and async router features.\n2. Validate incoming JSON payloads with Pydantic v2\u2019s new @model_validator decorator for strict schema enforcement.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise ORM, utilizing its recent bulk_create optimization.\n4. Expose a GraphQL query and mutation schema via the latest Strawberry GraphQL with code-first federation support.\n5. Broadcast new events to connected clients over WebSockets using the latest websockets library with per-message deflate compression.\n\nDeliverables:\n\u2022 A requirements.txt listing each third-party library pinned to its latest version as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments indicating where and how each external library is used.","question_index":19,"modules":["fastapi","pydantic","tortoise-orm","strawberry-graphql","websockets","uvicorn","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are a senior software engineer building a modern microblogging backend that supports authenticated posts, AI-generated images, real-time feed updates, and cloud storage integration.\n\nFunctional Requirements:\n1. Implement a REST API using a Python web framework released in the last 12 months (use the latest available as of 2025-05-06).\n2. Persist posts in a SQL database via an async ORM library created in the last 12 months, leveraging advanced async model features.\n3. Broadcast new posts to connected clients over WebSockets using a recently released high-performance async messaging library.\n4. Authenticate users with OAuth2 (Google) using the latest OAuth client library.\n5. Generate post images by calling Stable Diffusion through the most recent Python API client for AI image generation.\n6. Store and serve AI-generated images in AWS S3 using the newest AWS SDK for Python.\n7. Ensure all components run under Python 3.11+ and fit within 150 lines of code.\n\nDeliverables:\n\u2022 A numbered list of the specific libraries (with versions) you chose to satisfy each requirement.  \n\u2022 Complete, runnable Python 3.11+ code (<150 lines) combining all features.  \n\u2022 Clear inline comments marking where and why each external library is used.","question_index":7,"modules":["fastapi","sqlmodel","broadcaster","google-auth-oauthlib","stability-sdk","boto3","python-jose","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are a senior software engineer tasked with building a serverless real-time IoT analytics service that ingests sensor data, performs streaming aggregation, applies online anomaly detection, and streams results to clients.\n\nFunctional Requirements:\n1. Ingest JSON messages from an MQTT broker (\u201csensors\/+\/data\u201d) using the latest asyncio-based gmqtt library (as of 2025-05-06).  \n2. Stream-process incoming messages with streamz (latest version) to compute 1-minute tumbling-window averages for temperature and humidity.  \n3. Apply online anomaly detection on each windowed aggregation using River\u2019s latest HDBSCAN-based drift detector.  \n4. Persist raw messages and aggregated results into TimescaleDB via the asyncpg driver (latest version).  \n5. Expose a Python 3.11+ FastAPI application with a WebSocket endpoint for live metric and anomaly alert streaming.  \n6. Wrap the FastAPI app for AWS Lambda deployment using the latest mangum adapter.\n\nDeliverables:\n\u2022 A list of third-party dependencies pinned to \u201clatest\u201d versions as of 2025-05-06.  \n\u2022 A single, complete, runnable Python 3.11+ source file under 150 lines.  \n\u2022 Clear inline comments showing where each external library is used.  \n\u2022 The code must integrate all functional requirements and be deployable serverlessly on AWS Lambda.","question_index":38,"modules":["asyncpg","fastapi","gmqtt","mangum","river","streamz"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a real-time chat analysis service that ingests user messages, performs on-device LLM summarization and sentiment analysis, caches results, persists chat history, and broadcasts updates via WebSockets.\n\nFunctional Requirements:\n1. Expose a POST \/messages endpoint using FastAPI (latest) to receive JSON payloads with \u201cuser_id\u201d and \u201ctext\u201d.\n2. Perform sentiment analysis and summarization on each message using llama-cpp-python (latest) in an async background task.\n3. Cache sentiment results for identical texts in Redis via redis-py (latest) to avoid recomputation.\n4. Persist messages, sentiments, and summaries in PostgreSQL using SQLModel (latest version).\n5. Provide a WebSocket \/ws\/{user_id} endpoint (FastAPI) that broadcasts new summaries and sentiments to connected clients in real time.\n6. Secure all endpoints with OAuth2 password flow via Authlib (latest).\n\nDeliverables:\n\u2022 Restate the numbered requirements above.  \n\u2022 Provide complete, runnable Python 3.11+ code integrating the latest versions of FastAPI, llama-cpp-python, redis-py, SQLModel, and Authlib, under 150 lines total.  \n\u2022 Include clear comments indicating where and how each external library is used.","question_index":32,"modules":["fastapi","sqlmodel","llama-cpp","redis","authlib"],"modulenotfound":["llama-cpp"],"modulenotfound_count":1,"module_count":5}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python microservice for real-time IoT sensor analytics and visualization.\n\nFunctional Requirements:\n1. REST API: Use the latest asynchronous Python web framework available as of 2025-05-06 to expose CRUD endpoints with advanced dependency injection and OpenAPI schema generation.\n2. WebSockets: Integrate the latest real-time streaming library as of 2025-05-06 supporting protocol-level backpressure to push live sensor updates to clients.\n3. ML Inference: Embed an on-the-fly analytics model using the latest Python JIT-accelerated inference library as of 2025-05-06, auto-batching requests on CPU\/GPU.\n4. Graph Storage: Persist sensor relationships in a distributed graph database via the latest async driver as of 2025-05-06 with reactive query pipelining.\n5. Dashboard UI: Serve an interactive web dashboard using the latest server-driven UI component library as of 2025-05-06 for real-time charting and controls.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":14,"modules":["fastapi","uvicorn","websockets","torch","neo4j","streamlit","pydantic","numpy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"You are a Senior Software Engineer tasked with building a high-performance, real-time analytics microservice using the latest asynchronous Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled ASGI endpoint using the latest Litestar and aioquic (as of 2025-05-06) for bi-directional streaming.\n2. Implement real-time GraphQL subscriptions with the latest Strawberry GraphQL to push analytics updates.\n3. Define domain models and perform asynchronous CRUD operations on Postgres using the latest SQLModel.\n4. Integrate a Redis cache layer with the latest aioredis for hot-path data retrieval.\n5. Instrument request handling and background tasks with OpenTelemetry SDK for distributed tracing.\n\nDeliverables:\n- requirements.txt listing the latest library versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments indicating where each external library is used.","question_index":90,"modules":["litestar","aioquic","strawberry-graphql","sqlmodel","redis","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation","asyncpg","uvicorn","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-5-sonnet-20241022","question":"You are building a real-time collaborative code-review web application that leverages the latest AI, async, storage and security frameworks.\n\nFunctional Requirements:\n1. Implement a FastAPI 0.95+ server with async endpoints and Jinja2 templates for the main UI.\n2. Enable real-time code editing and review comments via WebSockets using the latest websockets library as of 2025-05-06.\n3. On each code submission, stream analysis from OpenAI\u2019s GPT-4o API (or equivalent newest model) and display suggestions in real time.\n4. Persist code snapshots and review metadata in Pinecone\u2019s newest Python client (vector storage) for similarity search.\n5. Secure all HTTP and WS routes with JWT authentication using the latest PyJWT release.\n6. Add structured logging and a \/healthz endpoint using structlog for observability.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total.  \n\u2022 Clear comments indicating where and why each external library is used.","question_index":37,"modules":["fastapi","uvicorn","websockets","jinja2","pinecone-client","python-jose","structlog","openai"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a real-time Q&A web application that ingests documents, generates embeddings, serves semantic search, supports live WebSocket notifications, and exposes observability metrics.\n\nFunctional Requirements:\n1. Use the latest FastAPI (>=0.100) to implement asynchronous REST endpoints for document ingestion and search, plus a WebSocket endpoint leveraging its new lifecycle event hooks.\n2. Upon document ingestion, asynchronously generate embeddings with the latest llama-cpp-python (v0.2+) using 4-bit quantization and store vectors in Pinecone (latest) using hybrid search.\n3. Implement a `\/search` endpoint that queries Pinecone for nearest neighbors based on user input and returns ranked results.\n4. Schedule periodic re-indexing tasks with the latest arq (v1.10+), utilizing its async Redis job scheduler.\n5. Integrate OpenTelemetry Python SDK (v1.21+) for auto-instrumentation, exporting traces and metrics to Prometheus.\n\nDeliverables:\n- A requirements.txt listing the latest libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":33,"modules":["fastapi","llama-cpp-python","pinecone-client","arq","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","prometheus-client","uvicorn","redis","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: As a senior software engineer, you must build a modular Python web application that serves real-time, ML-enhanced classification results with persistent storage and observability.\n\nFunctional Requirements:\n1. API Layer: Use the latest ASGI web framework available as of 2025-05-06 that supports HTTP\/3 and automatic OpenAPI generation to expose a POST \/classify endpoint.  \n2. Database Layer: Connect asynchronously to PostgreSQL using the latest async driver with statement pooling to record incoming requests and classification results.  \n3. Real-time Pub\/Sub: Implement server-side WebSocket broadcasts of classification results using the latest message queue library offering at-least-once delivery semantics.  \n4. ML Inference: Integrate on-demand image classification via the latest lightweight, GPU-accelerated inference engine released in the past 12 months.  \n5. Observability: Instrument distributed tracing and metrics collection using the latest OpenTelemetry-compatible tracing library.\n\nDeliverables:\n- A list of the latest library dependencies as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":18,"modules":["fastapi","uvicorn","asyncpg","redis","onnxruntime-gpu","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","pillow","pydantic","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: As a senior software engineer, build a high-performance microblogging API using the latest Python libraries for REST, validation, async database operations, caching, background tasks, and real-time streaming.\n\nFunctional Requirements:\n1. Implement a user signup endpoint in FastAPI (>=0.100.0) using Pydantic v2 root_validator for advanced schema validation.  \n2. Create and list posts asynchronously in PostgreSQL via SQLModel\u2019s latest async engine.  \n3. Maintain a cache of the 10 most recent posts in Redis using redis-py (>=5.0) asyncio client, updating on post creation.  \n4. Offload image thumbnail generation to an async Dramatiq (>=2.0) background task triggered on new post with image.  \n5. Provide a WebSocket endpoint in FastAPI to broadcast newly created posts to connected clients in real time.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library (FastAPI, Pydantic v2, SQLModel, redis-py asyncio, Dramatiq) is used. Use the latest available versions of all libraries as of 2025-05-06.","question_index":87,"modules":["fastapi","pydantic","sqlmodel","redis","dramatiq","pillow","sqlalchemy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are building a real-time news aggregation microservice with live client updates.\n\nFunctional Requirements:\n1. Develop a REST API using Litestar (the latest version as of 2025-05-06) leveraging its OpenAPI v3.1 auto-generation feature.\n2. Asynchronously fetch and parse external RSS feeds over HTTP\/3 using HTTPX AsyncClient (the latest), then cache results in aiocache (the latest) with a TTL.\n3. Validate and serialize each article using Pydantic v2 (the latest) functional-style models to ensure data integrity.\n4. Expose a WebSocket endpoint using the websockets library (the latest) with permessage-deflate compression for real-time article broadcasting.\n5. Schedule periodic background tasks with Arq (the latest) to re-poll feeds every 5 minutes.\n\nDeliverables:\n- A list of the chosen libraries and their latest versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":31,"modules":["httpx","feedparser","litestar","pydantic","aiocache","arq","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: Build a real-time collaborative note-taking microservice integrating REST, database persistence, caching, GraphQL, WebSockets, and background tasks.\n\nFunctional Requirements:\n1. Expose REST endpoints using FastAPI (latest) for creating, updating and retrieving notes; leverage Pydantic v2\u2019s new validation decorators.\n2. Persist notes asynchronously in SQLite via SQLModel (latest) with SQLAlchemy 2.0-style typing and async engine.\n3. Broadcast note creation and updates over WebSockets with FastAPI\u2019s WebSocket support to subscribed clients in real time.\n4. Cache note retrieval results in Redis using aioredis (latest), implement TTL and cache invalidation on updates.\n5. Provide a GraphQL schema and endpoint using Strawberry (latest) to query and filter notes with federation-style resolvers.\n6. Schedule a daily summary email using FastAPI BackgroundTasks and aiosmtplib (latest) for asynchronous SMTP delivery.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- A single Python 3.11+ source file under 150 lines that implements all requirements, with clear comments marking where each external library is used.","question_index":73,"modules":["fastapi","uvicorn","sqlmodel","pydantic","aioredis","strawberry-graphql","aiosmtplib","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-5-sonnet-20241022","question":"Scenario: You are a senior software engineer building a proof-of-concept Python service that ingests user text, generates embeddings and classifications via an LLM, stores them, and streams nearest-neighbor results in real time.\n\nFunctional Requirements:\n1. Implement a JWT-secured POST \/ingest endpoint using the latest async Python web framework as of 2025-05-06 (e.g., Litestar).  \n2. In \/ingest, accept JSON `{ \"text\": \"...\" }`, call a state-of-the-art LLM (via the latest Transformers or LangChain client) to produce both an embedding vector and a classification label.  \n3. Persist the embedding vector in a vector database using the newest ChromaDB client.  \n4. Log the original text and its classification into a relational database through the latest async ORM (e.g., Prisma Python).  \n5. Expose a GET \/stream endpoint that streams nearest-neighbor matches for incoming embeddings over server-sent events using the newest SSE library (e.g., httpx-sse or framework-native support).  \n6. Target Python 3.11+, include type hints, and keep all code under 150 lines.\n\nDeliverables:\n\u2022 A `requirements.txt` (or equivalent) listing each library with exact versions (latest as of 2025-05-06).  \n\u2022 A single Python module (\u2264150 lines) that meets all requirements, with clear comments marking where and why each external library is used.","question_index":81,"modules":["litestar","chromadb","langchain","prisma","python-jose","httpx","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are a senior software engineer building a proof-of-concept Python service that ingests user text, generates embeddings and classifications via an LLM, stores them, and streams nearest-neighbor results in real time.\n\nFunctional Requirements:\n1. Implement a JWT-secured POST \/ingest endpoint using the latest async Python web framework as of 2025-05-06 (e.g., Litestar).  \n2. In \/ingest, accept JSON `{ \"text\": \"...\" }`, call a state-of-the-art LLM (via the latest Transformers or LangChain client) to produce both an embedding vector and a classification label.  \n3. Persist the embedding vector in a vector database using the newest ChromaDB client.  \n4. Log the original text and its classification into a relational database through the latest async ORM (e.g., Prisma Python).  \n5. Expose a GET \/stream endpoint that streams nearest-neighbor matches for incoming embeddings over server-sent events using the newest SSE library (e.g., httpx-sse or framework-native support).  \n6. Target Python 3.11+, include type hints, and keep all code under 150 lines.\n\nDeliverables:\n\u2022 A `requirements.txt` (or equivalent) listing each library with exact versions (latest as of 2025-05-06).  \n\u2022 A single Python module (\u2264150 lines) that meets all requirements, with clear comments marking where and why each external library is used.","question_index":81,"modules":["litestar","pydantic","prisma","chromadb","langchain","langchain-openai","python-jose","python-dotenv"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Create a serverless real-time analytics API that ingests WebSocket events, performs on-the-fly NLP summarization, indexes embeddings, exposes a GraphQL endpoint, and deploys to AWS Lambda.\n\nFunctional Requirements:\n1. Use the latest Litestar (as of 2025-05-06) to build an HTTP\/2 GraphQL endpoint with schema-based typing.\n2. Implement a real-time WebSocket consumer using wsproto or litestar-websockets for event ingestion.\n3. Offload text summarization to an asynchronous background task leveraging the latest Transformers library.\n4. Generate embeddings with the newest OpenAI Embeddings API or sentence-transformers and store them in the latest ChromaDB vector store.\n5. Secure all endpoints with passwordless authentication via the latest fastapi-users or equivalent.\n6. Package and deploy the entire application to AWS Lambda using the latest Mangum adapter.\n\nDeliverables:\n- A numbered list of chosen \u201clatest\u201d library versions (as of 2025-05-06) mapping to each requirement.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":77,"modules":["litestar","transformers","openai","chromadb","mangum","strawberry-graphql","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are tasked with building a modular Python 3.11+ microservice that delivers real-time chat with semantic search and scheduled maintenance, leveraging only the latest libraries released within the past 12 months.\n\nFunctional Requirements:\n1. Implement HTTP REST endpoints for posting and retrieving chat messages using Litestar (latest release).  \n2. Establish a WebSocket chat channel that broadcasts new messages to all connected clients via the official Litestar WebSocket plugin (latest).  \n3. Persist each message to MongoDB with Beanie ODM (latest) and simultaneously index its vector embedding into Qdrant using qdrant-client (latest).  \n4. Expose a GraphQL query endpoint that performs semantic search over stored messages via strawberry-graphql (latest).  \n5. Schedule a daily background job to purge messages older than 30 days using Dramatiq (latest).\n\nDeliverables:\n- A requirements.txt (or pyproject.toml) listing all dependencies with exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments marking where and how each external library is used.","question_index":76,"modules":["litestar","motor","beanie","sentence-transformers","qdrant-client","strawberry-graphql","dramatiq","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a real-time collaborative document editor service with AI-powered summarization, OAuth2 access control, WebSocket syncing, Redis caching, and NoSQL persistence.\n\nFunctional Requirements:\n1. Use the latest async Python web framework (as of 2025-05-06) to define RESTful CRUD endpoints for documents.\n2. Implement OAuth2 authorization with the latest Python OAuth library released within the last 12 months for secure user login and token issuance.\n3. Establish bi-directional real-time document synchronization over WebSockets using the newest real-time communication library (with built-in pub\/sub).\n4. Integrate the latest AI inference library published in the past year to generate live summaries of incremental document edits.\n5. Employ the latest Redis client library (released within the last year) to cache active sessions and coordinate pub\/sub channels for syncing.\n6. Persist document state and version history in a NoSQL database using the newest async database client released in the last 12 months.\n\nDeliverables:\n1. A numbered list of the chosen \u201clatest\u201d libraries (with a one-line description of their role).\n2. Complete, runnable Python 3.11+ code under 150 lines, including clear comments indicating where and why each external library is used.","question_index":86,"modules":["fastapi","authlib","starlette-ws","inference-lite","redis-om","motor","pydantic","uvicorn"],"modulenotfound":["starlette-ws","inference-lite"],"modulenotfound_count":2,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Develop a real-time AI-driven sentiment analysis web service that ingests text, processes it via an on-device LLM, persists results, and streams live analytics to clients.\n\nFunctional Requirements:\n1. Expose RESTful and WebSocket endpoints using the latest Litestar Python framework (released within the last 12 months).\n2. Perform local LLM inference for sentiment analysis with the latest llama-cpp-python library (released within the last 12 months).\n3. Asynchronously persist raw text and sentiment scores in MongoDB using the latest async MongoDB driver (released within the last 12 months) with code-first schema.\n4. Offload sentiment analysis to a distributed background worker using the latest Dramatiq (or equivalent) async task queue released in the last 12 months.\n5. Broadcast real-time sentiment updates to connected clients over WebSockets.\n6. Render an interactive dashboard in server-side templates with the latest Plotly Python library (released within the last 12 months).\n7. Instrument the entire application with distributed tracing via the latest OpenTelemetry Python SDK (released within the last 12 months).\n\nDeliverables:\n- A requirements list specifying each third-party library and its exact latest version as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines total.\n- Clear inline comments indicating where and how each external library is used.","question_index":5,"modules":["litestar","llama-cpp-python","motor","dramatiq","redis","plotly","jinja2","opentelemetry-api","opentelemetry-sdk","opentelemetry-exporter-otlp","python-dotenv"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python web service that combines asynchronous REST & GraphQL APIs, real-time notifications, persistent storage, caching, and AI-driven text summarization.\n\nFunctional Requirements:\n1. Implement async HTTP endpoints with FastAPI (latest as of 2025-05-06) using Python 3.11 structural pattern matching and Pydantic v2 dataclass models.\n2. Provide a GraphQL endpoint powered by Strawberry GraphQL v1.0+ with schema federation support.\n3. Persist and query data in PostgreSQL via the latest Tortoise-ORM with asyncio-optimized connection pooling.\n4. Push real-time notifications over WebSockets using python-socketio latest async server mode.\n5. Summarize user-submitted text on demand using the latest llama-cpp-python library with optional GPU offload.\n6. Cache frequent database queries in Redis using aioredis latest cluster-mode support.\n\nDeliverables:\n- A requirements list specifying exact versions of all third-party libraries.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":74,"modules":["fastapi","uvicorn","pydantic","strawberry-graphql","tortoise-orm","asyncpg","python-socketio","llama-cpp-python","aioredis","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Develop a user registration microservice over HTTP\/3 that validates input, persists user data, and integrates with an external email verification API.\n\nFunctional Requirements:\n1. Expose a POST \/users endpoint using Starlite (latest) with HTTP\/3 support.  \n2. Define request and response schemas with Pydantic v2 (latest) for data validation.  \n3. Persist user records in PostgreSQL via the latest Prisma Python client, including model definitions and migrations.  \n4. Perform an asynchronous HTTP\/3 request to an external email verification service using httpx (latest), and incorporate its response.  \n5. Return a JSON payload containing the stored user data and email verification status.\n\nDeliverables:\n- A list of required third-party libraries with exact versions (as of today)  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":100,"modules":["starlite","pydantic","prisma","httpx","uvicorn","asyncpg","quart","aioquic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"You are a senior software engineer tasked with building a real-time analytics dashboard with AI-driven anomaly alerts using the latest Python libraries as of 2025-05-06.  \n\nFunctional Requirements:  \n1. Implement asynchronous REST API endpoints using the latest FastAPI.  \n2. Add WebSocket-based real-time alert streaming using the latest websockets.  \n3. Persist incoming metrics in PostgreSQL via async ORM operations using the latest SQLModel.  \n4. Cache expensive queries in Redis for high throughput using the latest aioredis.  \n5. Perform streaming anomaly detection on incoming metrics using the latest river library.  \n6. Enable file uploads of detailed reports to S3-compatible storage via the latest minio SDK.  \n\nDeliverables:  \n\u2022 A numbered list of required Python libraries (with versions as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":20,"modules":["fastapi","websockets","sqlmodel","asyncpg","aioredis","river","minio","uvicorn","python-multipart","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a real-time collaborative note-taking service that supports user auth, live updates, AI summaries, PDF export, and caching using the latest Python web stack.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST API for user signup\/login with FastAPI (latest) using OAuth2 password flow.\n2. Persist users and notes in PostgreSQL via SQLModel (latest async features).\n3. Create a WebSocket endpoint broadcasting live note edits using the newest websockets library.\n4. Add an endpoint `\/summarize` that calls an external AI summarization API via httpx\u2019s HTTP\/3 client (latest).\n5. Provide a `\/export\/{note_id}` endpoint that generates a PDF of the note using Playwright Python (latest).\n6. Cache active JWT tokens in Redis Streams using aioredis v2.x.\n7. Emit structured JSON logs via loguru (latest advanced configuration).\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- A complete, runnable Python 3.11+ codebase under 150 lines, with clear comments indicating where each external library is used.","question_index":9,"modules":["fastapi","uvicorn","sqlmodel","asyncpg","websockets","httpx","playwright","aioredis","loguru","python-jose","passlib","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: As a senior software engineer, build a modular web application that authenticates users with WebAuthn passkeys, streams real-time stock data, generates AI trading signals, processes payments, and delivers interactive dashboards.\n\nFunctional Requirements:\n1. Implement user authentication via WebAuthn using the latest pywebauthn library (as of 2025-05-06) with hardware security key support.\n2. Ingest and stream real-time stock price data over WebSockets using FastAPI and the latest Starlette HTTP\/2 push features.\n3. Generate AI-driven trading signals leveraging the latest OpenAI Python SDK\u2019s new function-calling interface.\n4. Integrate secure payment processing with the latest Stripe Python SDK supporting Payment Element and webhook handling.\n5. Serve interactive dashboards using the latest Plotly Dash or Reflex library for live data visualization and client-side callbacks.\n6. Add structured logging and Prometheus metrics using structlog and prometheus-client.\n\nDeliverables:\n- A concise mapping of each functional requirement to its implementation approach.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used and using the latest libraries available as of 2025-05-06.","question_index":56,"modules":["httpx","structlog","prometheus_client","prometheus_fastapi_instrumentator","stripe","openai","reflex","pywebauthn","jwt"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-7-sonnet-20250219","question":"You are a senior software engineer tasked with building a minimal Python web service for text ingestion, semantic search, and AI-powered summarization using the latest libraries.\n\nFunctional Requirements:\n1. HTTP API: use the latest Starlite framework (as of 2025-05-06) to expose async endpoints.\n2. Text Storage: on POST \/submit, accept JSON {\u201ctext\u201d: \u2026}, store original text and metadata in Redis via the latest redis-om Python client with async HashModel.\n3. Embedding: upon submission, generate a vector embedding using the latest ChromaDB Python client\u2019s async API and upsert into its vector store.\n4. Semantic Search: implement GET \/search?query=\u2026 that computes a query embedding via ChromaDB and returns top 3 matching texts from Redis.\n5. Summarization: implement GET \/summarize\/{id} to retrieve stored text by ID and produce a concise summary using the latest llama-cpp-python streaming API.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries (as of 2025-05-06).  \n\u2022 A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments indicating where and how each external library is used.","question_index":45,"modules":["starlite","redis-om","langchain-chroma","llama-cpp-python","pydantic","httpx","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-7-sonnet-20250219","question":"Build a real-time analytics microservice that lets authenticated users ingest sales data, stores it in PostgreSQL, streams live updates to connected clients, and generates PDF reports with embedded charts.\n\n1. Implement OAuth2 JWT authentication using the latest fastapi-jwt-auth as of May 6, 2025.  \n2. Create async CRUD endpoints for sales records with SQLModel and asyncpg.  \n3. Broadcast new sales events over WebSocket using python-socketio (latest).  \n4. Generate interactive charts in HTML via Plotly (latest) and convert them to PDF using WeasyPrint (latest).  \n5. Expose generated PDFs via an endpoint, serving files asynchronously with aiofiles.  \n\nDeliverables:  \n- A numbered confirmation of the functional requirements above.  \n- A single Python 3.11+ file under 150 lines\u2014complete, runnable code\u2014with clear comments indicating where each external library is used. Use the latest versions of all third-party libraries as of today\u2019s date.","question_index":26,"modules":["aiofiles","jwt","plotly","socketio","fastapi","pydantic","sqlmodel","weasyprint","uvicorn","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-7-sonnet-20250219","question":"You are a senior software engineer tasked with architecting a small, high-performance async web application using only the latest Python libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Litestar framework (with native HTTP\/3 support) to define async REST endpoints for user signup and profile retrieval.  \n2. Integrate the latest Piccolo-ORM to define a PostgreSQL \u201cusers\u201d table and perform async CRUD operations.  \n3. Implement real-time notifications via WebSocket using the latest python-socketio in asyncio mode.  \n4. Schedule a daily summary email job with APScheduler\u2019s new AsyncIOScheduler and send mail via an async SMTP library.  \n5. Fetch external data during signup from a third-party API using HTTPX\u2019s HTTP\/2 client.\n\nDeliverables:\n\u2022 A requirements.txt listing exact package names and latest versions.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":29,"modules":["litestar","piccolo","piccolo-api","python-socketio","websockets","apscheduler","httpx","aiosmtplib","python-dotenv","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are tasked with building a real-time collaborative task management web service for enterprise teams.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to expose HTTP endpoints and WebSocket routes, leveraging its new dependency injection and WebSocket lifetime management.\n2. Configure an async PostgreSQL database using the latest Tortoise-ORM with its auto-migration CLI and Pydantic model integration.\n3. Broadcast real-time task updates to connected clients via the latest python-socketio in async mode.\n4. Implement OAuth2 authentication with JWT access and refresh tokens using the latest Authlib, including token rotation.\n5. Cache sessions and publish update events using the latest aioredis async Redis client.\n6. Schedule and execute background jobs (e.g., daily summaries) with the latest APScheduler async support.\n7. Instrument request tracing and logging using the latest OpenTelemetry Python SDK, exporting to console.\n8. Auto-generate interactive API docs using FastAPI\u2019s built-in Swagger UI and Redoc.\n\nDeliverables:\n\u2022 A bullet list of selected libraries (with version numbers).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":15,"modules":["fastapi","tortoise-orm","python-socketio","authlib","aioredis","apscheduler","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","python-jose","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Senior software engineer tasked with developing a real-time GraphQL web service that validates incoming event data, persists it to PostgreSQL, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI web framework) as of 2025-05-06 to implement an async HTTP POST endpoint at `\/events`.\n2. Validate incoming JSON payloads with Pydantic v2 (latest) models, including at least one custom field validator.\n3. Persist validated events to PostgreSQL using the latest Prisma Python client (type-safe generated models and migrations).\n4. Expose a GraphQL API with the latest Strawberry GraphQL library, supporting queries for event history and subscriptions for real-time updates.\n5. Configure WebSocket support in Starlite to broadcast new events to all GraphQL subscription clients.\n\nDeliverables:\n- A brief recap of the functional requirements.\n- Complete, runnable Python 3.11+ code under 150 lines that uses the latest versions of each library as of 2025-05-06, with clear comments marking where each external library is imported and utilized.","question_index":22,"modules":["starlite","pydantic","prisma","strawberry-graphql","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are a senior software engineer building a high-performance real-time IoT analytics backend.\n\nFunctional Requirements:\n1. Expose an asynchronous HTTP API using FastAPI with WebSocket support to ingest and broadcast live sensor readings.\n2. Persist incoming data in PostgreSQL via SQLModel\u2019s async ORM and validate payloads with Pydantic v2 models.\n3. Enforce per-device rate limiting using Redis Streams and aioredis, leveraging RedisJSON for overflow handling.\n4. Secure all endpoints with OAuth2 passwordless login via FastAPI-Users, sending one-time codes.\n5. Schedule background aggregation jobs with Celery (beat scheduler) to compute rolling metrics and push updates over WebSockets.\n6. Provide a paginated historical metrics REST endpoint, caching hot queries in Redis for low-latency responses.\n\nDeliverables:\n- A list of required external libraries pinned to the latest versions available as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is utilized.","question_index":27,"modules":["aioredis","redis","celery","fastapi","fastapi_limiter","fastapi_users","fastapi_cache","pydantic","rejson","sqlalchemy","sqlmodel"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: As a senior software engineer, you must build a modular Python web application that serves real-time, ML-enhanced classification results with persistent storage and observability.\n\nFunctional Requirements:\n1. API Layer: Use the latest ASGI web framework available as of 2025-05-06 that supports HTTP\/3 and automatic OpenAPI generation to expose a POST \/classify endpoint.  \n2. Database Layer: Connect asynchronously to PostgreSQL using the latest async driver with statement pooling to record incoming requests and classification results.  \n3. Real-time Pub\/Sub: Implement server-side WebSocket broadcasts of classification results using the latest message queue library offering at-least-once delivery semantics.  \n4. ML Inference: Integrate on-demand image classification via the latest lightweight, GPU-accelerated inference engine released in the past 12 months.  \n5. Observability: Instrument distributed tracing and metrics collection using the latest OpenTelemetry-compatible tracing library.\n\nDeliverables:\n- A list of the latest library dependencies as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":18,"modules":["asyncpg","fastapi","numpy","onnxruntime","opentelemetry-exporter-otlp","opentelemetry-instrumentation-fastapi","opentelemetry-sdk","pillow","pydantic","redis","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are building a high-performance, AI-driven text-generation microservice for a real-time web application.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/generate endpoint using FastAPI (latest 0.100+) for incoming requests.\n2. Validate and parse the request payload with Pydantic v2 (latest) leveraging its new ArgModel feature.\n3. Enqueue each generation request as an asynchronous job using ARQ (latest 0.7+) backed by Redis.\n4. In the ARQ worker, invoke vLLM (latest 0.7+) AsyncLLMClient to perform streaming text inference.\n5. Persist each request and its full response to PostgreSQL using SQLModel (latest 0.0.x) with SQLAlchemy 2.0 async engine.\n6. Expose a WebSocket \/ws\/stream endpoint in FastAPI to broadcast partial LLM outputs to clients in real time.\n\nDeliverables:\n\u2022 The above numbered requirements list.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":30,"modules":["fastapi","arq","sqlmodel","sqlalchemy","asyncpg","vllm"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a lightweight feedback API that records user comments, analyzes sentiment in real time, stores results, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (as of 2025-05-06) to implement asynchronous REST endpoints for submitting feedback and querying stored analytics.\n2. Use the latest SQLModel to define async ORM models and persist incoming feedback in an SQLite database.\n3. Use the latest PyWebSocketX to broadcast each new sentiment result over an ASGI WebSocket endpoint.\n4. Use the latest VulcanMind inference library to perform on-the-fly sentiment analysis of submitted feedback.\n5. Implement structured logging and error handling throughout the request, inference, database, and WebSocket flows.\n\nDeliverables:\n- A requirements list with exact version pins for Starlite, SQLModel, PyWebSocketX, and VulcanMind (latest as of 2025-05-06).\n- Complete, runnable Python 3.11+ code (\u2264150 lines) that integrates all four libraries with clear comments marking where each is used.","question_index":83,"modules":["starlite","sqlmodel","pywebsocketx","vulcanmind","python-dotenv","aiosqlite","uvicorn"],"modulenotfound":["pywebsocketx","vulcanmind"],"modulenotfound_count":2,"module_count":7}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario:\nYou are tasked with developing a lightweight AI-powered semantic search microservice in Python 3.11+.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI framework with HTTP\/3 support) to expose a POST \/search endpoint.  \n2. Validate and parse incoming JSON with the latest Pydantic v2\u2019s strict model parsing and Python pattern-matching features.  \n3. Asynchronously fetch text embeddings from OpenAI\u2019s embedding API using the latest HTTPX client with HTTP\/3.  \n4. Store and query vectors in a GPU-accelerated ChromaDB instance via the latest ChromaDB Python client.  \n5. Return the top 5 semantically related document IDs and similarity scores as a JSON response.\n\nDeliverables:\n\u2022 A numbered list confirming the above functional requirements.  \n\u2022 A single, complete, runnable Python 3.11+ source file (\u2264150 lines) that implements the microservice, with clear comments indicating where each external library is used.","question_index":3,"modules":["starlite","dotenv","httpx","chromadb","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a real-time HTTP\/3-powered AI chat service that streams user messages, generates LLM responses, persists embeddings in a vector DB, and exposes chat history via GraphQL.\n\nFunctional Requirements:\n1. HTTP\/3 streaming: use the latest aioquic (as of 2025-05-06) to handle bidirectional HTTP\/3 connections for chat.\n2. On-device LLM inference: integrate the latest llama-cpp-python for quantized model loading and response generation.\n3. Vector storage: use the latest chromadb Python client to embed each message (via llama-cpp-python\u2019s embed API) and store\/retrieve vectors.\n4. GraphQL API: implement an async GraphQL schema with the latest strawberry to query chat history from ChromaDB.\n5. Async orchestration: ensure end-to-end async I\/O under Python 3.11+, coordinating the QUIC server, LLM, and DB.\n6. Code constraints: keep the complete solution under 150 lines, include clear comments marking where each external library is used.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of aioquic, llama-cpp-python, chromadb, and strawberry (latest as of 2025-05-06)  \n\u2022 A single Python 3.11+ script (<150 lines) with runnable code and comments identifying each external library usage.","question_index":79,"modules":["aioquic","llama-cpp-python","chromadb","strawberry-graphql","uvloop","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-7-sonnet-20250219","question":"You are a Senior Software Engineer building a high-throughput, real-time web application that ingests user events, enriches them with AI, persists outcomes, and serves analytics via a modern dashboard.\n\nFunctional Requirements:\n1. Implement JWT-based user authentication using the latest async web framework with schema-first validation (e.g., HTTP\/3 support).\n2. Expose a WebSocket endpoint for real-time event ingestion using the latest WebSocket library with per-message compression.\n3. Perform fully async CRUD on PostgreSQL using the latest ORM with native asyncio support and advanced connection pooling.\n4. Offload heavy computations to background workers via the latest task queue library supporting asyncio and result backends.\n5. Integrate an AI text-classification pipeline using the latest AI inference library with streaming outputs.\n6. Instrument the service with distributed tracing using the latest observability library supporting OpenTelemetry protocols.\n\nDeliverables:\n\u2022 requirements.txt listing the latest available versions of all dependencies (as of May 6, 2025).  \n\u2022 A single Python 3.11+ file under 150 lines that is complete and runnable, with clear comments marking where each external library is used.","question_index":12,"modules":["fastapi","pydantic","jwt","passlib","sqlalchemy","asyncpg","celery","torch","transformers","opentelemetry-api","opentelemetry-sdk","opentelemetry-exporter-otlp","python-multipart","aioquic","python-jose","bcrypt","redis","accelerate","opentelemetry-instrumentation-fastapi"],"modulenotfound":[],"modulenotfound_count":0,"module_count":19}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a real-time collaborative note-taking web application with authentication, persistence, caching, live updates, and scheduled cleanup.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (as of today) for note creation, retrieval, update, and deletion.\n2. Secure endpoints with JWT authentication using the latest PyJWT.\n3. Persist notes in a relational database via the latest SQLModel ORM, defining Note and User models.\n4. Enable real-time note synchronization between clients using WebSockets with the latest python-socketio.\n5. Cache frequently accessed note metadata in Redis using the latest redis-om to reduce DB load.\n6. Schedule automatic archiving of notes older than 30 days using the latest APScheduler.\n7. Document all endpoints with OpenAPI and include example request\/response schemas.\n\nDeliverables:\n- A numbered list of all third-party libraries (with versions) you\u2019ve chosen.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear comments indicating where each external library is used.","question_index":69,"modules":["fastapi","pyjwt","sqlmodel","socketio","redis-om","apscheduler","uvicorn","python-multipart","passlib"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: As a senior software engineer, build a high-performance Python web application integrating real-time streaming, GraphQL, async ORM, JWT security, and caching using the latest third-party libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. HTTP & WebSocket server with the latest Litestar, utilizing its new streaming response API for real-time event delivery.\n2. GraphQL API with the latest Strawberry-GraphQL, defining queries, mutations, and subscription streams for live data updates.\n3. Async database models and CRUD operations using the latest Piccolo ORM\u2019s Python 3.11+ async support and built-in migration tooling.\n4. JWT authentication via the latest python-jose, supporting secure login, token rotation, and injection of user context into requests.\n5. Redis-backed caching with the latest aioredis to cache GraphQL query results and automatically invalidate on data changes.\n6. Trigger real-time client notifications over WebSockets when database events occur, leveraging Litestar\u2019s WebSocket routing.\n\nDeliverables:\n\u2022 A numbered list of the final functional requirements.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":57,"modules":["litestar","strawberry-graphql","piccolo-orm","python-jose","aioredis"],"modulenotfound":["piccolo-orm"],"modulenotfound_count":1,"module_count":5}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Design a minimal real-time chat service combining HTTP endpoints, data validation, async ORM persistence, WebSocket broadcasting, and JWT security using the latest Python libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Starlite to expose POST \/register and GET \/messages endpoints, leveraging its advanced dependency injection system.\n2. Define and serialize request\/response models with the latest Pydantic v2, employing its new `model_serializer` API.\n3. Persist User and Message models to SQLite asynchronously with the latest Ormar ORM, utilizing its built-in async migration feature.\n4. Implement a `\/ws` WebSocket route using the latest websockets library for real-time message broadcasting to connected clients.\n5. Secure both HTTP routes and WebSocket connections with JWT authentication via the latest PyJWT release.\n\nDeliverables:\n- A concise mapping of each numbered requirement to the specific library and feature used.\n- Complete, runnable Python 3.11+ code under 150 lines total, with clear comments indicating where each external library is imported and applied.","question_index":21,"modules":["starlite","pydantic","ormar","databases","sqlalchemy","jwt"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are building a Python microservice that accepts user-uploaded text files, asynchronously generates summaries with a local LLM, and streams live progress to clients via WebSocket.\n\nFunctional Requirements:\n1. Implement a FastAPI app (use the latest FastAPI as of 2025-05-06) with an `\/upload` POST endpoint that accepts plain-text file uploads.  \n2. Upon upload, enqueue a background job using arq (latest version) to process the file asynchronously.  \n3. In the arq worker, load a local LLM via llama-cpp-python (latest version) using its new GPU offloading API to generate a summary.  \n4. Configure python-socketio (latest version) as an ASGI middleware in FastAPI to broadcast progress and final summary events to connected WebSocket clients.\n\nDeliverables:\n- A bullet list of the chosen third-party libraries with their exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":46,"modules":["fastapi","socketio","arq","llama-cpp-python","python-multipart","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build an end-to-end real-time customer feedback summarization service with live dashboard updates.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled JSON ingest endpoint using Litestar (latest as of 2025-05-06) with async support.\n2. Define a feedback model with Pydantic v2 and persist submissions into SQLite asynchronously via SQLModel (latest).\n3. On each new submission, call the OpenAI Python SDK (latest) asynchronously to generate a brief summary.\n4. Cache each summary in Redis via redis-om (latest) with a 1-hour TTL.\n5. Serve a live dashboard using Reflex (latest) that connects over WebSockets to display incoming summaries in real time.\n6. Use HTTPX (latest) with HTTP\/3 support for any external HTTP calls.\n\nDeliverables:\n\u2022 requirements.txt listing all dependencies pinned to the latest versions as of 2025-05-06  \n\u2022 A single Python 3.11+ script (under 150 lines) that implements the above, with clear comments indicating where and why each external library is used","question_index":61,"modules":["litestar","pydantic","sqlmodel","openai","redis-om","httpx","reflex"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Develop a Python 3.11+ web application that provides secure collaborative document management with GraphQL, real-time WebSocket updates, AI-powered summarization, and on-demand PDF export.\n\nFunctional Requirements:\n1. Implement a GraphQL API for document queries and mutations using the latest \u201cariadne\u201d library with subscription support.\n2. Enable OAuth2 login with passwordless WebAuthn using the latest \u201cauthlib\u201d and \u201cfido2\u201d libraries.\n3. Provide real-time document update notifications over WebSocket leveraging FastAPI\u2019s built-in WebSocket support.\n4. Schedule asynchronous AI summarization tasks using the latest \u201cllama-cpp-python\u201d library and \u201cAPScheduler\u201d for background job management.\n5. Generate PDF exports of documents on demand using the latest \u201cplaywright\u201d headless browser library in Python.\n6. Persist documents and user metadata in PostgreSQL via the latest \u201ctortoise-orm\u201d with native async and Pydantic v2 model support.\n\nDeliverables:\n- A brief requirements list naming each third-party library and its version.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used. Use the latest library versions available as of 2025-05-06.","question_index":88,"modules":["fastapi","uvicorn","ariadne","authlib","fido2","llama-cpp-python","apscheduler","playwright","tortoise-orm","pydantic","asyncpg","graphql-ws","websockets","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":14}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are designing a Python microservice for ingesting user text, storing embeddings, and providing real-time, rate-limited on-device summaries via WebSockets.\n\nFunctional Requirements:\n1. HTTP POST \/ingest  \n   - Validate payload (id: str, text: str) using Pydantic v2 root validators  \n   - Compute embeddings on-device with llama-cpp-python and store (id, embedding, text) in ChromaDB  \n2. HTTP GET \/query  \n   - Accept query text, validate with Pydantic v2  \n   - Compute embedding via llama-cpp-python, retrieve top-5 nearest documents from ChromaDB, return metadata  \n3. WebSocket \/summarize  \n   - Client sends document id  \n   - Fetch text from ChromaDB metadata  \n   - Stream back a summary using llama-cpp-python streaming API  \n   - Enforce token-per-second rate limiting via aiolimiter  \n4. Lifespan events  \n   - On startup, initialize ChromaDB client, Llama model, and aiolimiter  \n   - On shutdown, cleanly close any resources  \n\nDeliverables:\n- requirements.txt listing the latest available versions as of today of:\n  \u2022 starlite  \n  \u2022 pydantic v2  \n  \u2022 chromadb  \n  \u2022 llama-cpp-python  \n  \u2022 aiolimiter  \n- main.py: a single, runnable Python 3.11+ script (under 150 lines) implementing all above, with clear comments marking where each external library is used.","question_index":8,"modules":["starlite","pydantic","chromadb","llama-cpp-python","aiolimiter","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a serverless Python application enabling real-time geolocated chat with sentiment analysis, dynamic map visualization, and PDF session reporting.\n\nFunctional Requirements:\n1. Real\u2010time WebSocket chat using the latest Litestar (released in the last 12 months) with asynchronous message persistence via a modern async ORM.\n2. Interactive geospatial mapping of incoming messages on the client side using the latest Python geospatial visualization library released in the last 12 months.\n3. On-the-fly sentiment analysis of each message using the latest Python sentiment analysis library released in the last 12 months.\n4. PDF generation of the entire chat session\u2014including message timeline, geolocation markers, and sentiment scores\u2014using the latest Python PDF library released in the last 12 months.\n5. Deployment configuration for AWS Lambda using the latest Serverless Framework plugin and AWS Lambda Powertools (all released in the last 12 months).\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":97,"modules":["litestar","piccolo","nltk","plotly","reportlab","aws-lambda-powertools","websockets","serverless-python-requirements"],"modulenotfound":["serverless-python-requirements"],"modulenotfound_count":1,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are a senior software engineer tasked with building a real-time event analytics dashboard that ingests events, stores them, generates AI-driven summaries, caches counts, and streams updates to authenticated clients with full observability.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to implement a POST \/events REST endpoint for event ingestion.\n2. Validate incoming JSON payloads with the latest Pydantic v2 models.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise-ORM.\n4. Cache aggregated event counts in Redis via the latest aioredis.\n5. Generate streaming summaries of event batches with the latest OpenAI Python client.\n6. Stream real-time updates to clients over WebSocket using FastAPI.\n7. Secure all HTTP and WebSocket endpoints with JWT authentication via the latest fastapi-jwt-auth.\n8. Instrument HTTP, database, cache, and AI client calls with the latest OpenTelemetry for distributed tracing.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest library names and versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines integrating all subdomains, with clear comments indicating where each external library is used.","question_index":4,"modules":["fastapi","uvicorn","pydantic","tortoise-orm","asyncpg","aioredis","websockets","python-jose","python-multipart","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-aioredis","opentelemetry-instrumentation-asyncpg","opentelemetry-exporter-otlp","openai","fastapi-jwt-auth"],"modulenotfound":["opentelemetry-instrumentation-aioredis"],"modulenotfound_count":1,"module_count":17}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You\u2019re building a high-performance async task management API using the latest libraries in web framework, async ORM, caching, background processing, and observability as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement CRUD REST endpoints for a Task model using the latest starlite.\n2. Persist tasks in Postgres asynchronously with the latest Tortoise-ORM.\n3. Enqueue a background job on task creation using the latest Dramatiq with Redis broker.\n4. Cache GET \/tasks\/{id} responses in Redis via the latest redis.asyncio.\n5. Instrument all HTTP and database calls with OpenTelemetry using the latest opentelemetry-instrumentation-starlite.\n\nDeliverables:\n- A requirements.txt listing each external dependency with explicit versions.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":34,"modules":["starlite","tortoise-orm","dramatiq","redis-dramatiq","redis.asyncio","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-starlite","opentelemetry-exporter-otlp","uvicorn"],"modulenotfound":["redis-dramatiq","redis.asyncio","opentelemetry-instrumentation-starlite"],"modulenotfound_count":3,"module_count":10}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario  \nYou are building an AI-powered document search microservice that ingests user uploads, indexes them with a GPU-accelerated vector store, caches recent queries, and exposes HTTP\/3 endpoints for real-time search.  \n\nFunctional Requirements  \n1. HTTP API (Starlite, latest)  \n   1.1 POST \/upload: receive plain-text documents, validate with Pydantic.  \n   1.2 GET \/query?q=\u2026: return top-k results for query.  \n2. Document Ingestion (llama-index, latest)  \n   2.1 Stream tokenized embeddings upon upload.  \n3. Vector Store (ChromaDB, latest)  \n   3.1 Use GPU-backed indexing for fast similarity search.  \n4. Caching Layer (redis-om, latest)  \n   4.1 Cache each query + results with TTL and Pydantic models.  \n\nDeliverables  \n\u2022 A requirements.txt listing the latest versions (as of 2025-05-06) of all third-party libraries, each released within the past 12 months.  \n\u2022 Complete, runnable Python 3.11+ code (\u2264150 lines) implementing all requirements.  \n\u2022 Clear inline comments indicating where and how each external library is used.","question_index":47,"modules":["starlite","llama-index","chromadb","redis-om","pydantic","httpx","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are a senior engineer building a high-throughput job processing microservice with a modern async Python stack.  \n\nFunctional Requirements:  \n1. Expose a POST \/jobs endpoint using the latest Starlite (as of 2025-05-06) to accept JSON job submissions.  \n2. Validate incoming JSON payloads with the latest Pydantic v2 models.  \n3. Persist job records to PostgreSQL via the latest prisma-client-py async ORM.  \n4. Enqueue and process jobs in the background using the latest Taskiq queue.  \n5. Expose a GET \/jobs\/{id}\/status endpoint to retrieve current job status.  \n6. Ensure all I\/O is non-blocking and use Python 3.11+ async\/await throughout.  \n\nDeliverables:  \n\u2022 A requirements list specifying the exact latest versions of Starlite, Pydantic v2, prisma-client-py, and Taskiq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ source code under 150 lines.  \n\u2022 Clear inline comments marking where each external library is used.","question_index":99,"modules":["starlite","pydantic","prisma","taskiq","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a secure, real-time AI-powered chat web service.\n\nFunctional Requirements:\n1. Expose an async REST API using the latest FastAPI for user registration and login endpoints.  \n2. Implement JWT authentication with rotating refresh tokens via the latest Authlib.  \n3. Persist users and chat messages in SQLite using the latest SQLModel, leveraging its new async support.  \n4. Enable real-time chat notifications over WebSockets using the latest python-socketio and ASGI.  \n5. Integrate AI chat completions using the latest OpenAI Python client in an async background task.\n\nDeliverables:\n1. A requirements.txt listing exact, up-to-date package versions as of today\u2019s date.  \n2. A single Python 3.11+ file (<150 lines) containing complete, runnable code with clear comments marking where each external library is used.","question_index":91,"modules":["fastapi","uvicorn","sqlmodel","authlib","python-socketio","python-jose","python-multipart","openai","passlib","aiosqlite","socketio","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are a senior software engineer tasked with building an AI-powered collaborative document annotation API.\n\nFunctional Requirements:\n1. Secure user signup and login with JWT-based authentication using Starlite (latest) and Pydantic v2.\n2. REST endpoint to upload documents and persist metadata in Redis OM Python (latest).\n3. WebSocket route for real-time annotation broadcasting using Starlite\u2019s built-in WebSocket support.\n4. Background summarization of uploaded documents using llama-cpp-python (latest).\n\nDeliverables:\n- A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":1,"modules":["starlite","pydantic","redis-om","llama-cpp-python","passlib","uvicorn","python-jose"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are a senior engineer tasked with building a secure, high-throughput microservice for customer order ingestion, persistence, background processing, and caching.\n\nFunctional Requirements:\n1. Use FastAPI (latest as of 2025-05-06) with HTTP\/3 support to implement POST \/orders accepting JSON order data.\n2. Define an asynchronous Order model with SQLModel (latest) and connect to an async database engine (SQLite or PostgreSQL).\n3. Enqueue and process new orders via Arq (latest) background worker.\n4. Implement OAuth2 with JWT issuance and protected endpoints using Authlib (latest).\n5. Cache GET \/orders\/{id} responses in Redis using aiocache (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments marking where each external library is used.","question_index":68,"modules":["fastapi","uvicorn","sqlmodel","sqlalchemy","asyncpg","arq","authlib","aiocache","redis","pydantic","pydantic-extra-types","email-validator"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build an end-to-end Python web service that accepts data submissions, runs ML inference asynchronously, and pushes results back in real time.\n\nFunctional Requirements:\n1. Implement a POST \/submit endpoint using the latest FastAPI (ASGI, HTTP\/2) to accept JSON payloads.\n2. Validate and persist submissions into PostgreSQL via the latest SQLModel with asynchronous sessions.\n3. Dispatch an asynchronous classification task via the latest Celery (Redis broker) when new data arrives.\n4. In the Celery task, perform inference using the latest Hugging Face Transformers pipeline (e.g., text-classification).\n5. Cache inference results in Redis using the latest aioredis with a 5-minute TTL.\n6. Expose a WebSocket \/ws endpoint (FastAPI) to stream cached or newly computed results to subscribed clients.\n7. Instrument all HTTP requests and Celery tasks with the latest OpenTelemetry SDK for tracing.\n\nDeliverables:\n\u2022 requirements.txt listing \u201cthe latest\u201d versions of FastAPI, SQLModel, Celery, Redis, aioredis, Transformers, and OpenTelemetry as of 2025-05-06.  \n\u2022 A single Python 3.11+ file under 150 lines of runnable code. Include clear comments where each external library is used.","question_index":59,"modules":["fastapi","uvicorn","sqlmodel","asyncpg","celery","aioredis","transformers","opentelemetry-api","opentelemetry-sdk","opentelemetry-exporter-otlp","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-celery","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":13}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are tasked with building a real-time IoT analytics dashboard featuring anomaly detection, edge caching, vector search, and HTTP\/3 streaming.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of May 6, 2025) with HTTP\/3 and ASGI concurrency for API endpoints.\n2. Ingest IoT telemetry via real-time WebSocket streams using the latest websockets library.\n3. Perform transformer-based anomaly detection using the latest HuggingFace transformers and PyTorch 2.x.\n4. Store and query time-series embeddings in a vector database using the latest Pinecone Python client with HNSW indexing.\n5. Cache recent query results at the edge using the latest aioredis client with TTL invalidation.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the \u201clatest\u201d versions of all third-party libraries as of today.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":67,"modules":["fastapi","uvicorn","h2","websockets","transformers","torch","pinecone-client","aioredis","numpy","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a high-performance, AI-powered web dashboard for real-time analytics and user collaboration.\n\nFunctional Requirements:\n1. Web API: Use the latest FastAPI (as of 2025-05-06) with async endpoints and the new dependency-injection system.\n2. Real-Time Collaboration: Implement bidirectional WebSocket chat using python-socketio v5.x+ for live user updates.\n3. AI Integration: Leverage the latest llama-cpp-python (or equivalent) to compute embeddings on incoming messages.\n4. Database Persistence: Store users and message history asynchronously with SQLModel v0.7+ (Pydantic v2) on SQLite.\n5. Caching Layer: Cache AI embedding results in Redis via aioredis v3.x+ to minimize redundant computation.\n6. Security: Protect all endpoints with OAuth2 JWT authentication using Authlib v1.2+.\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all third-party libraries as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, with clear inline comments indicating where each external library is used.","question_index":58,"modules":["fastapi","uvicorn","socketio","llama-cpp-python","sqlmodel","aioredis","authlib","python-jose","python-multipart","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-7-sonnet-20250219","question":"You are a senior software engineer tasked with building a modern image-processing web service end-to-end.\n\nFunctional Requirements:\n1. Implement a REST endpoint in Litestar for image uploads that converts incoming images to AVIF using the latest pillow-avif-plugin.\n2. Asynchronously persist image metadata (filename, upload timestamp, AVIF URL, dimensions) in MongoDB via the latest Beanie ODM.\n3. Index and cache metadata in Redis for fast lookup using the latest Redis-OM Python library.\n4. Expose a GraphQL API via the latest Litestar-GraphQL plugin to query images with filtering, pagination, and type validation.\n5. Broadcast real-time upload notifications over WebSocket connections using Litestar\u2019s built-in WebSocket support.\n\nDeliverables:\n- A concise list of the five functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines, using the latest Litestar, Litestar-GraphQL, Beanie, Redis-OM, and pillow-avif-plugin (as of May 6, 2025), with clear comments indicating where each external library is used.","question_index":89,"modules":["litestar","motor","beanie","redis-om","pillow","pillow-avif","strawberry-graphql"],"modulenotfound":["pillow-avif"],"modulenotfound_count":1,"module_count":7}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Develop a high-performance asynchronous web service for ingesting user datasets, persisting metadata, launching background processing jobs, and tracing all operations end-to-end.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/upload endpoint using Starlite (latest) with async file streaming and dependency injection.\n2. Define and migrate a PostgreSQL table via Piccolo ORM (latest), storing upload metadata and task IDs, leveraging its GraphQL auto-schema feature.\n3. Enqueue and process tasks with Asynq (latest) in Redis, using priority queues, retry policies, and task result callbacks.\n4. Instrument incoming HTTP requests and background tasks with OpenTelemetry (latest), enabling W3C trace-context propagation across components.\n5. Provide an HTTP GET \/status\/{task_id} endpoint that queries Piccolo ORM for real-time task status.\n\nDeliverables:\n\u2022 The numbered functional requirements list (as above)  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, using the latest versions of all libraries as of 2025-05-06, with clear comments indicating where each external library is used.","question_index":43,"modules":["starlite","piccolo","asynq","opentelemetry","uvicorn"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":5}
{"model":"claude-3-7-sonnet-20250219","question":"As a senior software engineer, you are tasked with building a secure, real-time collaborative whiteboard microservice using cutting-edge Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3 API using the latest FastAPI (as of 2025-05-06) and uvicorn[standard] for low-latency streaming.\n2. Implement OAuth2 authorization with JWTs and refresh-token rotation using the latest Authlib.\n3. Persist whiteboard sessions and user metadata in PostgreSQL via the latest SQLModel with an async engine.\n4. Broadcast drawing events in real time over WebSockets using the latest websockets library with room-based pub\/sub.\n5. Vectorize stroke data into SVG on the fly using the latest Shapely or pyvips for advanced geometry operations.\n6. Upload periodic session snapshots to AWS S3 using the latest aioboto3 with asynchronous multipart uploads.\n\nDeliverables:\n- A requirements.txt listing the latest available versions of all external libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, fully runnable, with clear comments indicating where each third-party library is used.","question_index":25,"modules":["fastapi","uvicorn","authlib","sqlmodel","shapely","aioboto3","websockets","sqlalchemy","asyncpg","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a real-time sentiment analysis dashboard that fetches trending Twitter topics, analyzes sentiment, stores results, and serves updates via WebSockets.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to create both REST and WebSocket endpoints, leveraging its built-in asyncio and WebSocket broadcast features.\n2. Use the latest HTTPX to asynchronously fetch trending topics from the Twitter API v2 over HTTP\/3.\n3. Use the latest Hugging Face transformers to perform sentiment analysis via a pretrained pipeline.\n4. Use the latest SQLModel to define async SQLite models and persist topics with sentiment scores.\n5. Use the latest Jinja2 to render a minimal HTML dashboard for initial page load before WebSocket updates.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the latest versions of FastAPI, HTTPX, transformers, SQLModel, and Jinja2 as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":36,"modules":["fastapi","uvicorn","httpx","transformers","sqlmodel","jinja2","python-dotenv","aiosqlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Senior Software Engineer: You are developing an intelligent IoT monitoring dashboard that integrates real-time ingestion, streaming anomaly detection, WebSocket updates, dynamic UI rendering, and cloud infrastructure provisioning.\n\nFunctional Requirements:\n1. Build an ASGI web server using the latest ASGI framework (as of 2025-05-06) with HTTP\/2 and WebSocket endpoints for real-time data streaming.\n2. Connect asynchronously to an MQTT broker using the latest Python MQTT client library (as of 2025-05-06), subscribe to sensor topics, and relay messages to the server.\n3. Implement real-time anomaly detection on incoming sensor data using the latest Python anomaly detection library supporting incremental or deep learning.\n4. Serve a live, interactive front-end auto-generated from Python to JavaScript using the latest Python-to-JS UI framework.\n5. Provision and deploy all components via infrastructure-as-code using the latest Python IaC SDK on a major cloud provider.\n\nDeliverables:\n\u2022 Restate the numbered functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code integrating all five requirements, under 150 lines.  \n\u2022 Clear inline comments indicating where and how each external \u201clatest\u201d library (as of 2025-05-06) is imported and used.","question_index":49,"modules":["starlette","uvicorn","amqtt","pyod","numpy","reactive","pulumi","pulumi-aws"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a high-performance asynchronous message processing service for real-time sentiment analysis, storage, notification, and search.\n\nFunctional Requirements:\n1. Implement an async HTTP API using the latest FastAPI (as of May 6, 2025) to accept user messages.\n2. Define and persist message models in PostgreSQL via the latest SQLModel ORM.\n3. Schedule and execute sentiment analysis in the background using the latest Dramatiq with RabbitMQ.\n4. After analysis, update the database and push real-time notifications to connected clients over WebSockets.\n5. Index messages and sentiment scores into Elasticsearch using the latest official Python client.\n6. Provide a search endpoint to query stored messages by keyword and sentiment.\n\nDeliverables:\n\u2022 A numbered list of the \u201clatest\u201d third-party libraries (with versions) used for each sub-domain.  \n\u2022 Complete, runnable Python 3.11+ application code under 150 lines, with clear comments indicating where each external library is used.","question_index":48,"modules":["fastapi","sqlmodel","dramatiq","pika","elasticsearch","nltk","uvicorn","pydantic","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a microservice that accepts text input, asynchronously generates summaries, persists data, and provides real-time metrics.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI 3, HTTP\/3 support) to implement POST \/summaries accepting JSON { \"text\": \"\u2026\" }, enqueue a background job via Arq, and return a job ID.\n2. Persist requests and summaries in SQLite asynchronously with the latest SQLModel (Pydantic V2 model support).\n3. Offload summarization jobs to Redis using the latest Arq, invoking the latest LangChain to call OpenAI for a concise summary.\n4. Expose GET \/summaries\/{id} to retrieve stored summaries from the database.\n5. Instrument request counts and job durations with the latest prometheus_client, exposing metrics on GET \/metrics.\n\nDeliverables:\n- A bulleted list of chosen \u201clatest\u201d libraries with exact version numbers.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear inline comments indicating where each external library is used.\n- Self-contained implementation: after installing dependencies, code runs without further setup.","question_index":95,"modules":["starlite","sqlmodel","arq","langchain","prometheus_client","openai","aiosqlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Develop a modern, high-performance AI-powered web service for real-time text processing.\n\nFunctional Requirements:\n1. Build a JSON REST API with Litestar using its latest dependency-injection and ASGI lifespan hooks.\n2. Integrate Tortoise ORM (async) to manage a PostgreSQL \u201cjobs\u201d table with migrations.\n3. Use llama-cpp-python to generate text embeddings or responses via a local LLaMA model.\n4. Expose a WebSocket endpoint with python-socketio (asyncio mode) for live client notifications.\n5. Cache recent embeddings in Redis using redis-py\u2019s asyncio client.\n6. Schedule background tasks (e.g., stale job cleanup) leveraging asyncio\u2019s latest task groups.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":60,"modules":["litestar","tortoise-orm","asyncpg","llama-cpp-python","python-socketio","redis","aerich","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are a senior software engineer building a real-time social media monitoring application that ingests tweets, analyzes sentiment, classifies images, displays live charts, and deploys as a serverless microservice using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Ingest tweets in real time from Twitter API v2 using the latest HTTPX async streaming client.\n2. Perform NLP sentiment analysis on tweet text with the latest Hugging Face Transformers pipeline.\n3. Classify embedded images using the latest Ultralytics YOLOv8 model.\n4. Render an interactive, live-updating dashboard using the latest Plotly Dash framework.\n5. Scaffold serverless deployment with the latest AWS Lambda Powertools for Python.\n\nDeliverables:\n- A requirements list of all external dependencies.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":98,"modules":["httpx","tweepy","transformers","torch","ultralytics","plotly","dash","dash-bootstrap-components","aws-lambda-powertools","pillow"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: A knowledge management platform must allow users to upload PDFs, index their content, and perform real-time similarity searches via a high-performance API.\n\nFunctional Requirements:\n1. Implement a PDF upload REST endpoint using the latest unstructured-sdk to extract text.\n2. Use the latest ChromaDB Python client to generate and store embeddings for each document.\n3. Provide a search REST endpoint that queries ChromaDB for top-k similar documents given a text query.\n4. Create a real-time WebSocket over HTTP\/3 endpoint using the latest aioquic to stream incremental search results.\n5. Assemble the application as an asynchronous service using the latest Starlite web framework.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":6,"modules":["starlite","unstructured","chromadb","aioquic","uvicorn","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: As a senior software engineer, build an asyncio-based Python microservice showcasing HTTP\/3 REST, GraphQL, async ORM, real-time WebSockets, and scheduled tasks using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST endpoint at `\/api\/items` using FastAPI (latest) and Hypercorn (latest) with HTTP\/3 support.\n2. Expose a GraphQL API at `\/graphql` using Strawberry (latest) with asynchronous resolvers fetching from the database.\n3. Integrate PostgreSQL via Tortoise ORM (latest), define an `Item` model, and apply programmatic migrations on startup.\n4. Provide WebSocket notifications at `\/ws\/updates` using python-socketio (latest), broadcasting item creation and deletion events.\n5. Schedule an hourly cleanup job with APScheduler (latest) in asyncio mode to remove `Item` records older than 7 days.\n\nDeliverables:\n- A `requirements.txt` listing the latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is imported and utilized.","question_index":66,"modules":["fastapi","hypercorn","strawberry-graphql","tortoise-orm","asyncpg","python-socketio","apscheduler"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are building a high-performance Python backend that ingests user articles, generates AI-powered summaries, stores and caches results, and exposes them via REST, GraphQL, and real-time channels with background processing.\n\nFunctional Requirements:\n1. Accept article submissions over a REST API using FastAPI and validate payloads with Pydantic v2.  \n2. Offload summarization to a background worker with Dramatiq using Redis as a broker.  \n3. Summarize articles via the latest OpenAI Python SDK and cache summaries with aiocache.  \n4. Persist articles and summaries in SQLite using SQLModel\u2019s async ORM features.  \n5. Expose stored summaries through a GraphQL endpoint implemented with Strawberry GraphQL.  \n6. Broadcast new summary notifications to connected clients over WebSockets with python-socketio.  \n\nDeliverables:\n\u2022 List of the latest third-party libraries (as of today) and their install commands.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":63,"modules":["fastapi","uvicorn","pydantic","dramatiq","redis","openai","aiocache","sqlmodel","aiosqlite","strawberry-graphql","python-socketio","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Develop an internal Python microservice where authenticated users can submit text to be summarized by an LLM and receive real-time notifications via WebSockets.\n\nFunctional Requirements:\n1. Implement an asynchronous REST API using the latest Starlite framework (2025-05-06) leveraging its dependency injection and lifespan event system.\n2. Integrate OAuth2 Authorization Code Flow for user authentication with Authlib\u2019s async client libraries released in the last 12 months.\n3. Persist users, submissions, and summaries in PostgreSQL using the latest Piccolo ORM with async schema migrations and connection pooling.\n4. Summarize submitted text by calling OpenAI\u2019s GPT-4 Turbo via the newest openai Python SDK and its function-calling feature.\n5. Broadcast summary completion events over WebSockets using a modern Python websockets library (with HTTP\/2 support) released in the past year.\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":82,"modules":["starlite","authlib","piccolo","openai","websockets","python-dotenv","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-7-sonnet-20250219","question":"You are tasked with building a real-time analytics web dashboard that ingests streaming data, processes it, summarizes it with AI, secures user access, and visualizes results dynamically using the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Implement a REST API in Python 3.11+ with FastAPI (HTTP\/3 support via Hypercorn) to accept JSON event streams.\n2. Broadcast processed insights in real time over WebSocket using python-socketio.\n3. Buffer incoming events and generate streaming AI summaries using the latest LangChain and OpenAI Python SDK.\n4. Perform real-time aggregation and lazy analytics on events with Polars.\n5. Produce dynamic Plotly charts of key metrics and serve them as JSON for front-end rendering.\n6. Secure all endpoints and WebSocket connections with JWT-based auth via fastapi-users.\n\nDeliverables:\n\u2022 A numbered list of the above functional requirements.  \n\u2022 A single Python 3.11+ file (under 150 lines) that fulfills all requirements, complete and runnable. Include clear comments indicating where and how each external library is used.","question_index":84,"modules":["polars","plotly","fastapi","fastapi-users","hypercorn","langchain","openai","python-socketio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are a senior software engineer tasked with building a next-generation real-time collaborative data analytics dashboard. Use the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a FastAPI server with a WebSocket endpoint for live data updates using the latest fastapi[full].\n2. Expose a type-safe GraphQL API at `\/graphql` using Strawberry with async schema stitching.\n3. Integrate AI-driven insights by generating embeddings on incoming data with llama-cpp-python.\n4. Perform vector similarity search over embeddings via the Weaviate Python client\u2019s async API.\n5. Secure all endpoints with passwordless authentication using py_webauthn for WebAuthn flows.\n6. Containerize the application using docker-py to build images and dynamically generate a docker-compose config.\n7. Instrument the app with async Prometheus metrics using prometheus-client.\n\nDeliverables:\n- A list of required libraries (with versions) using the latest releases as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":72,"modules":["fastapi","strawberry-graphql","llama-cpp-python","weaviate-client","webauthn","docker","prometheus-client","uvicorn","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Develop a real-time collaborative note-taking web application with AI-powered summarization.\n\nFunctional Requirements:\n1. Implement user authentication via OAuth2 password flow using the latest FastAPI features for secure login.  \n2. Expose a WebSocket endpoint for real-time CRDT-based document state synchronization using the newest FastAPI WebSocket enhancements.  \n3. Persist users and documents in PostgreSQL via the latest Piccolo ORM async API with JSONB indexing for rich querying.  \n4. Cache live presence and session metadata in Redis using aioredis with RedisJSON support for low-latency reads\/writes.  \n5. Summarize document edits on demand using the latest LangChain library and ChatOpenAI (GPT-4) integration.  \n6. Serve a minimal HTML frontend via Jinja2 templates that connects to the WebSocket and displays live updates.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of each dependency as of 2025-05-06.  \n\u2022 main.py: Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":52,"modules":["fastapi","piccolo","python-jose","python-multipart","aioredis","rejson","langchain","langchain-openai","jinja2","uvicorn","websockets","pydantic","y-py","y-crdt","python-dotenv"],"modulenotfound":["y-crdt"],"modulenotfound_count":1,"module_count":15}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Senior Software Engineer, you are tasked with building a prototype real-time collaborative note-taking service using only the latest Python libraries released in the last 12 months.\n\nFunctional Requirements:\n1. Develop an asynchronous REST API for note CRUD operations using the latest Litestar framework.\n2. Persist notes in a SQL database via a newly released async ORM library.\n3. Cache frequent note queries in Redis with a recently published async Redis client.\n4. Enable real-time collaboration via WebSockets using a modern Python WebSocket library launched within the last year.\n5. Expose an AI-powered summarization endpoint using the latest OpenAI Python client.\n6. Render an interactive HTML dashboard showing active collaborators with a new Python-to-JS visualization tool.\n7. Instrument distributed tracing and structured logging using a cutting-edge observability library released in the past 12 months.\n\nDeliverables:\n- requirements.txt listing only libraries created in the last 12 months with exact versions.\n- A single Python 3.11+ file under 150 lines containing the complete, runnable application, with clear comments indicating where each external library is used.","question_index":41,"modules":["litestar","sqlmodel","redis-om","websockets-latest","openai","litestar-vite","structlog","aiosqlite"],"modulenotfound":["websockets-latest"],"modulenotfound_count":1,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario:\nCreate an AI-powered article summarization service that accepts user-submitted text, stores it, generates summaries asynchronously via an AI API, caches results, and streams them back to clients in real time.\n\nFunctional Requirements:\n1. Implement a REST API using FastAPI (latest) with endpoints POST \/articles to submit text and GET \/stream\/{article_id} for server-sent events.\n2. Persist submissions in a database using SQLModel (latest) with an async SQLite or PostgreSQL engine.\n3. Orchestrate a background job queue using Celery (latest) with Redis as broker to process newly submitted articles.\n4. Perform summarization using the OpenAI Python SDK (latest) and its chat completion endpoint.\n5. Cache generated summaries in Redis via aioredis (latest) with a configurable TTL.\n6. Stream summaries to connected clients in real time using server-sent events via sse-starlette (latest).\n\nDeliverables:\n\u2022 A list of all third-party libraries (with versions pinned to the latest as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":64,"modules":["fastapi","uvicorn","sqlmodel","celery","redis","aioredis","openai","sse-starlette","python-dotenv","httpx","sqlalchemy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-7-sonnet-20250219","question":"You are a senior software engineer building a high-performance, real-time analytics web service for IoT sensor data.  \n\nFunctional Requirements:  \n1. Expose HTTP REST ingest endpoints using the latest Nebula framework (released 2024-06) with its async dependency-injection.  \n2. Persist incoming readings to PostgreSQL via StarDB ORM (v1.0+, released 2024-08), leveraging async transactions and auto-migration.  \n3. Secure all endpoints with OAuth2 PKCE flows using AuthFusion (released 2024-12).  \n4. Broadcast processed data updates to dashboard clients via WaveSocket (released 2024-10) server-clustering.  \n5. Schedule hourly aggregation jobs using ChronoTask (released 2024-11) cron-style scheduler.  \n\nDeliverables:  \n\u2022 A requirements.txt listing the latest versions of all libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":92,"modules":["nebula","stardb","authfusion","wavesocket","chronotask","python-dotenv","uvicorn","pytest"],"modulenotfound":["nebula","stardb","authfusion","wavesocket"],"modulenotfound_count":4,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are a senior software engineer building a high-throughput chat summarization microservice in Python 3.11+ using only the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Real-time WebSocket ingestion: implement a `\/ws` endpoint using fastapi-socketio (latest) with session storage in Redis via `redis.asyncio`.\n2. AI summarization: enqueue incoming messages in Arq (latest) and process them with the OpenAI Python SDK (latest) calling GPT-4 Turbo for summaries.\n3. Async data persistence: store raw messages and summaries in PostgreSQL using SQLModel + encode\/databases with the asyncpg driver.\n4. Dynamic admin dashboard: expose `\/admin` rendering Jinja2 templates enhanced with HTMX via fastapi-htmx (latest).\n5. Email alerts: send summary notifications asynchronously using FastAPI-Mail (latest).\n6. Containerization: provide a multi-stage Dockerfile based on `python:3.11-slim`.\n\nDeliverables:\n- A `requirements.txt` listing the exact versions of all third-party libraries (as of May 6, 2025).\n- A single `main.py` (under 150 lines) containing complete, runnable Python 3.11+ code with clear comments indicating where and how each external library is used.","question_index":23,"modules":["fastapi","fastapi-socketio","fastapi-htmx","redis","arq","sqlmodel","databases","jinja2","openai","fastapi-mail","uvicorn","python-dotenv"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a compact Python microservice that ingests text and images, leverages AI summarization, real-time streams results, and stores everything in a modern database.\n\nFunctional Requirements:\n1. Use the latest Litestar (async web framework released Dec 2023) to define:\n   a. POST \/articles to receive JSON {\u201ctitle\u201d:\u2026, \u201ccontent\u201d:\u2026}.\n   b. WebSocket endpoint \/ws\/summary for streaming summaries.\n2. Connect asynchronously to PostgreSQL via the latest Prisma Python client (released Jul 2023), defining models for Article and Image.\n3. Summarize incoming content using the latest llama-cpp-python (released Sep 2023), utilizing its streaming interface for token-by-token output.\n4. Implement \/upload to accept multipart\/form-data image files, generate HEIF thumbnails with the latest pillow-heif (released Oct 2023), and persist file paths.\n5. Ensure all endpoints use async\/await, proper error handling, and pydantic-based validation.\n\nDeliverables:\n\u2013 A requirements.txt listing the latest library versions as of 2025-05-06.\n\u2013 A single, runnable Python 3.11+ script under 150 lines, with clear comments indicating where each external library is used.","question_index":16,"modules":["llama_cpp","litestar","pillow_heif","pydantic","prisma","uvicorn","python-multipart"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":7}
{"model":"claude-3-7-sonnet-20250219","question":"You are tasked with building a prototype web service for authenticated semantic search with real-time visual feedback.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) server with:\n   a. an OAuth2 login flow using PyOIDC (latest) against an external Identity Provider  \n   b. a POST \/query endpoint to accept natural-language queries  \n   c. a WebSocket \/ws endpoint to stream real-time results  \n2. Upon authentication, embed incoming queries using sentence-transformers (latest) and perform vector similarity search in ChromaDB (latest).  \n3. As results arrive, push them incrementally over the WebSocket to clients.  \n4. Generate a Plotly (latest) bar chart of top-k result scores and serve it as an HTML snippet.  \n5. Ensure all components run under Python 3.11+, integrate only \u201cthe latest\u201d library releases as of 2025-05-06, and keep implementation concise.  \n\nDeliverables:\n\u2022 A list of chosen libraries with exact versions (the \u201crequirements\u201d).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":42,"modules":["fastapi","pydantic","python-jose","pyoidc","sentence-transformers","chromadb","plotly","uvicorn"],"modulenotfound":["pyoidc"],"modulenotfound_count":1,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are building a high-throughput, real-time analytics web application for processing IoT sensor streams using the latest Python async frameworks and libraries.\n\nFunctional Requirements:\n1. Ingest JSON sensor events via a REST API built on FastAPI (use its latest HTTP\/3 support).\n2. Broadcast processed metrics to connected clients over WebSockets using python-socketio (asyncio).\n3. Offload CPU-intensive aggregation to background workers with Arq (latest Redis-backed queue).\n4. Persist time-series summaries in PostgreSQL via SQLModel (Pydantic v2-powered ORM).\n5. Cache hot query results in Redis using aioredis for sub-millisecond lookups.\n6. Expose tracing and Prometheus metrics using OpenTelemetry and prometheus-client.\n\nDeliverables:\n- A bullet list of the latest library dependencies as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments indicating where each external library is used.","question_index":40,"modules":["fastapi","hypercorn","sqlmodel","aioredis","arq","socketio","opentelemetry-api","opentelemetry-sdk","prometheus-client","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-7-sonnet-20250219","question":"You are a senior software engineer tasked with building a Python web application that integrates user authentication, real-time notifications, AI-based recommendations, and payment processing.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (2025-05-06) and Python 3.11 features (e.g. match-case).\n2. Secure all endpoints with OAuth2 JWT authentication via the latest fastapi-users, including email verification flows.\n3. Provide real-time notifications over WebSockets using the latest python-socketio async server.\n4. Handle one-time charges using the latest Stripe Python SDK via the Payment Intents API.\n5. Generate AI-driven product recommendations by invoking gpt-4-turbo through the latest openai library.\n6. Emit structured, context-rich logs for each operation using the latest structlog.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06  \n\u2022 Complete, runnable Python 3.11+ code (under 150 lines) with clear comments marking where each external library is used","question_index":96,"modules":["fastapi","python-multipart","fastapi-users","fastapi-socketio","python-socketio","python-jose","stripe","openai","structlog","uvicorn","pydantic","sqlalchemy","uuid","aiosqlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":14}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: As a senior software engineer, design a high-performance microservice for real-time document collaboration with secure access, persistence, and cloud integration.  \n\nFunctional Requirements:  \n1. Implement an asynchronous JSON REST API using the latest Python web framework available as of 2025-05-06, supporting pagination and input validation.  \n2. Secure all endpoints with OAuth2 PKCE and JWT using the latest authentication library (released within the last 12 months).  \n3. Enable bidirectional real-time collaboration events over WebSocket using the latest WebSocket library (released within the last 12 months).  \n4. Persist document data in an async NoSQL database (e.g., MongoDB) using the newest driver supporting transactions and change streams.  \n5. Upload and serve document attachments to an S3-compatible object storage using the latest SDK with multipart upload support.  \n6. Cache frequently accessed documents in Redis with TTL and automatic invalidation using the newest Redis client library.  \n\nDeliverables:  \n- A requirements.txt listing \u201cthe latest\u201d versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":80,"modules":["fastapi","uvicorn","pydantic","motor","aioboto3","redis-om","authlib","python-jose","websockets","python-multipart","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a high-performance Python microservice that combines cutting-edge web, real-time, AI and observability features into a single ASGI application.\n\nFunctional Requirements:\n1. HTTP\/3 ASGI server with automatic TLS and QUIC support using the latest HTTP\/3 library released within the past 12 months.  \n2. Real-time WebSocket endpoint that streams JSON events with zero-copy backpressure control using the newest streaming library created in the last year.  \n3. Semantic search pipeline: accept text uploads, generate embeddings, and perform vector similarity queries via the latest vector-store client released within 12 months.  \n4. On-device LLM inference endpoint that consumes streamed text segments and returns summaries using the most recent quantized model library published this year.  \n5. Distributed tracing and metrics export using the newest OpenTelemetry OTLP exporter released within the past 12 months.\n\nDeliverables:\n- A requirements list (requirements.txt) specifying the exact, latest versions of each third-party library as of 2025-05-06.  \n- A single Python 3.11+ script under 150 lines that implements all features above, with clear comments marking where each external library is used.","question_index":51,"modules":["uvicorn","starlette","aioquic","websockets","backpressure","lancedb","sentence-transformers","llama-cpp-python","opentelemetry-api","opentelemetry-sdk","opentelemetry-exporter-otlp","python-multipart"],"modulenotfound":["backpressure"],"modulenotfound_count":1,"module_count":12}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario  \nDevelop an end-to-end Python web service that provides passwordless WebAuthn authentication, stores and summarizes text inputs via OpenAI, broadcasts real-time summary events, and serves interactive charts.\n\nFunctional Requirements  \n1. Implement passwordless registration and login using the latest \u201cwebauthn\u201d library (FIDO2) with Litestar middleware.  \n2. Create REST endpoints under Litestar (latest version) for submitting text to summarize and retrieving stored summaries.  \n3. Use the latest SQLModel with an async PostgreSQL engine to persist input texts, summaries, timestamps, and user IDs.  \n4. Invoke the latest \u201copenai\u201d Python client to perform GPT-4 Turbo summarization with function-calling.  \n5. Serialize and broadcast new summaries over WebSockets using the latest \u201cmsgspec\u201d for high-performance JSON.  \n6. Serve an HTML page at \u201c\/charts\u201d that embeds an interactive Plotly chart of summary lengths over time.\n\nDeliverables  \n\u2022 A numbered list of all external libraries (with exact latest versions as of 2025-05-06) and their roles.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":13,"modules":["litestar","webauthn","sqlmodel","openai","msgspec","asyncpg","plotly","python-dotenv"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: As a senior software engineer, you must develop a real-time analytics dashboard backend that requires user authentication, data ingestion, live updates, background enrichment via an external API, and a web-based dashboard.\n\nFunctional Requirements:\n1. Implement user registration and login endpoints using JWT authentication with asymmetric keys via FastAPI-JWT-Auth (latest).\n2. Create a POST \/data endpoint to ingest JSON payloads and store them asynchronously in SQLite using SQLModel (latest).\n3. Broadcast each new record to connected WebSocket clients at \/ws using the latest websockets library, leveraging async broadcast.\n4. Schedule a background job every minute with arq (latest) to enrich stored records by streaming JSON from a third-party API via httpx (latest).\n5. Serve a simple HTML dashboard at GET \/dashboard that opens a WebSocket to \/ws and displays incoming records in real time.\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":75,"modules":["arq","httpx","jwt","fastapi","sqlmodel","starlette","cryptography","passlib","bcrypt","uvicorn","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Design and implement a Python microservice that exposes HTTP endpoints, real-time WebSocket notifications, persistent storage, caching, scheduled tasks, and email alerts using the latest third-party libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use FastAPI (latest) with HTTP\/2 support to expose async REST endpoints for user registration and notification retrieval.\n2. Use SQLModel (latest) as an async ORM for PostgreSQL, defining typed models for users and notifications.\n3. Implement WebSocket broadcasting via FastAPI\u2019s WebSocket support to push new notifications to connected clients.\n4. Leverage Aioredis (latest) Redis Streams to cache session state and stream notification events.\n5. Schedule periodic database cleanup and cache eviction using APScheduler (latest) with its asyncio integration.\n6. Send email alerts for critical notifications using Aiosmtplib (latest) with STARTTLS.\n7. Perform external health-check API calls using HTTPX (latest) with async retries and timeouts.\n\nDeliverables:\n- A requirements list naming each library and its latest version as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, including clear comments that indicate where each external library is used.","question_index":44,"modules":["fastapi","uvicorn","sqlmodel","asyncpg","aioredis","apscheduler","aiosmtplib","httpx","python-jose","python-multipart","email-validator"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are tasked as a senior software engineer to build an end-to-end collaborative document editing service with real-time updates, async persistence, GraphQL subscriptions, and cloud file storage.\n\nFunctional Requirements:\n1. Use the latest Litestar (first released within the last 12 months) to create an HTTP\/2 web server with WebSocket endpoints for real-time edit streams.\n2. Implement a GraphQL API with Strawberry GraphQL (latest) that supports queries, mutations, and live subscriptions for document change events.\n3. Persist documents in a PostgreSQL database via a fully async ORM of your choice that was first released in the last 12 months and supports automated migrations.\n4. Generate AWS S3 presigned URLs for resumable file attachments using aiobotocore (latest) with async upload\/download.\n5. Schedule periodic cleanup of stale sessions using an async task scheduler library first released within the last 12 months.\n\nDeliverables:\n- A brief list of the chosen third-party libraries (with exact versions) that meet the \u201cfirst released within the last 12 months\u201d requirement.\n- Complete, runnable Python 3.11+ code (under 150 lines) fulfilling the above requirements, with clear comments indicating where and how each external library is used.","question_index":78,"modules":["litestar","edgedb","strawberry","aiobotocore","taskiq","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario:\nYou are a senior software engineer tasked with building a real-time AI summarization microservice.\n\nFunctional Requirements:\n1. Implement an async RESTful POST endpoint `\/summarize` that accepts JSON `{ \"text\": \"<string>\" }` and returns a summary generated via `llama-cpp-python`.\n2. Implement a WebSocket endpoint `\/stream` that streams incremental summary tokens to the client as they are produced.\n3. Persist each original text and its summary in an async SQLite database using `SQLModel`.\n4. Cache recent summaries in Redis for 10 minutes with `aioredis` to avoid redundant computation.\n5. Expose Prometheus-compatible metrics on `\/metrics` (request counts, latency histograms, cache hit\/miss) via `prometheus-client`.\n6. Use a background task to warm up the Llama model at startup and to periodically clean up expired cache entries.\n\nDeliverables:\n- A restatement of the functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Code must use the latest versions of FastAPI, SQLModel, aioredis, llama-cpp-python, and prometheus-client as of 2025-05-06.\n- Include clear comments indicating where each external library is used.","question_index":54,"modules":["fastapi","llama-cpp-python","aioredis","prometheus-client","sqlmodel","uvicorn","starlette"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-7-sonnet-20250219","question":"You are a senior software engineer tasked with building a high-throughput real-time sentiment analysis microservice.\n\nFunctional Requirements:\n1. Use FastAPI (latest) to expose a POST \/analyze endpoint that accepts JSON text payloads and stores request metadata in PostgreSQL via SQLModel (latest).\n2. Secure all endpoints with OAuth2 JWT authentication using Authlib (latest), leveraging its PKCE support.\n3. Perform real-time sentiment analysis on incoming text using the latest Hugging Face Transformers pipeline with GPU acceleration via PyTorch (latest) and cache results in Redis using aiocache (latest).\n4. Broadcast live sentiment scores to connected clients over WebSockets using python-socketio (latest) and run the server on Uvicorn with HTTP\/3 support.\n5. Asynchronously upload any user-submitted audio snippets to S3-compatible storage via aiobotocore (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all external libraries as of 2025-05-06  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":62,"modules":["fastapi","pydantic","sqlmodel","authlib","transformers","torch","aiocache","python-socketio","uvicorn","aiobotocore","python-multipart","python-jose","alembic","psycopg2-binary","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":15}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a high-performance Python web application combining HTTP\/3 REST, GraphQL, real-time WebSockets, async database operations, data analytics, AI summarization, and distributed tracing.\n\nFunctional Requirements:\n1. HTTP\/3 REST API: use FastAPI (latest as of 2025-05-06) with Uvicorn HTTP\/3 support to serve \/items endpoints.\n2. Async GraphQL API: use Strawberry (latest as of 2025-05-06) to expose a GraphQL schema with DataLoader for batch fetching.\n3. Async DB access: use SQLModel (latest as of 2025-05-06) with an async SQLAlchemy engine to perform PostgreSQL CRUD.\n4. Real-time WebSockets: use websockets library (latest as of 2025-05-06) to broadcast item updates at \/ws.\n5. Data analytics: use Polars (latest as of 2025-05-06) lazy API to compute summary statistics on items.\n6. AI summarization: use OpenAI Python SDK (latest as of 2025-05-06) to generate summaries of item descriptions.\n7. Observability: use OpenTelemetry Python SDK (latest as of 2025-05-06) to instrument all endpoints for distributed tracing.\n\nDeliverables:\n- requirements.txt listing exact latest versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments marking where each external library is used.","question_index":53,"modules":["fastapi","uvicorn","strawberry-graphql","sqlmodel","websockets","polars","openai","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","httpx","python-dotenv","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":13}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are a senior software engineer building a real-time sentiment and OCR-enhanced geospatial chat dashboard for remote field teams.\n\nFunctional Requirements:\n1. Use FastAPI (latest version as of 2025-05-06) to implement an async REST API that serves a minimal HTML client.\n2. Implement bi-directional real-time chat via Socket.IO using python-socketio (latest) on the FastAPI server.\n3. Perform on-the-fly sentiment analysis of text messages using Hugging Face\u2019s transformers pipeline (latest) and broadcast sentiment scores.\n4. Accept image uploads from clients, extract embedded text with EasyOCR (latest), and inject the OCR result into the chat stream.\n5. Plot message geolocation metadata on a Folium (latest) map and serve it as an embeddable HTML snippet.\n\nDeliverables:\n- A requirements list enumerating the exact \u201clatest\u201d third-party libraries and versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":2,"modules":["fastapi","uvicorn","socketio","transformers","torch","easyocr","folium","jinja2"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Develop a Python microservice that supports AI-driven product imagery, real-time stock updates, secure user authentication, and checkout processing.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) app with JWT-based auth using Authlib (latest) and secure password hashing.  \n2. Define async ORM models for Users and Products with SQLModel (latest) on PostgreSQL, including migration setup.  \n3. On product creation, generate an AI image via Diffusers (latest) or Stability-SDK (latest) and expose it via a REST endpoint.  \n4. Provide a WebSocket endpoint for real-time inventory updates using Starlette\u2019s WebSocket support (latest).  \n5. Create a \/checkout POST endpoint integrating Stripe\u2019s Python SDK (latest) with webhook handling to confirm payments.\n\nDeliverables:\n\u2022 A bullet list of the \u201clatest\u201d libraries (with pip install specifiers as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":85,"modules":["fastapi","uvicorn","sqlmodel","asyncpg","alembic","authlib","python-jose","passlib","diffusers","stripe","python-multipart","torch"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are developing a high-performance, real-time AI-powered web service for personalized content recommendations.\n\nFunctional Requirements:\n1. Implement a RESTful POST \/recommend endpoint using the latest Python web framework released within the last 12 months, accepting JSON input and returning JSON responses.\n2. Add a WebSocket \/ws endpoint for streaming live recommendation updates using the newest asyncio-based WebSocket library from the past year.\n3. Integrate semantic search over user profiles by connecting to a vector database via its newest Python client library released within the last 12 months.\n4. Schedule periodic model retraining in a background worker using the most recent Python task scheduler released in the last 12 months.\n5. Store and serve user-uploaded media files on cloud object storage using the latest cloud storage client library published within the past 12 months.\n\nDeliverables:\n\u2022 A requirements list (requirements.txt) specifying the exact library names and versions (all released within the last 12 months).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total, with clear comments marking where and why each external library is used.","question_index":71,"modules":["fastapi","uvicorn","websockets","qdrant-client","schedule","boto3","pydantic","httpx","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Develop a high-performance Python microservice that ingests real-time social media posts via WebSockets, generates AI summaries, stores them in MongoDB Atlas, serves secured HTTP\/3 endpoints, and provides an interactive Streamlit dashboard for sentiment trends.\n\nFunctional Requirements:\n1. Initialize an asynchronous FastAPI app with HTTP\/3 (h2\/h3) and ASGI support.  \n2. Protect all HTTP endpoints with OAuth2 JWT using Authlib\u2019s latest OAuth library.  \n3. Consume a WebSocket stream of JSON posts from wss:\/\/stream.example.com asynchronously.  \n4. Summarize each post using OpenAI\u2019s latest Python SDK leveraging function-calling or embeddings.  \n5. Persist summaries and metadata in MongoDB Atlas via Motor (async driver).  \n6. Expose a secured REST endpoint `\/summaries` supporting pagination and filtering by sentiment.  \n7. Build a Streamlit dashboard that fetches `\/summaries` and visualizes sentiment trends over time with Plotly.\n\nDeliverables:\n- A requirements list naming the latest versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":28,"modules":["fastapi","uvicorn","aioquic","websockets","openai","motor","authlib","python-jose","python-multipart","streamlit","plotly","httpx","python-dotenv"],"modulenotfound":[],"modulenotfound_count":0,"module_count":13}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Design and implement a high-performance real-time chat backend with semantic search and background processing.\n\nFunctional Requirements:\n1. Asynchronous REST API: use FastAPI (latest as of 2025-05-06) to expose user and message endpoints.\n2. HTTP\/3 WebSockets: implement real-time chat over WebSockets with Hypercorn\u2019s HTTP\/3 support.\n3. Async ORM persistence: store users and messages in PostgreSQL via SQLModel (async).\n4. Vector semantic search: index and query messages in Qdrant using the latest qdrant-client.\n5. Background sentiment analysis: enqueue and run analysis jobs with ARQ (async Redis-backed queue).\n6. JWT authentication: secure all endpoints with python-jose for JWT issuance and validation.\n7. Redis caching: cache active WebSocket connections or session data via redis.asyncio.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06  \n\u2022 A single Python 3.11+ file under 150 lines that:\n  \u2013 Is fully runnable  \n  \u2013 Shows clear comments where each external library is used  \n  \u2013 Integrates all above requirements into a cohesive application","question_index":11,"modules":["fastapi","hypercorn","sqlmodel","asyncpg","redis","arq","qdrant-client","sentence-transformers","python-jose","passlib","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: A fintech startup requires a live transaction monitoring service that ingests, processes, stores, and notifies users of suspicious activity in real time.\n\nFunctional Requirements:\n1. Build an HTTP endpoint to receive and stream-respond to JSON transaction payloads using the latest async web framework released in the last 12 months.\n2. Validate and serialize incoming data with the latest data validation library, enforcing strict type constraints and custom validators.\n3. Persist transactions asynchronously in a relational database using the latest async ORM library compatible with Python 3.11+.\n4. Offload CPU-intensive anomaly detection to a background worker queue using the latest task orchestration library.\n5. Broadcast real-time alerts to connected clients via WebSockets using the latest real-time communication library.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of the latest libraries (as of May 6, 2025)  \n\u2022 A single Python 3.11+ file under 150 lines, complete and runnable, with clear comments indicating where each external library is used.","question_index":39,"modules":["fastapi","pydantic","sqlalchemy","asyncpg","dramatiq","redis","broadcaster","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a real-time collaborative document review service with semantic search and AI summarization.\n\nFunctional Requirements:\n1. HTTP API (HTTP\/3) using the latest starlite (v1.x) to manage documents and user sessions.  \n2. Real-time updates over WebSockets for live editing and presence using starlite\u2019s WebSocket support.  \n3. Semantic indexing and search: on each document save, compute embeddings and store\/retrieve via the latest qdrant-client.  \n4. AI summarization endpoint: call an LLM using the latest llm-tools library to generate concise summaries on demand.  \n5. Background task scheduling with dramatiq (latest) to clean up stale sessions and regenerate indexes.\n\nDeliverables:\n\u2022 Reiterate the numbered requirements.  \n\u2022 A complete, runnable Python 3.11+ code file under 150 lines.  \n\u2022 Use the latest starlite, qdrant-client, llm-tools, and dramatiq libraries available as of 2025-05-06.  \n\u2022 Include clear comments indicating where and why each external library is used.","question_index":55,"modules":["starlite","qdrant-client","llm-tools","dramatiq","uvicorn"],"modulenotfound":["llm-tools"],"modulenotfound_count":1,"module_count":5}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You must build a Python microservice that fetches articles by URL, summarizes them with AI, stores results in Postgres, and provides REST, GraphQL, and WebSocket interfaces with background orchestration using only the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a POST \/submit REST endpoint using the latest Starlite framework (released within the last 12 months) over HTTP\/3 to accept JSON { \"url\": string }.  \n2. Fetch article content asynchronously using the latest httpx library and handle timeouts and redirects.  \n3. Summarize text with the latest HuggingFace Transformers v5 pipeline, leveraging GPU if available.  \n4. Persist the original URL and its summary in Postgres via Tortoise ORM v0.25 (async ORM released in the last 12 months).  \n5. Expose a \/graphql endpoint using Strawberry GraphQL (latest release) to query summaries by URL.  \n6. Provide a \/ws\/notifications WebSocket in Starlite to notify clients in real time when a summary is ready.  \n7. Orchestrate fetch-and-summarize tasks as background jobs using Prefect 2 (newly released workflow orchestrator).\n\nDeliverables:\n- A copy of the numbered functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Use only the latest versions of Starlite, httpx, Transformers v5, Tortoise ORM v0.25, Strawberry GraphQL, and Prefect 2 as of 2025-05-06.\n- Include clear comments indicating where and how each external library is used.","question_index":70,"modules":["starlite","httpx","torch","transformers","tortoise-orm","strawberry-graphql","prefect","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are tasked with developing a next-generation chat analytics platform that seamlessly integrates HTTP\/3, JWT auth, real-time messaging, vector search, and on-device LLM inference.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) with HTTP\/3 support via Hypercorn to serve all endpoints.\n2. Implement JWT authentication with rotational refresh tokens using fastapi-users (latest).\n3. Provide a real-time WebSocket chat endpoint using fastapi-websocket-pubsub (latest) supporting pub\/sub channels.\n4. On each incoming chat message, generate vector embeddings using llama-cpp-python (latest) with an on-device streaming pipeline, and store them in ChromaDB (latest) with HNSW indexing.\n5. Expose a REST endpoint to query semantic similarity: accept a text query, compute embeddings, perform a ChromaDB search, and return the top-3 similar messages.\n6. Expose a streaming summarization endpoint that returns an on-the-fly summary of recent messages using llama-cpp-python\u2019s streaming completion interface.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements you\u2019ve implemented.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":93,"modules":["chromadb","jwt","llama_cpp","fastapi","fastapi_users","fastapi_websocket_pubsub","hypercorn","pydantic","sqlalchemy"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":9}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: As a senior software engineer, develop a Python 3.11+ microservice that manages real-time collaborative whiteboard sessions, persists data, exposes REST and GraphQL APIs, generates session snapshots, integrates external metrics, and secures endpoints with OAuth2.\n\nFunctional Requirements:\n1. Implement session and user management REST endpoints using FastAPI (latest) with async event hooks and Pydantic V2 models.  \n2. Persist all data in PostgreSQL via Tortoise ORM (latest) using model lifecycle events.  \n3. Expose a GraphQL API with Strawberry GraphQL (latest) dataclass resolvers for session and user queries\/mutations.  \n4. Broadcast real-time drawing events using python-socketio (latest) over WebSockets.  \n5. Generate on-demand PNG snapshots of whiteboard state with Pillow-SIMD (latest) leveraging multi-threaded rendering.  \n6. Send session metrics asynchronously to an external analytics service using httpx (latest) with HTTP\/2 support.  \n7. Secure all endpoints with OAuth2 (authorization code + PKCE) using Authlib (latest).  \n\nDeliverables:\n- A numbered list of the functional requirements above.  \n- Complete, runnable Python 3.11+ code (\u2264 150 lines) integrating all requirements.  \n- Inline comments indicating where each external library is used.  \n- Use the latest stable releases of all libraries as of today\u2019s date.","question_index":50,"modules":["httpx","socketio","strawberry","authlib","fastapi","pillow-simd","pydantic","tortoise-orm"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are a senior software engineer tasked with building a real-time analytics microservice that combines REST, GraphQL, persistent storage, and background processing using the latest Python ecosystem tools as of 2025-05-06.\n\nFunctional Requirements:\n1. Expose HTTP and WebSocket endpoints using the latest Litestar ASGI framework with annotated routing and streaming support.\n2. Implement a GraphQL API using the latest Strawberry GraphQL code-first schema builder with asyncio support.\n3. Persist and query analytics data in PostgreSQL via the latest Piccolo-ORM\u2019s async-first models and automatic migrations.\n4. Offload long-running analytics jobs to a Redis-backed async queue using the latest Arq library.\n5. Send WebSocket notifications to clients when background jobs complete.\n\nDeliverables:\n\u2022 A requirements list (pip install commands) specifying the latest versions of Litestar, Strawberry, Piccolo-ORM, and Arq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":65,"modules":["litestar","strawberry-graphql","piccolo","arq","redis","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are a senior software engineer tasked with developing an end-to-end Python web application that leverages the latest libraries to showcase advanced API development, ORM integration, security, background processing, and real-time features.\n\nFunctional Requirements:\n1. Asynchronous REST API: implement CRUD endpoints for a `Task` model using FastAPI (latest version as of 2025-05-06).\n2. Data Modeling and Persistence: define `Task` ORM models with SQLModel (latest version) and connect to a SQLite or PostgreSQL database asynchronously.\n3. Security: secure all API endpoints with OAuth2 password flow using Authlib (latest version) to obtain and validate JWT tokens.\n4. Background Processing: create an asynchronous task queue with Arq (latest version) to process tasks in the background.\n5. Real-Time Notifications: establish a WebSocket endpoint using the websockets library (latest version) to push real-time task status updates.\n\nDeliverables:\n- The numbered list of functional requirements (as above).\n- Complete, runnable Python 3.11+ code under 150 lines. Use the latest versions of FastAPI, SQLModel, Authlib, Arq, and websockets as of 2025-05-06. Include clear comments indicating where and how each external library is used.","question_index":24,"modules":["uvicorn","arq","authlib","fastapi","passlib","pydantic","sqlmodel","aiosqlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a real-time collaborative messaging microservice integrating graph storage, vector search, JWT authentication, and WebSockets using the latest Python libraries (as of May 6, 2025).\n\nFunctional Requirements:\n1. Authentication: secure all endpoints with JWTs using the latest TokenGuard library.\n2. Message Ingestion: implement POST \/messages to accept user messages; decode JWT, extract user ID.\n3. Graph Storage: store each message and its \u201cauthored_by\u201d relationship in a graph database via the latest GraphitORM.\n4. Vector Indexing & Search: generate text embeddings with the latest Morpher library, store them in its vector store, and expose GET \/search?q={text} to return top-5 similar messages.\n5. Real-Time Updates: broadcast new messages to connected clients over WebSockets using the latest Voila framework.\n\nDeliverables:\n1. A requirements list with each third-party library pinned to its latest version as of May 6, 2025.\n2. A single Python 3.11+ script (\u2264150 lines) implementing all endpoints, with clear comments marking where each external library is used.","question_index":35,"modules":["tokenguard","graphitorm","morpher-embeddings","voila-ws","fastapi","uvicorn","python-jose","pydantic"],"modulenotfound":["tokenguard","graphitorm","morpher-embeddings","voila-ws"],"modulenotfound_count":4,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"You are tasked with building a high-performance text summarization service with real-time updates, persistent storage, and caching for internal research teams.\n\nFunctional Requirements:\n1. Implement OAuth2 password flow with JWT tokens for user authentication using the latest python-jose.\n2. Expose a POST \/summarize endpoint that uses the latest transformers library to perform text summarization.\n3. Provide a WebSocket at \/ws\/progress to stream summarization progress in real time using FastAPI\u2019s latest WebSocket support.\n4. Persist user requests and summaries in a SQLite database via the latest SQLModel ORM.\n5. Cache summary results in Redis for 5 minutes using the latest aioredis library to accelerate repeat requests.\n\nDeliverables:\n\u2022 A requirements list specifying the latest versions of FastAPI, transformers, python-jose, SQLModel, aioredis (as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":10,"modules":["fastapi","uvicorn","transformers","python-jose","sqlmodel","aioredis","websockets","python-multipart","passlib"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-7-sonnet-20250219","question":"You are a senior software engineer.\n\nScenario: Build an end-to-end Python service for user profile CRUD with high-performance caching and real-time notifications.\n\nFunctional Requirements:\n1. Implement RESTful CRUD endpoints for user profiles using the latest FastAPI (as of 2025-05-06), leveraging its async lifespan context.\n2. Define an async PostgreSQL-backed User model with SQLModel (latest) using Relations and the new async engine.\n3. Cache GET \/users\/{id} responses in Redis with redis.asyncio (latest), utilizing cluster mode and automatic TTL invalidation on updates.\n4. Broadcast profile change events in real time over WebSocket using the websockets library (latest) with permessage-deflate compression.\n\nDeliverables:\n- A requirements.txt listing the latest versions of FastAPI, SQLModel, redis.asyncio and websockets as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":17,"modules":["fastapi","sqlmodel","redis","websockets","asyncpg","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are a senior software engineer building a real-time sentiment-analysis web service for social media feeds.\n\nFunctional Requirements:\n1. Use the latest Python web framework (released within the last 12 months) to expose REST endpoints with dependency injection and OpenAPI support.\n2. Implement real-time websocket notifications for incoming sentiment updates using a recently released WebSocket library (last 12 months).\n3. Persist incoming posts and sentiment scores in a NoSQL database via its newest async client library (released within the last 12 months).\n4. Offload sentiment inference to a CPU-optimized ML library (first released within the last 12 months) with zero-copy or memory-efficient batch inference.\n5. Schedule periodic cleanup and summary-generation tasks using a lightweight scheduler library introduced within the last 12 months.\n6. Serve a minimal interactive dashboard at \u201c\/dashboard\u201d using a modern, Python-native plotting\/UI library created in the last 12 months.\n\nDeliverables:\n- A bullet list of all third-party libraries chosen, pinned to \u201clatest\u201d versions as of today\u2019s date.\n- A single, complete Python 3.11+ file (under 150 lines) that implements all requirements, with clear comments marking where each external library is used.","question_index":94,"modules":["fastapi","websockets","motor","transformers","optimum","apscheduler","dash","plotly","pandas","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: As a senior software engineer, you must build a real-time analytics backend that ingests user events, persists them, and serves both REST and GraphQL clients with low latency and live updates.\n\nFunctional Requirements:\n1. Implement a POST \/events endpoint using the latest FastAPI (as of 2025-05-06), leveraging its new lifespan context and async router features.\n2. Validate incoming JSON payloads with Pydantic v2\u2019s new @model_validator decorator for strict schema enforcement.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise ORM, utilizing its recent bulk_create optimization.\n4. Expose a GraphQL query and mutation schema via the latest Strawberry GraphQL with code-first federation support.\n5. Broadcast new events to connected clients over WebSockets using the latest websockets library with per-message deflate compression.\n\nDeliverables:\n\u2022 A requirements.txt listing each third-party library pinned to its latest version as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments indicating where and how each external library is used.","question_index":19,"modules":["fastapi","uvicorn","pydantic","tortoise-orm","asyncpg","strawberry-graphql","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are a senior software engineer building a modern microblogging backend that supports authenticated posts, AI-generated images, real-time feed updates, and cloud storage integration.\n\nFunctional Requirements:\n1. Implement a REST API using a Python web framework released in the last 12 months (use the latest available as of 2025-05-06).\n2. Persist posts in a SQL database via an async ORM library created in the last 12 months, leveraging advanced async model features.\n3. Broadcast new posts to connected clients over WebSockets using a recently released high-performance async messaging library.\n4. Authenticate users with OAuth2 (Google) using the latest OAuth client library.\n5. Generate post images by calling Stable Diffusion through the most recent Python API client for AI image generation.\n6. Store and serve AI-generated images in AWS S3 using the newest AWS SDK for Python.\n7. Ensure all components run under Python 3.11+ and fit within 150 lines of code.\n\nDeliverables:\n\u2022 A numbered list of the specific libraries (with versions) you chose to satisfy each requirement.  \n\u2022 Complete, runnable Python 3.11+ code (<150 lines) combining all features.  \n\u2022 Clear inline comments marking where and why each external library is used.","question_index":7,"modules":["fastapi","sqlmodel","asyncpg","authlib","broadcaster","stability-sdk","boto3"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are a senior software engineer tasked with building a serverless real-time IoT analytics service that ingests sensor data, performs streaming aggregation, applies online anomaly detection, and streams results to clients.\n\nFunctional Requirements:\n1. Ingest JSON messages from an MQTT broker (\u201csensors\/+\/data\u201d) using the latest asyncio-based gmqtt library (as of 2025-05-06).  \n2. Stream-process incoming messages with streamz (latest version) to compute 1-minute tumbling-window averages for temperature and humidity.  \n3. Apply online anomaly detection on each windowed aggregation using River\u2019s latest HDBSCAN-based drift detector.  \n4. Persist raw messages and aggregated results into TimescaleDB via the asyncpg driver (latest version).  \n5. Expose a Python 3.11+ FastAPI application with a WebSocket endpoint for live metric and anomaly alert streaming.  \n6. Wrap the FastAPI app for AWS Lambda deployment using the latest mangum adapter.\n\nDeliverables:\n\u2022 A list of third-party dependencies pinned to \u201clatest\u201d versions as of 2025-05-06.  \n\u2022 A single, complete, runnable Python 3.11+ source file under 150 lines.  \n\u2022 Clear inline comments showing where each external library is used.  \n\u2022 The code must integrate all functional requirements and be deployable serverlessly on AWS Lambda.","question_index":38,"modules":["gmqtt","streamz","river","asyncpg","fastapi","websockets","mangum","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a real-time chat analysis service that ingests user messages, performs on-device LLM summarization and sentiment analysis, caches results, persists chat history, and broadcasts updates via WebSockets.\n\nFunctional Requirements:\n1. Expose a POST \/messages endpoint using FastAPI (latest) to receive JSON payloads with \u201cuser_id\u201d and \u201ctext\u201d.\n2. Perform sentiment analysis and summarization on each message using llama-cpp-python (latest) in an async background task.\n3. Cache sentiment results for identical texts in Redis via redis-py (latest) to avoid recomputation.\n4. Persist messages, sentiments, and summaries in PostgreSQL using SQLModel (latest version).\n5. Provide a WebSocket \/ws\/{user_id} endpoint (FastAPI) that broadcasts new summaries and sentiments to connected clients in real time.\n6. Secure all endpoints with OAuth2 password flow via Authlib (latest).\n\nDeliverables:\n\u2022 Restate the numbered requirements above.  \n\u2022 Provide complete, runnable Python 3.11+ code integrating the latest versions of FastAPI, llama-cpp-python, redis-py, SQLModel, and Authlib, under 150 lines total.  \n\u2022 Include clear comments indicating where and how each external library is used.","question_index":32,"modules":["fastapi","authlib","llama-cpp-python","redis","sqlmodel","jose","starlette"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python microservice for real-time IoT sensor analytics and visualization.\n\nFunctional Requirements:\n1. REST API: Use the latest asynchronous Python web framework available as of 2025-05-06 to expose CRUD endpoints with advanced dependency injection and OpenAPI schema generation.\n2. WebSockets: Integrate the latest real-time streaming library as of 2025-05-06 supporting protocol-level backpressure to push live sensor updates to clients.\n3. ML Inference: Embed an on-the-fly analytics model using the latest Python JIT-accelerated inference library as of 2025-05-06, auto-batching requests on CPU\/GPU.\n4. Graph Storage: Persist sensor relationships in a distributed graph database via the latest async driver as of 2025-05-06 with reactive query pipelining.\n5. Dashboard UI: Serve an interactive web dashboard using the latest server-driven UI component library as of 2025-05-06 for real-time charting and controls.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":14,"modules":["fastapi","uvicorn","websockets","pydantic","neo4j","numpy","onnxruntime","dash","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-7-sonnet-20250219","question":"You are a Senior Software Engineer tasked with building a high-performance, real-time analytics microservice using the latest asynchronous Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled ASGI endpoint using the latest Litestar and aioquic (as of 2025-05-06) for bi-directional streaming.\n2. Implement real-time GraphQL subscriptions with the latest Strawberry GraphQL to push analytics updates.\n3. Define domain models and perform asynchronous CRUD operations on Postgres using the latest SQLModel.\n4. Integrate a Redis cache layer with the latest aioredis for hot-path data retrieval.\n5. Instrument request handling and background tasks with OpenTelemetry SDK for distributed tracing.\n\nDeliverables:\n- requirements.txt listing the latest library versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments indicating where each external library is used.","question_index":90,"modules":["litestar","aioquic","strawberry-graphql","sqlmodel","asyncpg","aioredis","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-asgi","pydantic","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"claude-3-7-sonnet-20250219","question":"You are building a real-time collaborative code-review web application that leverages the latest AI, async, storage and security frameworks.\n\nFunctional Requirements:\n1. Implement a FastAPI 0.95+ server with async endpoints and Jinja2 templates for the main UI.\n2. Enable real-time code editing and review comments via WebSockets using the latest websockets library as of 2025-05-06.\n3. On each code submission, stream analysis from OpenAI\u2019s GPT-4o API (or equivalent newest model) and display suggestions in real time.\n4. Persist code snapshots and review metadata in Pinecone\u2019s newest Python client (vector storage) for similarity search.\n5. Secure all HTTP and WS routes with JWT authentication using the latest PyJWT release.\n6. Add structured logging and a \/healthz endpoint using structlog for observability.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total.  \n\u2022 Clear comments indicating where and why each external library is used.","question_index":37,"modules":["jwt","structlog","fastapi","openai","pinecone"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a real-time Q&A web application that ingests documents, generates embeddings, serves semantic search, supports live WebSocket notifications, and exposes observability metrics.\n\nFunctional Requirements:\n1. Use the latest FastAPI (>=0.100) to implement asynchronous REST endpoints for document ingestion and search, plus a WebSocket endpoint leveraging its new lifecycle event hooks.\n2. Upon document ingestion, asynchronously generate embeddings with the latest llama-cpp-python (v0.2+) using 4-bit quantization and store vectors in Pinecone (latest) using hybrid search.\n3. Implement a `\/search` endpoint that queries Pinecone for nearest neighbors based on user input and returns ranked results.\n4. Schedule periodic re-indexing tasks with the latest arq (v1.10+), utilizing its async Redis job scheduler.\n5. Integrate OpenTelemetry Python SDK (v1.21+) for auto-instrumentation, exporting traces and metrics to Prometheus.\n\nDeliverables:\n- A requirements.txt listing the latest libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":33,"modules":["fastapi","llama-cpp-python","pinecone-client","arq","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","opentelemetry-exporter-prometheus","pydantic","websockets","redis","prometheus-client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: As a senior software engineer, build a high-performance microblogging API using the latest Python libraries for REST, validation, async database operations, caching, background tasks, and real-time streaming.\n\nFunctional Requirements:\n1. Implement a user signup endpoint in FastAPI (>=0.100.0) using Pydantic v2 root_validator for advanced schema validation.  \n2. Create and list posts asynchronously in PostgreSQL via SQLModel\u2019s latest async engine.  \n3. Maintain a cache of the 10 most recent posts in Redis using redis-py (>=5.0) asyncio client, updating on post creation.  \n4. Offload image thumbnail generation to an async Dramatiq (>=2.0) background task triggered on new post with image.  \n5. Provide a WebSocket endpoint in FastAPI to broadcast newly created posts to connected clients in real time.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library (FastAPI, Pydantic v2, SQLModel, redis-py asyncio, Dramatiq) is used. Use the latest available versions of all libraries as of 2025-05-06.","question_index":87,"modules":["fastapi","pydantic","sqlmodel","redis","dramatiq","pillow","asyncpg","python-multipart","email-validator"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: You are building a real-time news aggregation microservice with live client updates.\n\nFunctional Requirements:\n1. Develop a REST API using Litestar (the latest version as of 2025-05-06) leveraging its OpenAPI v3.1 auto-generation feature.\n2. Asynchronously fetch and parse external RSS feeds over HTTP\/3 using HTTPX AsyncClient (the latest), then cache results in aiocache (the latest) with a TTL.\n3. Validate and serialize each article using Pydantic v2 (the latest) functional-style models to ensure data integrity.\n4. Expose a WebSocket endpoint using the websockets library (the latest) with permessage-deflate compression for real-time article broadcasting.\n5. Schedule periodic background tasks with Arq (the latest) to re-poll feeds every 5 minutes.\n\nDeliverables:\n- A list of the chosen libraries and their latest versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":31,"modules":["litestar","httpx","aiocache","pydantic","websockets","arq","typing_extensions"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"claude-3-7-sonnet-20250219","question":"Scenario: Build a real-time collaborative note-taking microservice integrating REST, database persistence, caching, GraphQL, WebSockets, and background tasks.\n\nFunctional Requirements:\n1. Expose REST endpoints using FastAPI (latest) for creating, updating and retrieving notes; leverage Pydantic v2\u2019s new validation decorators.\n2. Persist notes asynchronously in SQLite via SQLModel (latest) with SQLAlchemy 2.0-style typing and async engine.\n3. Broadcast note creation and updates over WebSockets with FastAPI\u2019s WebSocket support to subscribed clients in real time.\n4. Cache note retrieval results in Redis using aioredis (latest), implement TTL and cache invalidation on updates.\n5. Provide a GraphQL schema and endpoint using Strawberry (latest) to query and filter notes with federation-style resolvers.\n6. Schedule a daily summary email using FastAPI BackgroundTasks and aiosmtplib (latest) for asynchronous SMTP delivery.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- A single Python 3.11+ source file under 150 lines that implements all requirements, with clear comments marking where each external library is used.","question_index":73,"modules":["fastapi","pydantic","sqlmodel","sqlalchemy","aiosqlite","redis","strawberry-graphql","aiosmtplib","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.0-flash","question":"Scenario: As a senior software engineer, build a high-performance Python web application integrating real-time streaming, GraphQL, async ORM, JWT security, and caching using the latest third-party libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. HTTP & WebSocket server with the latest Litestar, utilizing its new streaming response API for real-time event delivery.\n2. GraphQL API with the latest Strawberry-GraphQL, defining queries, mutations, and subscription streams for live data updates.\n3. Async database models and CRUD operations using the latest Piccolo ORM\u2019s Python 3.11+ async support and built-in migration tooling.\n4. JWT authentication via the latest python-jose, supporting secure login, token rotation, and injection of user context into requests.\n5. Redis-backed caching with the latest aioredis to cache GraphQL query results and automatically invalidate on data changes.\n6. Trigger real-time client notifications over WebSockets when database events occur, leveraging Litestar\u2019s WebSocket routing.\n\nDeliverables:\n\u2022 A numbered list of the final functional requirements.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":57,"modules":["litestar","strawberry-graphql","piccolo","python-jose","aioredis","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.0-flash","question":"Scenario: You are tasked with building a modular Python 3.11+ microservice that delivers real-time chat with semantic search and scheduled maintenance, leveraging only the latest libraries released within the past 12 months.\n\nFunctional Requirements:\n1. Implement HTTP REST endpoints for posting and retrieving chat messages using Litestar (latest release).  \n2. Establish a WebSocket chat channel that broadcasts new messages to all connected clients via the official Litestar WebSocket plugin (latest).  \n3. Persist each message to MongoDB with Beanie ODM (latest) and simultaneously index its vector embedding into Qdrant using qdrant-client (latest).  \n4. Expose a GraphQL query endpoint that performs semantic search over stored messages via strawberry-graphql (latest).  \n5. Schedule a daily background job to purge messages older than 30 days using Dramatiq (latest).\n\nDeliverables:\n- A requirements.txt (or pyproject.toml) listing all dependencies with exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments marking where and how each external library is used.","question_index":76,"modules":["dramatiq","pymongo","beanie","litestar","strawberry","qdrant_client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.0-flash","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python web service that combines asynchronous REST & GraphQL APIs, real-time notifications, persistent storage, caching, and AI-driven text summarization.\n\nFunctional Requirements:\n1. Implement async HTTP endpoints with FastAPI (latest as of 2025-05-06) using Python 3.11 structural pattern matching and Pydantic v2 dataclass models.\n2. Provide a GraphQL endpoint powered by Strawberry GraphQL v1.0+ with schema federation support.\n3. Persist and query data in PostgreSQL via the latest Tortoise-ORM with asyncio-optimized connection pooling.\n4. Push real-time notifications over WebSockets using python-socketio latest async server mode.\n5. Summarize user-submitted text on demand using the latest llama-cpp-python library with optional GPU offload.\n6. Cache frequent database queries in Redis using aioredis latest cluster-mode support.\n\nDeliverables:\n- A requirements list specifying exact versions of all third-party libraries.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":74,"modules":["aioredis","socketio","strawberry","fastapi","llama_cpp","tortoise","uvicorn","pydantic","python_dotenv"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":9}
{"model":"gemini-2.0-flash","question":"Scenario: Build a real-time collaborative note-taking service that supports user auth, live updates, AI summaries, PDF export, and caching using the latest Python web stack.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST API for user signup\/login with FastAPI (latest) using OAuth2 password flow.\n2. Persist users and notes in PostgreSQL via SQLModel (latest async features).\n3. Create a WebSocket endpoint broadcasting live note edits using the newest websockets library.\n4. Add an endpoint `\/summarize` that calls an external AI summarization API via httpx\u2019s HTTP\/3 client (latest).\n5. Provide a `\/export\/{note_id}` endpoint that generates a PDF of the note using Playwright Python (latest).\n6. Cache active JWT tokens in Redis Streams using aioredis v2.x.\n7. Emit structured JSON logs via loguru (latest advanced configuration).\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- A complete, runnable Python 3.11+ codebase under 150 lines, with clear comments indicating where each external library is used.","question_index":9,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"You are a senior software engineer tasked with building a minimal Python web service for text ingestion, semantic search, and AI-powered summarization using the latest libraries.\n\nFunctional Requirements:\n1. HTTP API: use the latest Starlite framework (as of 2025-05-06) to expose async endpoints.\n2. Text Storage: on POST \/submit, accept JSON {\u201ctext\u201d: \u2026}, store original text and metadata in Redis via the latest redis-om Python client with async HashModel.\n3. Embedding: upon submission, generate a vector embedding using the latest ChromaDB Python client\u2019s async API and upsert into its vector store.\n4. Semantic Search: implement GET \/search?query=\u2026 that computes a query embedding via ChromaDB and returns top 3 matching texts from Redis.\n5. Summarization: implement GET \/summarize\/{id} to retrieve stored text by ID and produce a concise summary using the latest llama-cpp-python streaming API.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries (as of 2025-05-06).  \n\u2022 A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments indicating where and how each external library is used.","question_index":45,"modules":["starlite","redis_om","chromadb","llama_cpp"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":4}
{"model":"gemini-2.0-flash","question":"Scenario: Senior software engineer tasked with developing a real-time GraphQL web service that validates incoming event data, persists it to PostgreSQL, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI web framework) as of 2025-05-06 to implement an async HTTP POST endpoint at `\/events`.\n2. Validate incoming JSON payloads with Pydantic v2 (latest) models, including at least one custom field validator.\n3. Persist validated events to PostgreSQL using the latest Prisma Python client (type-safe generated models and migrations).\n4. Expose a GraphQL API with the latest Strawberry GraphQL library, supporting queries for event history and subscriptions for real-time updates.\n5. Configure WebSocket support in Starlite to broadcast new events to all GraphQL subscription clients.\n\nDeliverables:\n- A brief recap of the functional requirements.\n- Complete, runnable Python 3.11+ code under 150 lines that uses the latest versions of each library as of 2025-05-06, with clear comments marking where each external library is imported and utilized.","question_index":22,"modules":["pydantic","starlite","strawberry","prisma"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.0-flash","question":"Scenario: Design a minimal real-time chat service combining HTTP endpoints, data validation, async ORM persistence, WebSocket broadcasting, and JWT security using the latest Python libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Starlite to expose POST \/register and GET \/messages endpoints, leveraging its advanced dependency injection system.\n2. Define and serialize request\/response models with the latest Pydantic v2, employing its new `model_serializer` API.\n3. Persist User and Message models to SQLite asynchronously with the latest Ormar ORM, utilizing its built-in async migration feature.\n4. Implement a `\/ws` WebSocket route using the latest websockets library for real-time message broadcasting to connected clients.\n5. Secure both HTTP routes and WebSocket connections with JWT authentication via the latest PyJWT release.\n\nDeliverables:\n- A concise mapping of each numbered requirement to the specific library and feature used.\n- Complete, runnable Python 3.11+ code under 150 lines total, with clear comments indicating where each external library is imported and applied.","question_index":21,"modules":["jwt","ormar","pydantic","starlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.0-flash","question":"Scenario: You are building a Python microservice that accepts user-uploaded text files, asynchronously generates summaries with a local LLM, and streams live progress to clients via WebSocket.\n\nFunctional Requirements:\n1. Implement a FastAPI app (use the latest FastAPI as of 2025-05-06) with an `\/upload` POST endpoint that accepts plain-text file uploads.  \n2. Upon upload, enqueue a background job using arq (latest version) to process the file asynchronously.  \n3. In the arq worker, load a local LLM via llama-cpp-python (latest version) using its new GPU offloading API to generate a summary.  \n4. Configure python-socketio (latest version) as an ASGI middleware in FastAPI to broadcast progress and final summary events to connected WebSocket clients.\n\nDeliverables:\n- A bullet list of the chosen third-party libraries with their exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":46,"modules":["fastapi","arq","llama_cpp","socketio"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":4}
{"model":"gemini-2.0-flash","question":"Scenario: Build an end-to-end real-time customer feedback summarization service with live dashboard updates.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled JSON ingest endpoint using Litestar (latest as of 2025-05-06) with async support.\n2. Define a feedback model with Pydantic v2 and persist submissions into SQLite asynchronously via SQLModel (latest).\n3. On each new submission, call the OpenAI Python SDK (latest) asynchronously to generate a brief summary.\n4. Cache each summary in Redis via redis-om (latest) with a 1-hour TTL.\n5. Serve a live dashboard using Reflex (latest) that connects over WebSockets to display incoming summaries in real time.\n6. Use HTTPX (latest) with HTTP\/3 support for any external HTTP calls.\n\nDeliverables:\n\u2022 requirements.txt listing all dependencies pinned to the latest versions as of 2025-05-06  \n\u2022 A single Python 3.11+ script (under 150 lines) that implements the above, with clear comments indicating where and why each external library is used","question_index":61,"modules":["openai","redis","redis_om","reflex","sqlmodel","httpx","litestar","aiosqlite","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.0-flash","question":"Scenario: Develop a Python 3.11+ web application that provides secure collaborative document management with GraphQL, real-time WebSocket updates, AI-powered summarization, and on-demand PDF export.\n\nFunctional Requirements:\n1. Implement a GraphQL API for document queries and mutations using the latest \u201cariadne\u201d library with subscription support.\n2. Enable OAuth2 login with passwordless WebAuthn using the latest \u201cauthlib\u201d and \u201cfido2\u201d libraries.\n3. Provide real-time document update notifications over WebSocket leveraging FastAPI\u2019s built-in WebSocket support.\n4. Schedule asynchronous AI summarization tasks using the latest \u201cllama-cpp-python\u201d library and \u201cAPScheduler\u201d for background job management.\n5. Generate PDF exports of documents on demand using the latest \u201cplaywright\u201d headless browser library in Python.\n6. Persist documents and user metadata in PostgreSQL via the latest \u201ctortoise-orm\u201d with native async and Pydantic v2 model support.\n\nDeliverables:\n- A brief requirements list naming each third-party library and its version.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used. Use the latest library versions available as of 2025-05-06.","question_index":88,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: You are designing a Python microservice for ingesting user text, storing embeddings, and providing real-time, rate-limited on-device summaries via WebSockets.\n\nFunctional Requirements:\n1. HTTP POST \/ingest  \n   - Validate payload (id: str, text: str) using Pydantic v2 root validators  \n   - Compute embeddings on-device with llama-cpp-python and store (id, embedding, text) in ChromaDB  \n2. HTTP GET \/query  \n   - Accept query text, validate with Pydantic v2  \n   - Compute embedding via llama-cpp-python, retrieve top-5 nearest documents from ChromaDB, return metadata  \n3. WebSocket \/summarize  \n   - Client sends document id  \n   - Fetch text from ChromaDB metadata  \n   - Stream back a summary using llama-cpp-python streaming API  \n   - Enforce token-per-second rate limiting via aiolimiter  \n4. Lifespan events  \n   - On startup, initialize ChromaDB client, Llama model, and aiolimiter  \n   - On shutdown, cleanly close any resources  \n\nDeliverables:\n- requirements.txt listing the latest available versions as of today of:\n  \u2022 starlite  \n  \u2022 pydantic v2  \n  \u2022 chromadb  \n  \u2022 llama-cpp-python  \n  \u2022 aiolimiter  \n- main.py: a single, runnable Python 3.11+ script (under 150 lines) implementing all above, with clear comments marking where each external library is used.","question_index":8,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: Build a serverless Python application enabling real-time geolocated chat with sentiment analysis, dynamic map visualization, and PDF session reporting.\n\nFunctional Requirements:\n1. Real\u2010time WebSocket chat using the latest Litestar (released in the last 12 months) with asynchronous message persistence via a modern async ORM.\n2. Interactive geospatial mapping of incoming messages on the client side using the latest Python geospatial visualization library released in the last 12 months.\n3. On-the-fly sentiment analysis of each message using the latest Python sentiment analysis library released in the last 12 months.\n4. PDF generation of the entire chat session\u2014including message timeline, geolocation markers, and sentiment scores\u2014using the latest Python PDF library released in the last 12 months.\n5. Deployment configuration for AWS Lambda using the latest Serverless Framework plugin and AWS Lambda Powertools (all released in the last 12 months).\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":97,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: You are a senior software engineer tasked with building a real-time event analytics dashboard that ingests events, stores them, generates AI-driven summaries, caches counts, and streams updates to authenticated clients with full observability.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to implement a POST \/events REST endpoint for event ingestion.\n2. Validate incoming JSON payloads with the latest Pydantic v2 models.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise-ORM.\n4. Cache aggregated event counts in Redis via the latest aioredis.\n5. Generate streaming summaries of event batches with the latest OpenAI Python client.\n6. Stream real-time updates to clients over WebSocket using FastAPI.\n7. Secure all HTTP and WebSocket endpoints with JWT authentication via the latest fastapi-jwt-auth.\n8. Instrument HTTP, database, cache, and AI client calls with the latest OpenTelemetry for distributed tracing.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest library names and versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines integrating all subdomains, with clear comments indicating where each external library is used.","question_index":4,"modules":["aioredis","openai","fastapi","fastapi_jwt_auth","pydantic","tortoise","opentelemetry","websockets","uvicorn"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":9}
{"model":"gemini-2.0-flash","question":"Scenario: You\u2019re building a high-performance async task management API using the latest libraries in web framework, async ORM, caching, background processing, and observability as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement CRUD REST endpoints for a Task model using the latest starlite.\n2. Persist tasks in Postgres asynchronously with the latest Tortoise-ORM.\n3. Enqueue a background job on task creation using the latest Dramatiq with Redis broker.\n4. Cache GET \/tasks\/{id} responses in Redis via the latest redis.asyncio.\n5. Instrument all HTTP and database calls with OpenTelemetry using the latest opentelemetry-instrumentation-starlite.\n\nDeliverables:\n- A requirements.txt listing each external dependency with explicit versions.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":34,"modules":["dramatiq","redis","starlite","tortoise"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.0-flash","question":"Scenario  \nYou are building an AI-powered document search microservice that ingests user uploads, indexes them with a GPU-accelerated vector store, caches recent queries, and exposes HTTP\/3 endpoints for real-time search.  \n\nFunctional Requirements  \n1. HTTP API (Starlite, latest)  \n   1.1 POST \/upload: receive plain-text documents, validate with Pydantic.  \n   1.2 GET \/query?q=\u2026: return top-k results for query.  \n2. Document Ingestion (llama-index, latest)  \n   2.1 Stream tokenized embeddings upon upload.  \n3. Vector Store (ChromaDB, latest)  \n   3.1 Use GPU-backed indexing for fast similarity search.  \n4. Caching Layer (redis-om, latest)  \n   4.1 Cache each query + results with TTL and Pydantic models.  \n\nDeliverables  \n\u2022 A requirements.txt listing the latest versions (as of 2025-05-06) of all third-party libraries, each released within the past 12 months.  \n\u2022 Complete, runnable Python 3.11+ code (\u2264150 lines) implementing all requirements.  \n\u2022 Clear inline comments indicating where and how each external library is used.","question_index":47,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: You are a senior engineer building a high-throughput job processing microservice with a modern async Python stack.  \n\nFunctional Requirements:  \n1. Expose a POST \/jobs endpoint using the latest Starlite (as of 2025-05-06) to accept JSON job submissions.  \n2. Validate incoming JSON payloads with the latest Pydantic v2 models.  \n3. Persist job records to PostgreSQL via the latest prisma-client-py async ORM.  \n4. Enqueue and process jobs in the background using the latest Taskiq queue.  \n5. Expose a GET \/jobs\/{id}\/status endpoint to retrieve current job status.  \n6. Ensure all I\/O is non-blocking and use Python 3.11+ async\/await throughout.  \n\nDeliverables:  \n\u2022 A requirements list specifying the exact latest versions of Starlite, Pydantic v2, prisma-client-py, and Taskiq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ source code under 150 lines.  \n\u2022 Clear inline comments marking where each external library is used.","question_index":99,"modules":["pydantic","starlite","taskiq","prisma","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.0-flash","question":"Scenario: Build a secure, real-time AI-powered chat web service.\n\nFunctional Requirements:\n1. Expose an async REST API using the latest FastAPI for user registration and login endpoints.  \n2. Implement JWT authentication with rotating refresh tokens via the latest Authlib.  \n3. Persist users and chat messages in SQLite using the latest SQLModel, leveraging its new async support.  \n4. Enable real-time chat notifications over WebSockets using the latest python-socketio and ASGI.  \n5. Integrate AI chat completions using the latest OpenAI Python client in an async background task.\n\nDeliverables:\n1. A requirements.txt listing exact, up-to-date package versions as of today\u2019s date.  \n2. A single Python 3.11+ file (<150 lines) containing complete, runnable code with clear comments marking where each external library is used.","question_index":91,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: You are a senior software engineer tasked with building an AI-powered collaborative document annotation API.\n\nFunctional Requirements:\n1. Secure user signup and login with JWT-based authentication using Starlite (latest) and Pydantic v2.\n2. REST endpoint to upload documents and persist metadata in Redis OM Python (latest).\n3. WebSocket route for real-time annotation broadcasting using Starlite\u2019s built-in WebSocket support.\n4. Background summarization of uploaded documents using llama-cpp-python (latest).\n\nDeliverables:\n- A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":1,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: You are a senior engineer tasked with building a secure, high-throughput microservice for customer order ingestion, persistence, background processing, and caching.\n\nFunctional Requirements:\n1. Use FastAPI (latest as of 2025-05-06) with HTTP\/3 support to implement POST \/orders accepting JSON order data.\n2. Define an asynchronous Order model with SQLModel (latest) and connect to an async database engine (SQLite or PostgreSQL).\n3. Enqueue and process new orders via Arq (latest) background worker.\n4. Implement OAuth2 with JWT issuance and protected endpoints using Authlib (latest).\n5. Cache GET \/orders\/{id} responses in Redis using aiocache (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments marking where each external library is used.","question_index":68,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: Build an end-to-end Python web service that accepts data submissions, runs ML inference asynchronously, and pushes results back in real time.\n\nFunctional Requirements:\n1. Implement a POST \/submit endpoint using the latest FastAPI (ASGI, HTTP\/2) to accept JSON payloads.\n2. Validate and persist submissions into PostgreSQL via the latest SQLModel with asynchronous sessions.\n3. Dispatch an asynchronous classification task via the latest Celery (Redis broker) when new data arrives.\n4. In the Celery task, perform inference using the latest Hugging Face Transformers pipeline (e.g., text-classification).\n5. Cache inference results in Redis using the latest aioredis with a 5-minute TTL.\n6. Expose a WebSocket \/ws endpoint (FastAPI) to stream cached or newly computed results to subscribed clients.\n7. Instrument all HTTP requests and Celery tasks with the latest OpenTelemetry SDK for tracing.\n\nDeliverables:\n\u2022 requirements.txt listing \u201cthe latest\u201d versions of FastAPI, SQLModel, Celery, Redis, aioredis, Transformers, and OpenTelemetry as of 2025-05-06.  \n\u2022 A single Python 3.11+ file under 150 lines of runnable code. Include clear comments where each external library is used.","question_index":59,"modules":["aioredis","celery","fastapi","opentelemetry","pydantic","sqlmodel","transformers"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":7}
{"model":"gemini-2.0-flash","question":"Scenario: You are tasked with building a real-time IoT analytics dashboard featuring anomaly detection, edge caching, vector search, and HTTP\/3 streaming.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of May 6, 2025) with HTTP\/3 and ASGI concurrency for API endpoints.\n2. Ingest IoT telemetry via real-time WebSocket streams using the latest websockets library.\n3. Perform transformer-based anomaly detection using the latest HuggingFace transformers and PyTorch 2.x.\n4. Store and query time-series embeddings in a vector database using the latest Pinecone Python client with HNSW indexing.\n5. Cache recent query results at the edge using the latest aioredis client with TTL invalidation.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the \u201clatest\u201d versions of all third-party libraries as of today.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":67,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: Build a high-performance, AI-powered web dashboard for real-time analytics and user collaboration.\n\nFunctional Requirements:\n1. Web API: Use the latest FastAPI (as of 2025-05-06) with async endpoints and the new dependency-injection system.\n2. Real-Time Collaboration: Implement bidirectional WebSocket chat using python-socketio v5.x+ for live user updates.\n3. AI Integration: Leverage the latest llama-cpp-python (or equivalent) to compute embeddings on incoming messages.\n4. Database Persistence: Store users and message history asynchronously with SQLModel v0.7+ (Pydantic v2) on SQLite.\n5. Caching Layer: Cache AI embedding results in Redis via aioredis v3.x+ to minimize redundant computation.\n6. Security: Protect all endpoints with OAuth2 JWT authentication using Authlib v1.2+.\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all third-party libraries as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, with clear inline comments indicating where each external library is used.","question_index":58,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"You are a senior software engineer tasked with building a modern image-processing web service end-to-end.\n\nFunctional Requirements:\n1. Implement a REST endpoint in Litestar for image uploads that converts incoming images to AVIF using the latest pillow-avif-plugin.\n2. Asynchronously persist image metadata (filename, upload timestamp, AVIF URL, dimensions) in MongoDB via the latest Beanie ODM.\n3. Index and cache metadata in Redis for fast lookup using the latest Redis-OM Python library.\n4. Expose a GraphQL API via the latest Litestar-GraphQL plugin to query images with filtering, pagination, and type validation.\n5. Broadcast real-time upload notifications over WebSocket connections using Litestar\u2019s built-in WebSocket support.\n\nDeliverables:\n- A concise list of the five functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines, using the latest Litestar, Litestar-GraphQL, Beanie, Redis-OM, and pillow-avif-plugin (as of May 6, 2025), with clear comments indicating where each external library is used.","question_index":89,"modules":["PIL","litestar","beanie","motor","redis","redis-om","litestar-graphql","strawberry"],"modulenotfound":["litestar-graphql"],"modulenotfound_count":1,"module_count":8}
{"model":"gemini-2.0-flash","question":"You are a Senior Software Engineer building a high-throughput, real-time web application that ingests user events, enriches them with AI, persists outcomes, and serves analytics via a modern dashboard.\n\nFunctional Requirements:\n1. Implement JWT-based user authentication using the latest async web framework with schema-first validation (e.g., HTTP\/3 support).\n2. Expose a WebSocket endpoint for real-time event ingestion using the latest WebSocket library with per-message compression.\n3. Perform fully async CRUD on PostgreSQL using the latest ORM with native asyncio support and advanced connection pooling.\n4. Offload heavy computations to background workers via the latest task queue library supporting asyncio and result backends.\n5. Integrate an AI text-classification pipeline using the latest AI inference library with streaming outputs.\n6. Instrument the service with distributed tracing using the latest observability library supporting OpenTelemetry protocols.\n\nDeliverables:\n\u2022 requirements.txt listing the latest available versions of all dependencies (as of May 6, 2025).  \n\u2022 A single Python 3.11+ file under 150 lines that is complete and runnable, with clear comments marking where each external library is used.","question_index":12,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: Develop a high-performance asynchronous web service for ingesting user datasets, persisting metadata, launching background processing jobs, and tracing all operations end-to-end.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/upload endpoint using Starlite (latest) with async file streaming and dependency injection.\n2. Define and migrate a PostgreSQL table via Piccolo ORM (latest), storing upload metadata and task IDs, leveraging its GraphQL auto-schema feature.\n3. Enqueue and process tasks with Asynq (latest) in Redis, using priority queues, retry policies, and task result callbacks.\n4. Instrument incoming HTTP requests and background tasks with OpenTelemetry (latest), enabling W3C trace-context propagation across components.\n5. Provide an HTTP GET \/status\/{task_id} endpoint that queries Piccolo ORM for real-time task status.\n\nDeliverables:\n\u2022 The numbered functional requirements list (as above)  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, using the latest versions of all libraries as of 2025-05-06, with clear comments indicating where each external library is used.","question_index":43,"modules":["asynq","opentelemetry","piccolo","starlite"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":4}
{"model":"gemini-2.0-flash","question":"As a senior software engineer, you are tasked with building a secure, real-time collaborative whiteboard microservice using cutting-edge Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3 API using the latest FastAPI (as of 2025-05-06) and uvicorn[standard] for low-latency streaming.\n2. Implement OAuth2 authorization with JWTs and refresh-token rotation using the latest Authlib.\n3. Persist whiteboard sessions and user metadata in PostgreSQL via the latest SQLModel with an async engine.\n4. Broadcast drawing events in real time over WebSockets using the latest websockets library with room-based pub\/sub.\n5. Vectorize stroke data into SVG on the fly using the latest Shapely or pyvips for advanced geometry operations.\n6. Upload periodic session snapshots to AWS S3 using the latest aioboto3 with asynchronous multipart uploads.\n\nDeliverables:\n- A requirements.txt listing the latest available versions of all external libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, fully runnable, with clear comments indicating where each third-party library is used.","question_index":25,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: Build a real-time sentiment analysis dashboard that fetches trending Twitter topics, analyzes sentiment, stores results, and serves updates via WebSockets.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to create both REST and WebSocket endpoints, leveraging its built-in asyncio and WebSocket broadcast features.\n2. Use the latest HTTPX to asynchronously fetch trending topics from the Twitter API v2 over HTTP\/3.\n3. Use the latest Hugging Face transformers to perform sentiment analysis via a pretrained pipeline.\n4. Use the latest SQLModel to define async SQLite models and persist topics with sentiment scores.\n5. Use the latest Jinja2 to render a minimal HTML dashboard for initial page load before WebSocket updates.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the latest versions of FastAPI, HTTPX, transformers, SQLModel, and Jinja2 as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":36,"modules":["httpx","fastapi","transformers","sqlmodel","jinja2"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.0-flash","question":"Senior Software Engineer: You are developing an intelligent IoT monitoring dashboard that integrates real-time ingestion, streaming anomaly detection, WebSocket updates, dynamic UI rendering, and cloud infrastructure provisioning.\n\nFunctional Requirements:\n1. Build an ASGI web server using the latest ASGI framework (as of 2025-05-06) with HTTP\/2 and WebSocket endpoints for real-time data streaming.\n2. Connect asynchronously to an MQTT broker using the latest Python MQTT client library (as of 2025-05-06), subscribe to sensor topics, and relay messages to the server.\n3. Implement real-time anomaly detection on incoming sensor data using the latest Python anomaly detection library supporting incremental or deep learning.\n4. Serve a live, interactive front-end auto-generated from Python to JavaScript using the latest Python-to-JS UI framework.\n5. Provision and deploy all components via infrastructure-as-code using the latest Python IaC SDK on a major cloud provider.\n\nDeliverables:\n\u2022 Restate the numbered functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code integrating all five requirements, under 150 lines.  \n\u2022 Clear inline comments indicating where and how each external \u201clatest\u201d library (as of 2025-05-06) is imported and used.","question_index":49,"modules":["uvicorn","fastapi","paho-mqtt","river","flet","pulumi","pulumi_aws"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.0-flash","question":"Scenario: Build a real-time collaborative note-taking web application with authentication, persistence, caching, live updates, and scheduled cleanup.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (as of today) for note creation, retrieval, update, and deletion.\n2. Secure endpoints with JWT authentication using the latest PyJWT.\n3. Persist notes in a relational database via the latest SQLModel ORM, defining Note and User models.\n4. Enable real-time note synchronization between clients using WebSockets with the latest python-socketio.\n5. Cache frequently accessed note metadata in Redis using the latest redis-om to reduce DB load.\n6. Schedule automatic archiving of notes older than 30 days using the latest APScheduler.\n7. Document all endpoints with OpenAPI and include example request\/response schemas.\n\nDeliverables:\n- A numbered list of all third-party libraries (with versions) you\u2019ve chosen.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear comments indicating where each external library is used.","question_index":69,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: Build a high-performance asynchronous message processing service for real-time sentiment analysis, storage, notification, and search.\n\nFunctional Requirements:\n1. Implement an async HTTP API using the latest FastAPI (as of May 6, 2025) to accept user messages.\n2. Define and persist message models in PostgreSQL via the latest SQLModel ORM.\n3. Schedule and execute sentiment analysis in the background using the latest Dramatiq with RabbitMQ.\n4. After analysis, update the database and push real-time notifications to connected clients over WebSockets.\n5. Index messages and sentiment scores into Elasticsearch using the latest official Python client.\n6. Provide a search endpoint to query stored messages by keyword and sentiment.\n\nDeliverables:\n\u2022 A numbered list of the \u201clatest\u201d third-party libraries (with versions) used for each sub-domain.  \n\u2022 Complete, runnable Python 3.11+ application code under 150 lines, with clear comments indicating where each external library is used.","question_index":48,"modules":["fastapi","sqlmodel","dramatiq","aiohttp","elasticsearch"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.0-flash","question":"Scenario: Build a microservice that accepts text input, asynchronously generates summaries, persists data, and provides real-time metrics.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI 3, HTTP\/3 support) to implement POST \/summaries accepting JSON { \"text\": \"\u2026\" }, enqueue a background job via Arq, and return a job ID.\n2. Persist requests and summaries in SQLite asynchronously with the latest SQLModel (Pydantic V2 model support).\n3. Offload summarization jobs to Redis using the latest Arq, invoking the latest LangChain to call OpenAI for a concise summary.\n4. Expose GET \/summaries\/{id} to retrieve stored summaries from the database.\n5. Instrument request counts and job durations with the latest prometheus_client, exposing metrics on GET \/metrics.\n\nDeliverables:\n- A bulleted list of chosen \u201clatest\u201d libraries with exact version numbers.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear inline comments indicating where each external library is used.\n- Self-contained implementation: after installing dependencies, code runs without further setup.","question_index":95,"modules":["starlite","sqlmodel","arq","langchain","openai","prometheus_client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.0-flash","question":"Scenario: Create a serverless real-time analytics API that ingests WebSocket events, performs on-the-fly NLP summarization, indexes embeddings, exposes a GraphQL endpoint, and deploys to AWS Lambda.\n\nFunctional Requirements:\n1. Use the latest Litestar (as of 2025-05-06) to build an HTTP\/2 GraphQL endpoint with schema-based typing.\n2. Implement a real-time WebSocket consumer using wsproto or litestar-websockets for event ingestion.\n3. Offload text summarization to an asynchronous background task leveraging the latest Transformers library.\n4. Generate embeddings with the newest OpenAI Embeddings API or sentence-transformers and store them in the latest ChromaDB vector store.\n5. Secure all endpoints with passwordless authentication via the latest fastapi-users or equivalent.\n6. Package and deploy the entire application to AWS Lambda using the latest Mangum adapter.\n\nDeliverables:\n- A numbered list of chosen \u201clatest\u201d library versions (as of 2025-05-06) mapping to each requirement.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":77,"modules":["litestar","transformers","chromadb","openai","mangum"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.0-flash","question":"Scenario: Develop a modern, high-performance AI-powered web service for real-time text processing.\n\nFunctional Requirements:\n1. Build a JSON REST API with Litestar using its latest dependency-injection and ASGI lifespan hooks.\n2. Integrate Tortoise ORM (async) to manage a PostgreSQL \u201cjobs\u201d table with migrations.\n3. Use llama-cpp-python to generate text embeddings or responses via a local LLaMA model.\n4. Expose a WebSocket endpoint with python-socketio (asyncio mode) for live client notifications.\n5. Cache recent embeddings in Redis using redis-py\u2019s asyncio client.\n6. Schedule background tasks (e.g., stale job cleanup) leveraging asyncio\u2019s latest task groups.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":60,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: You are a senior software engineer building a real-time social media monitoring application that ingests tweets, analyzes sentiment, classifies images, displays live charts, and deploys as a serverless microservice using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Ingest tweets in real time from Twitter API v2 using the latest HTTPX async streaming client.\n2. Perform NLP sentiment analysis on tweet text with the latest Hugging Face Transformers pipeline.\n3. Classify embedded images using the latest Ultralytics YOLOv8 model.\n4. Render an interactive, live-updating dashboard using the latest Plotly Dash framework.\n5. Scaffold serverless deployment with the latest AWS Lambda Powertools for Python.\n\nDeliverables:\n- A requirements list of all external dependencies.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":98,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: A knowledge management platform must allow users to upload PDFs, index their content, and perform real-time similarity searches via a high-performance API.\n\nFunctional Requirements:\n1. Implement a PDF upload REST endpoint using the latest unstructured-sdk to extract text.\n2. Use the latest ChromaDB Python client to generate and store embeddings for each document.\n3. Provide a search REST endpoint that queries ChromaDB for top-k similar documents given a text query.\n4. Create a real-time WebSocket over HTTP\/3 endpoint using the latest aioquic to stream incremental search results.\n5. Assemble the application as an asynchronous service using the latest Starlite web framework.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":6,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: As a senior software engineer, build an asyncio-based Python microservice showcasing HTTP\/3 REST, GraphQL, async ORM, real-time WebSockets, and scheduled tasks using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST endpoint at `\/api\/items` using FastAPI (latest) and Hypercorn (latest) with HTTP\/3 support.\n2. Expose a GraphQL API at `\/graphql` using Strawberry (latest) with asynchronous resolvers fetching from the database.\n3. Integrate PostgreSQL via Tortoise ORM (latest), define an `Item` model, and apply programmatic migrations on startup.\n4. Provide WebSocket notifications at `\/ws\/updates` using python-socketio (latest), broadcasting item creation and deletion events.\n5. Schedule an hourly cleanup job with APScheduler (latest) in asyncio mode to remove `Item` records older than 7 days.\n\nDeliverables:\n- A `requirements.txt` listing the latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is imported and utilized.","question_index":66,"modules":["socketio","strawberry","apscheduler","fastapi","hypercorn","tortoise"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.0-flash","question":"Scenario: Develop a real-time AI-driven sentiment analysis web service that ingests text, processes it via an on-device LLM, persists results, and streams live analytics to clients.\n\nFunctional Requirements:\n1. Expose RESTful and WebSocket endpoints using the latest Litestar Python framework (released within the last 12 months).\n2. Perform local LLM inference for sentiment analysis with the latest llama-cpp-python library (released within the last 12 months).\n3. Asynchronously persist raw text and sentiment scores in MongoDB using the latest async MongoDB driver (released within the last 12 months) with code-first schema.\n4. Offload sentiment analysis to a distributed background worker using the latest Dramatiq (or equivalent) async task queue released in the last 12 months.\n5. Broadcast real-time sentiment updates to connected clients over WebSockets.\n6. Render an interactive dashboard in server-side templates with the latest Plotly Python library (released within the last 12 months).\n7. Instrument the entire application with distributed tracing via the latest OpenTelemetry Python SDK (released within the last 12 months).\n\nDeliverables:\n- A requirements list specifying each third-party library and its exact latest version as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines total.\n- Clear inline comments indicating where and how each external library is used.","question_index":5,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"You are a senior software engineer tasked with building a real-time analytics dashboard with AI-driven anomaly alerts using the latest Python libraries as of 2025-05-06.  \n\nFunctional Requirements:  \n1. Implement asynchronous REST API endpoints using the latest FastAPI.  \n2. Add WebSocket-based real-time alert streaming using the latest websockets.  \n3. Persist incoming metrics in PostgreSQL via async ORM operations using the latest SQLModel.  \n4. Cache expensive queries in Redis for high throughput using the latest aioredis.  \n5. Perform streaming anomaly detection on incoming metrics using the latest river library.  \n6. Enable file uploads of detailed reports to S3-compatible storage via the latest minio SDK.  \n\nDeliverables:  \n\u2022 A numbered list of required Python libraries (with versions as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":20,"modules":["fastapi","uvicorn","websockets","sqlmodel","asyncpg","aioredis","river","minio","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.0-flash","question":"Build a real-time analytics microservice that lets authenticated users ingest sales data, stores it in PostgreSQL, streams live updates to connected clients, and generates PDF reports with embedded charts.\n\n1. Implement OAuth2 JWT authentication using the latest fastapi-jwt-auth as of May 6, 2025.  \n2. Create async CRUD endpoints for sales records with SQLModel and asyncpg.  \n3. Broadcast new sales events over WebSocket using python-socketio (latest).  \n4. Generate interactive charts in HTML via Plotly (latest) and convert them to PDF using WeasyPrint (latest).  \n5. Expose generated PDFs via an endpoint, serving files asynchronously with aiofiles.  \n\nDeliverables:  \n- A numbered confirmation of the functional requirements above.  \n- A single Python 3.11+ file under 150 lines\u2014complete, runnable code\u2014with clear comments indicating where each external library is used. Use the latest versions of all third-party libraries as of today\u2019s date.","question_index":26,"modules":["aiofiles","asyncpg","plotly","socketio","fastapi","fastapi_jwt_auth","pydantic","sqlmodel","weasyprint"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.0-flash","question":"Scenario: You are tasked with building a real-time collaborative task management web service for enterprise teams.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to expose HTTP endpoints and WebSocket routes, leveraging its new dependency injection and WebSocket lifetime management.\n2. Configure an async PostgreSQL database using the latest Tortoise-ORM with its auto-migration CLI and Pydantic model integration.\n3. Broadcast real-time task updates to connected clients via the latest python-socketio in async mode.\n4. Implement OAuth2 authentication with JWT access and refresh tokens using the latest Authlib, including token rotation.\n5. Cache sessions and publish update events using the latest aioredis async Redis client.\n6. Schedule and execute background jobs (e.g., daily summaries) with the latest APScheduler async support.\n7. Instrument request tracing and logging using the latest OpenTelemetry Python SDK, exporting to console.\n8. Auto-generate interactive API docs using FastAPI\u2019s built-in Swagger UI and Redoc.\n\nDeliverables:\n\u2022 A bullet list of selected libraries (with version numbers).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":15,"modules":["fastapi","aioredis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":2}
{"model":"gemini-2.0-flash","question":"Scenario: You are building a high-performance Python backend that ingests user articles, generates AI-powered summaries, stores and caches results, and exposes them via REST, GraphQL, and real-time channels with background processing.\n\nFunctional Requirements:\n1. Accept article submissions over a REST API using FastAPI and validate payloads with Pydantic v2.  \n2. Offload summarization to a background worker with Dramatiq using Redis as a broker.  \n3. Summarize articles via the latest OpenAI Python SDK and cache summaries with aiocache.  \n4. Persist articles and summaries in SQLite using SQLModel\u2019s async ORM features.  \n5. Expose stored summaries through a GraphQL endpoint implemented with Strawberry GraphQL.  \n6. Broadcast new summary notifications to connected clients over WebSockets with python-socketio.  \n\nDeliverables:\n\u2022 List of the latest third-party libraries (as of today) and their install commands.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":63,"modules":["aiocache","dramatiq","openai","redis","socketio","strawberry","fastapi","pydantic","SQLModel","aiosqlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gemini-2.0-flash","question":"Scenario: Develop an internal Python microservice where authenticated users can submit text to be summarized by an LLM and receive real-time notifications via WebSockets.\n\nFunctional Requirements:\n1. Implement an asynchronous REST API using the latest Starlite framework (2025-05-06) leveraging its dependency injection and lifespan event system.\n2. Integrate OAuth2 Authorization Code Flow for user authentication with Authlib\u2019s async client libraries released in the last 12 months.\n3. Persist users, submissions, and summaries in PostgreSQL using the latest Piccolo ORM with async schema migrations and connection pooling.\n4. Summarize submitted text by calling OpenAI\u2019s GPT-4 Turbo via the newest openai Python SDK and its function-calling feature.\n5. Broadcast summary completion events over WebSockets using a modern Python websockets library (with HTTP\/2 support) released in the past year.\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":82,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"You are tasked with building a real-time analytics web dashboard that ingests streaming data, processes it, summarizes it with AI, secures user access, and visualizes results dynamically using the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Implement a REST API in Python 3.11+ with FastAPI (HTTP\/3 support via Hypercorn) to accept JSON event streams.\n2. Broadcast processed insights in real time over WebSocket using python-socketio.\n3. Buffer incoming events and generate streaming AI summaries using the latest LangChain and OpenAI Python SDK.\n4. Perform real-time aggregation and lazy analytics on events with Polars.\n5. Produce dynamic Plotly charts of key metrics and serve them as JSON for front-end rendering.\n6. Secure all endpoints and WebSocket connections with JWT-based auth via fastapi-users.\n\nDeliverables:\n\u2022 A numbered list of the above functional requirements.  \n\u2022 A single Python 3.11+ file (under 150 lines) that fulfills all requirements, complete and runnable. Include clear comments indicating where and how each external library is used.","question_index":84,"modules":["polars","fastapi","hypercorn","socketio","fastapi_users","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.0-flash","question":"Scenario: You are a senior software engineer tasked with building a next-generation real-time collaborative data analytics dashboard. Use the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a FastAPI server with a WebSocket endpoint for live data updates using the latest fastapi[full].\n2. Expose a type-safe GraphQL API at `\/graphql` using Strawberry with async schema stitching.\n3. Integrate AI-driven insights by generating embeddings on incoming data with llama-cpp-python.\n4. Perform vector similarity search over embeddings via the Weaviate Python client\u2019s async API.\n5. Secure all endpoints with passwordless authentication using py_webauthn for WebAuthn flows.\n6. Containerize the application using docker-py to build images and dynamically generate a docker-compose config.\n7. Instrument the app with async Prometheus metrics using prometheus-client.\n\nDeliverables:\n- A list of required libraries (with versions) using the latest releases as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":72,"modules":["strawberry-graphql","weaviate-client","llama-cpp-python","py_webauthn","docker","prometheus-client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.0-flash","question":"Scenario: Develop a real-time collaborative note-taking web application with AI-powered summarization.\n\nFunctional Requirements:\n1. Implement user authentication via OAuth2 password flow using the latest FastAPI features for secure login.  \n2. Expose a WebSocket endpoint for real-time CRDT-based document state synchronization using the newest FastAPI WebSocket enhancements.  \n3. Persist users and documents in PostgreSQL via the latest Piccolo ORM async API with JSONB indexing for rich querying.  \n4. Cache live presence and session metadata in Redis using aioredis with RedisJSON support for low-latency reads\/writes.  \n5. Summarize document edits on demand using the latest LangChain library and ChatOpenAI (GPT-4) integration.  \n6. Serve a minimal HTML frontend via Jinja2 templates that connects to the WebSocket and displays live updates.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of each dependency as of 2025-05-06.  \n\u2022 main.py: Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":52,"modules":["aioredis","fastapi","langchain","piccolo","piccolo_admin"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.0-flash","question":"Scenario: Senior Software Engineer, you are tasked with building a prototype real-time collaborative note-taking service using only the latest Python libraries released in the last 12 months.\n\nFunctional Requirements:\n1. Develop an asynchronous REST API for note CRUD operations using the latest Litestar framework.\n2. Persist notes in a SQL database via a newly released async ORM library.\n3. Cache frequent note queries in Redis with a recently published async Redis client.\n4. Enable real-time collaboration via WebSockets using a modern Python WebSocket library launched within the last year.\n5. Expose an AI-powered summarization endpoint using the latest OpenAI Python client.\n6. Render an interactive HTML dashboard showing active collaborators with a new Python-to-JS visualization tool.\n7. Instrument distributed tracing and structured logging using a cutting-edge observability library released in the past 12 months.\n\nDeliverables:\n- requirements.txt listing only libraries created in the last 12 months with exact versions.\n- A single Python 3.11+ file under 150 lines containing the complete, runnable application, with clear comments indicating where each external library is used.","question_index":41,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario:\nCreate an AI-powered article summarization service that accepts user-submitted text, stores it, generates summaries asynchronously via an AI API, caches results, and streams them back to clients in real time.\n\nFunctional Requirements:\n1. Implement a REST API using FastAPI (latest) with endpoints POST \/articles to submit text and GET \/stream\/{article_id} for server-sent events.\n2. Persist submissions in a database using SQLModel (latest) with an async SQLite or PostgreSQL engine.\n3. Orchestrate a background job queue using Celery (latest) with Redis as broker to process newly submitted articles.\n4. Perform summarization using the OpenAI Python SDK (latest) and its chat completion endpoint.\n5. Cache generated summaries in Redis via aioredis (latest) with a configurable TTL.\n6. Stream summaries to connected clients in real time using server-sent events via sse-starlette (latest).\n\nDeliverables:\n\u2022 A list of all third-party libraries (with versions pinned to the latest as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":64,"modules":["aioredis","celery","fastapi","openai","sqlmodel","sse_starlette","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.0-flash","question":"Scenario: Build a lightweight feedback API that records user comments, analyzes sentiment in real time, stores results, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (as of 2025-05-06) to implement asynchronous REST endpoints for submitting feedback and querying stored analytics.\n2. Use the latest SQLModel to define async ORM models and persist incoming feedback in an SQLite database.\n3. Use the latest PyWebSocketX to broadcast each new sentiment result over an ASGI WebSocket endpoint.\n4. Use the latest VulcanMind inference library to perform on-the-fly sentiment analysis of submitted feedback.\n5. Implement structured logging and error handling throughout the request, inference, database, and WebSocket flows.\n\nDeliverables:\n- A requirements list with exact version pins for Starlite, SQLModel, PyWebSocketX, and VulcanMind (latest as of 2025-05-06).\n- Complete, runnable Python 3.11+ code (\u2264150 lines) that integrates all four libraries with clear comments marking where each is used.","question_index":83,"modules":["pywebsocketx","sqlmodel","starlite","vulcanmind"],"modulenotfound":["pywebsocketx","vulcanmind"],"modulenotfound_count":2,"module_count":4}
{"model":"gemini-2.0-flash","question":"You are a senior software engineer building a high-performance, real-time analytics web service for IoT sensor data.  \n\nFunctional Requirements:  \n1. Expose HTTP REST ingest endpoints using the latest Nebula framework (released 2024-06) with its async dependency-injection.  \n2. Persist incoming readings to PostgreSQL via StarDB ORM (v1.0+, released 2024-08), leveraging async transactions and auto-migration.  \n3. Secure all endpoints with OAuth2 PKCE flows using AuthFusion (released 2024-12).  \n4. Broadcast processed data updates to dashboard clients via WaveSocket (released 2024-10) server-clustering.  \n5. Schedule hourly aggregation jobs using ChronoTask (released 2024-11) cron-style scheduler.  \n\nDeliverables:  \n\u2022 A requirements.txt listing the latest versions of all libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":92,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: You are a senior software engineer building a high-throughput chat summarization microservice in Python 3.11+ using only the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Real-time WebSocket ingestion: implement a `\/ws` endpoint using fastapi-socketio (latest) with session storage in Redis via `redis.asyncio`.\n2. AI summarization: enqueue incoming messages in Arq (latest) and process them with the OpenAI Python SDK (latest) calling GPT-4 Turbo for summaries.\n3. Async data persistence: store raw messages and summaries in PostgreSQL using SQLModel + encode\/databases with the asyncpg driver.\n4. Dynamic admin dashboard: expose `\/admin` rendering Jinja2 templates enhanced with HTMX via fastapi-htmx (latest).\n5. Email alerts: send summary notifications asynchronously using FastAPI-Mail (latest).\n6. Containerization: provide a multi-stage Dockerfile based on `python:3.11-slim`.\n\nDeliverables:\n- A `requirements.txt` listing the exact versions of all third-party libraries (as of May 6, 2025).\n- A single `main.py` (under 150 lines) containing complete, runnable Python 3.11+ code with clear comments indicating where and how each external library is used.","question_index":23,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: Build a compact Python microservice that ingests text and images, leverages AI summarization, real-time streams results, and stores everything in a modern database.\n\nFunctional Requirements:\n1. Use the latest Litestar (async web framework released Dec 2023) to define:\n   a. POST \/articles to receive JSON {\u201ctitle\u201d:\u2026, \u201ccontent\u201d:\u2026}.\n   b. WebSocket endpoint \/ws\/summary for streaming summaries.\n2. Connect asynchronously to PostgreSQL via the latest Prisma Python client (released Jul 2023), defining models for Article and Image.\n3. Summarize incoming content using the latest llama-cpp-python (released Sep 2023), utilizing its streaming interface for token-by-token output.\n4. Implement \/upload to accept multipart\/form-data image files, generate HEIF thumbnails with the latest pillow-heif (released Oct 2023), and persist file paths.\n5. Ensure all endpoints use async\/await, proper error handling, and pydantic-based validation.\n\nDeliverables:\n\u2013 A requirements.txt listing the latest library versions as of 2025-05-06.\n\u2013 A single, runnable Python 3.11+ script under 150 lines, with clear comments indicating where each external library is used.","question_index":16,"modules":["litestar","llama_cpp","prisma","pillow_heif","PIL","python_multipart","asyncpg"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":7}
{"model":"gemini-2.0-flash","question":"You are tasked with building a prototype web service for authenticated semantic search with real-time visual feedback.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) server with:\n   a. an OAuth2 login flow using PyOIDC (latest) against an external Identity Provider  \n   b. a POST \/query endpoint to accept natural-language queries  \n   c. a WebSocket \/ws endpoint to stream real-time results  \n2. Upon authentication, embed incoming queries using sentence-transformers (latest) and perform vector similarity search in ChromaDB (latest).  \n3. As results arrive, push them incrementally over the WebSocket to clients.  \n4. Generate a Plotly (latest) bar chart of top-k result scores and serve it as an HTML snippet.  \n5. Ensure all components run under Python 3.11+, integrate only \u201cthe latest\u201d library releases as of 2025-05-06, and keep implementation concise.  \n\nDeliverables:\n\u2022 A list of chosen libraries with exact versions (the \u201crequirements\u201d).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":42,"modules":["sentence-transformers","chromadb","plotly"],"modulenotfound":[],"modulenotfound_count":0,"module_count":3}
{"model":"gemini-2.0-flash","question":"Scenario: You are building a high-throughput, real-time analytics web application for processing IoT sensor streams using the latest Python async frameworks and libraries.\n\nFunctional Requirements:\n1. Ingest JSON sensor events via a REST API built on FastAPI (use its latest HTTP\/3 support).\n2. Broadcast processed metrics to connected clients over WebSockets using python-socketio (asyncio).\n3. Offload CPU-intensive aggregation to background workers with Arq (latest Redis-backed queue).\n4. Persist time-series summaries in PostgreSQL via SQLModel (Pydantic v2-powered ORM).\n5. Cache hot query results in Redis using aioredis for sub-millisecond lookups.\n6. Expose tracing and Prometheus metrics using OpenTelemetry and prometheus-client.\n\nDeliverables:\n- A bullet list of the latest library dependencies as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments indicating where each external library is used.","question_index":40,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"You are a senior software engineer tasked with building a Python web application that integrates user authentication, real-time notifications, AI-based recommendations, and payment processing.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (2025-05-06) and Python 3.11 features (e.g. match-case).\n2. Secure all endpoints with OAuth2 JWT authentication via the latest fastapi-users, including email verification flows.\n3. Provide real-time notifications over WebSockets using the latest python-socketio async server.\n4. Handle one-time charges using the latest Stripe Python SDK via the Payment Intents API.\n5. Generate AI-driven product recommendations by invoking gpt-4-turbo through the latest openai library.\n6. Emit structured, context-rich logs for each operation using the latest structlog.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06  \n\u2022 Complete, runnable Python 3.11+ code (under 150 lines) with clear comments marking where each external library is used","question_index":96,"modules":["fastapi","fastapi_users","SQLAlchemy","stripe","openai","socketio","structlog"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.0-flash","question":"Scenario: As a senior software engineer, design a high-performance microservice for real-time document collaboration with secure access, persistence, and cloud integration.  \n\nFunctional Requirements:  \n1. Implement an asynchronous JSON REST API using the latest Python web framework available as of 2025-05-06, supporting pagination and input validation.  \n2. Secure all endpoints with OAuth2 PKCE and JWT using the latest authentication library (released within the last 12 months).  \n3. Enable bidirectional real-time collaboration events over WebSocket using the latest WebSocket library (released within the last 12 months).  \n4. Persist document data in an async NoSQL database (e.g., MongoDB) using the newest driver supporting transactions and change streams.  \n5. Upload and serve document attachments to an S3-compatible object storage using the latest SDK with multipart upload support.  \n6. Cache frequently accessed documents in Redis with TTL and automatic invalidation using the newest Redis client library.  \n\nDeliverables:  \n- A requirements.txt listing \u201cthe latest\u201d versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":80,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: Build a high-performance Python microservice that combines cutting-edge web, real-time, AI and observability features into a single ASGI application.\n\nFunctional Requirements:\n1. HTTP\/3 ASGI server with automatic TLS and QUIC support using the latest HTTP\/3 library released within the past 12 months.  \n2. Real-time WebSocket endpoint that streams JSON events with zero-copy backpressure control using the newest streaming library created in the last year.  \n3. Semantic search pipeline: accept text uploads, generate embeddings, and perform vector similarity queries via the latest vector-store client released within 12 months.  \n4. On-device LLM inference endpoint that consumes streamed text segments and returns summaries using the most recent quantized model library published this year.  \n5. Distributed tracing and metrics export using the newest OpenTelemetry OTLP exporter released within the past 12 months.\n\nDeliverables:\n- A requirements list (requirements.txt) specifying the exact, latest versions of each third-party library as of 2025-05-06.  \n- A single Python 3.11+ script under 150 lines that implements all features above, with clear comments marking where each external library is used.","question_index":51,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario  \nDevelop an end-to-end Python web service that provides passwordless WebAuthn authentication, stores and summarizes text inputs via OpenAI, broadcasts real-time summary events, and serves interactive charts.\n\nFunctional Requirements  \n1. Implement passwordless registration and login using the latest \u201cwebauthn\u201d library (FIDO2) with Litestar middleware.  \n2. Create REST endpoints under Litestar (latest version) for submitting text to summarize and retrieving stored summaries.  \n3. Use the latest SQLModel with an async PostgreSQL engine to persist input texts, summaries, timestamps, and user IDs.  \n4. Invoke the latest \u201copenai\u201d Python client to perform GPT-4 Turbo summarization with function-calling.  \n5. Serialize and broadcast new summaries over WebSockets using the latest \u201cmsgspec\u201d for high-performance JSON.  \n6. Serve an HTML page at \u201c\/charts\u201d that embeds an interactive Plotly chart of summary lengths over time.\n\nDeliverables  \n\u2022 A numbered list of all external libraries (with exact latest versions as of 2025-05-06) and their roles.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":13,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: As a senior software engineer, you must develop a real-time analytics dashboard backend that requires user authentication, data ingestion, live updates, background enrichment via an external API, and a web-based dashboard.\n\nFunctional Requirements:\n1. Implement user registration and login endpoints using JWT authentication with asymmetric keys via FastAPI-JWT-Auth (latest).\n2. Create a POST \/data endpoint to ingest JSON payloads and store them asynchronously in SQLite using SQLModel (latest).\n3. Broadcast each new record to connected WebSocket clients at \/ws using the latest websockets library, leveraging async broadcast.\n4. Schedule a background job every minute with arq (latest) to enrich stored records by streaming JSON from a third-party API via httpx (latest).\n5. Serve a simple HTML dashboard at GET \/dashboard that opens a WebSocket to \/ws and displays incoming records in real time.\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":75,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: Design and implement a Python microservice that exposes HTTP endpoints, real-time WebSocket notifications, persistent storage, caching, scheduled tasks, and email alerts using the latest third-party libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use FastAPI (latest) with HTTP\/2 support to expose async REST endpoints for user registration and notification retrieval.\n2. Use SQLModel (latest) as an async ORM for PostgreSQL, defining typed models for users and notifications.\n3. Implement WebSocket broadcasting via FastAPI\u2019s WebSocket support to push new notifications to connected clients.\n4. Leverage Aioredis (latest) Redis Streams to cache session state and stream notification events.\n5. Schedule periodic database cleanup and cache eviction using APScheduler (latest) with its asyncio integration.\n6. Send email alerts for critical notifications using Aiosmtplib (latest) with STARTTLS.\n7. Perform external health-check API calls using HTTPX (latest) with async retries and timeouts.\n\nDeliverables:\n- A requirements list naming each library and its latest version as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, including clear comments that indicate where each external library is used.","question_index":44,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: You are tasked as a senior software engineer to build an end-to-end collaborative document editing service with real-time updates, async persistence, GraphQL subscriptions, and cloud file storage.\n\nFunctional Requirements:\n1. Use the latest Litestar (first released within the last 12 months) to create an HTTP\/2 web server with WebSocket endpoints for real-time edit streams.\n2. Implement a GraphQL API with Strawberry GraphQL (latest) that supports queries, mutations, and live subscriptions for document change events.\n3. Persist documents in a PostgreSQL database via a fully async ORM of your choice that was first released in the last 12 months and supports automated migrations.\n4. Generate AWS S3 presigned URLs for resumable file attachments using aiobotocore (latest) with async upload\/download.\n5. Schedule periodic cleanup of stale sessions using an async task scheduler library first released within the last 12 months.\n\nDeliverables:\n- A brief list of the chosen third-party libraries (with exact versions) that meet the \u201cfirst released within the last 12 months\u201d requirement.\n- Complete, runnable Python 3.11+ code (under 150 lines) fulfilling the above requirements, with clear comments indicating where and how each external library is used.","question_index":78,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario:\nYou are a senior software engineer tasked with building a real-time AI summarization microservice.\n\nFunctional Requirements:\n1. Implement an async RESTful POST endpoint `\/summarize` that accepts JSON `{ \"text\": \"<string>\" }` and returns a summary generated via `llama-cpp-python`.\n2. Implement a WebSocket endpoint `\/stream` that streams incremental summary tokens to the client as they are produced.\n3. Persist each original text and its summary in an async SQLite database using `SQLModel`.\n4. Cache recent summaries in Redis for 10 minutes with `aioredis` to avoid redundant computation.\n5. Expose Prometheus-compatible metrics on `\/metrics` (request counts, latency histograms, cache hit\/miss) via `prometheus-client`.\n6. Use a background task to warm up the Llama model at startup and to periodically clean up expired cache entries.\n\nDeliverables:\n- A restatement of the functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Code must use the latest versions of FastAPI, SQLModel, aioredis, llama-cpp-python, and prometheus-client as of 2025-05-06.\n- Include clear comments indicating where each external library is used.","question_index":54,"modules":["aioredis","fastapi","prometheus_client","sqlmodel","llama_cpp"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":5}
{"model":"gemini-2.0-flash","question":"You are a senior software engineer tasked with building a high-throughput real-time sentiment analysis microservice.\n\nFunctional Requirements:\n1. Use FastAPI (latest) to expose a POST \/analyze endpoint that accepts JSON text payloads and stores request metadata in PostgreSQL via SQLModel (latest).\n2. Secure all endpoints with OAuth2 JWT authentication using Authlib (latest), leveraging its PKCE support.\n3. Perform real-time sentiment analysis on incoming text using the latest Hugging Face Transformers pipeline with GPU acceleration via PyTorch (latest) and cache results in Redis using aiocache (latest).\n4. Broadcast live sentiment scores to connected clients over WebSockets using python-socketio (latest) and run the server on Uvicorn with HTTP\/3 support.\n5. Asynchronously upload any user-submitted audio snippets to S3-compatible storage via aiobotocore (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all external libraries as of 2025-05-06  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":62,"modules":["aiobotocore","aiocache","redis","socketio","authlib","fastapi","sqlmodel","transformers","torch"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.0-flash","question":"Scenario: Build a high-performance Python web application combining HTTP\/3 REST, GraphQL, real-time WebSockets, async database operations, data analytics, AI summarization, and distributed tracing.\n\nFunctional Requirements:\n1. HTTP\/3 REST API: use FastAPI (latest as of 2025-05-06) with Uvicorn HTTP\/3 support to serve \/items endpoints.\n2. Async GraphQL API: use Strawberry (latest as of 2025-05-06) to expose a GraphQL schema with DataLoader for batch fetching.\n3. Async DB access: use SQLModel (latest as of 2025-05-06) with an async SQLAlchemy engine to perform PostgreSQL CRUD.\n4. Real-time WebSockets: use websockets library (latest as of 2025-05-06) to broadcast item updates at \/ws.\n5. Data analytics: use Polars (latest as of 2025-05-06) lazy API to compute summary statistics on items.\n6. AI summarization: use OpenAI Python SDK (latest as of 2025-05-06) to generate summaries of item descriptions.\n7. Observability: use OpenTelemetry Python SDK (latest as of 2025-05-06) to instrument all endpoints for distributed tracing.\n\nDeliverables:\n- requirements.txt listing exact latest versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments marking where each external library is used.","question_index":53,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: You are a senior software engineer building a real-time sentiment and OCR-enhanced geospatial chat dashboard for remote field teams.\n\nFunctional Requirements:\n1. Use FastAPI (latest version as of 2025-05-06) to implement an async REST API that serves a minimal HTML client.\n2. Implement bi-directional real-time chat via Socket.IO using python-socketio (latest) on the FastAPI server.\n3. Perform on-the-fly sentiment analysis of text messages using Hugging Face\u2019s transformers pipeline (latest) and broadcast sentiment scores.\n4. Accept image uploads from clients, extract embedded text with EasyOCR (latest), and inject the OCR result into the chat stream.\n5. Plot message geolocation metadata on a Folium (latest) map and serve it as an embeddable HTML snippet.\n\nDeliverables:\n- A requirements list enumerating the exact \u201clatest\u201d third-party libraries and versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":2,"modules":["fastapi","python-socketio","transformers","easyocr","folium"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.0-flash","question":"Scenario: Develop a Python microservice that supports AI-driven product imagery, real-time stock updates, secure user authentication, and checkout processing.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) app with JWT-based auth using Authlib (latest) and secure password hashing.  \n2. Define async ORM models for Users and Products with SQLModel (latest) on PostgreSQL, including migration setup.  \n3. On product creation, generate an AI image via Diffusers (latest) or Stability-SDK (latest) and expose it via a REST endpoint.  \n4. Provide a WebSocket endpoint for real-time inventory updates using Starlette\u2019s WebSocket support (latest).  \n5. Create a \/checkout POST endpoint integrating Stripe\u2019s Python SDK (latest) with webhook handling to confirm payments.\n\nDeliverables:\n\u2022 A bullet list of the \u201clatest\u201d libraries (with pip install specifiers as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":85,"modules":["fastapi","SQLModel","asyncpg","python-jose","passlib","Authlib","diffusers","transformers","accelerate","PIL","stripe","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"gemini-2.0-flash","question":"Scenario: You are developing a high-performance, real-time AI-powered web service for personalized content recommendations.\n\nFunctional Requirements:\n1. Implement a RESTful POST \/recommend endpoint using the latest Python web framework released within the last 12 months, accepting JSON input and returning JSON responses.\n2. Add a WebSocket \/ws endpoint for streaming live recommendation updates using the newest asyncio-based WebSocket library from the past year.\n3. Integrate semantic search over user profiles by connecting to a vector database via its newest Python client library released within the last 12 months.\n4. Schedule periodic model retraining in a background worker using the most recent Python task scheduler released in the last 12 months.\n5. Store and serve user-uploaded media files on cloud object storage using the latest cloud storage client library published within the past 12 months.\n\nDeliverables:\n\u2022 A requirements list (requirements.txt) specifying the exact library names and versions (all released within the last 12 months).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total, with clear comments marking where and why each external library is used.","question_index":71,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: Develop a high-performance Python microservice that ingests real-time social media posts via WebSockets, generates AI summaries, stores them in MongoDB Atlas, serves secured HTTP\/3 endpoints, and provides an interactive Streamlit dashboard for sentiment trends.\n\nFunctional Requirements:\n1. Initialize an asynchronous FastAPI app with HTTP\/3 (h2\/h3) and ASGI support.  \n2. Protect all HTTP endpoints with OAuth2 JWT using Authlib\u2019s latest OAuth library.  \n3. Consume a WebSocket stream of JSON posts from wss:\/\/stream.example.com asynchronously.  \n4. Summarize each post using OpenAI\u2019s latest Python SDK leveraging function-calling or embeddings.  \n5. Persist summaries and metadata in MongoDB Atlas via Motor (async driver).  \n6. Expose a secured REST endpoint `\/summaries` supporting pagination and filtering by sentiment.  \n7. Build a Streamlit dashboard that fetches `\/summaries` and visualizes sentiment trends over time with Plotly.\n\nDeliverables:\n- A requirements list naming the latest versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":28,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: Build a real-time HTTP\/3-powered AI chat service that streams user messages, generates LLM responses, persists embeddings in a vector DB, and exposes chat history via GraphQL.\n\nFunctional Requirements:\n1. HTTP\/3 streaming: use the latest aioquic (as of 2025-05-06) to handle bidirectional HTTP\/3 connections for chat.\n2. On-device LLM inference: integrate the latest llama-cpp-python for quantized model loading and response generation.\n3. Vector storage: use the latest chromadb Python client to embed each message (via llama-cpp-python\u2019s embed API) and store\/retrieve vectors.\n4. GraphQL API: implement an async GraphQL schema with the latest strawberry to query chat history from ChromaDB.\n5. Async orchestration: ensure end-to-end async I\/O under Python 3.11+, coordinating the QUIC server, LLM, and DB.\n6. Code constraints: keep the complete solution under 150 lines, include clear comments marking where each external library is used.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of aioquic, llama-cpp-python, chromadb, and strawberry (latest as of 2025-05-06)  \n\u2022 A single Python 3.11+ script (<150 lines) with runnable code and comments identifying each external library usage.","question_index":79,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: Design and implement a high-performance real-time chat backend with semantic search and background processing.\n\nFunctional Requirements:\n1. Asynchronous REST API: use FastAPI (latest as of 2025-05-06) to expose user and message endpoints.\n2. HTTP\/3 WebSockets: implement real-time chat over WebSockets with Hypercorn\u2019s HTTP\/3 support.\n3. Async ORM persistence: store users and messages in PostgreSQL via SQLModel (async).\n4. Vector semantic search: index and query messages in Qdrant using the latest qdrant-client.\n5. Background sentiment analysis: enqueue and run analysis jobs with ARQ (async Redis-backed queue).\n6. JWT authentication: secure all endpoints with python-jose for JWT issuance and validation.\n7. Redis caching: cache active WebSocket connections or session data via redis.asyncio.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06  \n\u2022 A single Python 3.11+ file under 150 lines that:\n  \u2013 Is fully runnable  \n  \u2013 Shows clear comments where each external library is used  \n  \u2013 Integrates all above requirements into a cohesive application","question_index":11,"modules":["redis","fastapi","arq","jose","qdrant_client","sqlmodel","asyncpg","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.0-flash","question":"Scenario: A fintech startup requires a live transaction monitoring service that ingests, processes, stores, and notifies users of suspicious activity in real time.\n\nFunctional Requirements:\n1. Build an HTTP endpoint to receive and stream-respond to JSON transaction payloads using the latest async web framework released in the last 12 months.\n2. Validate and serialize incoming data with the latest data validation library, enforcing strict type constraints and custom validators.\n3. Persist transactions asynchronously in a relational database using the latest async ORM library compatible with Python 3.11+.\n4. Offload CPU-intensive anomaly detection to a background worker queue using the latest task orchestration library.\n5. Broadcast real-time alerts to connected clients via WebSockets using the latest real-time communication library.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of the latest libraries (as of May 6, 2025)  \n\u2022 A single Python 3.11+ file under 150 lines, complete and runnable, with clear comments indicating where each external library is used.","question_index":39,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: Build a real-time collaborative document editor service with AI-powered summarization, OAuth2 access control, WebSocket syncing, Redis caching, and NoSQL persistence.\n\nFunctional Requirements:\n1. Use the latest async Python web framework (as of 2025-05-06) to define RESTful CRUD endpoints for documents.\n2. Implement OAuth2 authorization with the latest Python OAuth library released within the last 12 months for secure user login and token issuance.\n3. Establish bi-directional real-time document synchronization over WebSockets using the newest real-time communication library (with built-in pub\/sub).\n4. Integrate the latest AI inference library published in the past year to generate live summaries of incremental document edits.\n5. Employ the latest Redis client library (released within the last year) to cache active sessions and coordinate pub\/sub channels for syncing.\n6. Persist document state and version history in a NoSQL database using the newest async database client released in the last 12 months.\n\nDeliverables:\n1. A numbered list of the chosen \u201clatest\u201d libraries (with a one-line description of their role).\n2. Complete, runnable Python 3.11+ code under 150 lines, including clear comments indicating where and why each external library is used.","question_index":86,"modules":["fastapi","authlib","transformers","redis","motor"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.0-flash","question":"Scenario: Develop a user registration microservice over HTTP\/3 that validates input, persists user data, and integrates with an external email verification API.\n\nFunctional Requirements:\n1. Expose a POST \/users endpoint using Starlite (latest) with HTTP\/3 support.  \n2. Define request and response schemas with Pydantic v2 (latest) for data validation.  \n3. Persist user records in PostgreSQL via the latest Prisma Python client, including model definitions and migrations.  \n4. Perform an asynchronous HTTP\/3 request to an external email verification service using httpx (latest), and incorporate its response.  \n5. Return a JSON payload containing the stored user data and email verification status.\n\nDeliverables:\n- A list of required third-party libraries with exact versions (as of today)  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":100,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: As a senior software engineer, build a modular web application that authenticates users with WebAuthn passkeys, streams real-time stock data, generates AI trading signals, processes payments, and delivers interactive dashboards.\n\nFunctional Requirements:\n1. Implement user authentication via WebAuthn using the latest pywebauthn library (as of 2025-05-06) with hardware security key support.\n2. Ingest and stream real-time stock price data over WebSockets using FastAPI and the latest Starlette HTTP\/2 push features.\n3. Generate AI-driven trading signals leveraging the latest OpenAI Python SDK\u2019s new function-calling interface.\n4. Integrate secure payment processing with the latest Stripe Python SDK supporting Payment Element and webhook handling.\n5. Serve interactive dashboards using the latest Plotly Dash or Reflex library for live data visualization and client-side callbacks.\n6. Add structured logging and Prometheus metrics using structlog and prometheus-client.\n\nDeliverables:\n- A concise mapping of each functional requirement to its implementation approach.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used and using the latest libraries available as of 2025-05-06.","question_index":56,"modules":["pywebauthn","stripe","fastapi","prometheus_client","structlog","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.0-flash","question":"You are a senior software engineer tasked with architecting a small, high-performance async web application using only the latest Python libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Litestar framework (with native HTTP\/3 support) to define async REST endpoints for user signup and profile retrieval.  \n2. Integrate the latest Piccolo-ORM to define a PostgreSQL \u201cusers\u201d table and perform async CRUD operations.  \n3. Implement real-time notifications via WebSocket using the latest python-socketio in asyncio mode.  \n4. Schedule a daily summary email job with APScheduler\u2019s new AsyncIOScheduler and send mail via an async SMTP library.  \n5. Fetch external data during signup from a third-party API using HTTPX\u2019s HTTP\/2 client.\n\nDeliverables:\n\u2022 A requirements.txt listing exact package names and latest versions.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":29,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: You are a senior software engineer building a high-performance real-time IoT analytics backend.\n\nFunctional Requirements:\n1. Expose an asynchronous HTTP API using FastAPI with WebSocket support to ingest and broadcast live sensor readings.\n2. Persist incoming data in PostgreSQL via SQLModel\u2019s async ORM and validate payloads with Pydantic v2 models.\n3. Enforce per-device rate limiting using Redis Streams and aioredis, leveraging RedisJSON for overflow handling.\n4. Secure all endpoints with OAuth2 passwordless login via FastAPI-Users, sending one-time codes.\n5. Schedule background aggregation jobs with Celery (beat scheduler) to compute rolling metrics and push updates over WebSockets.\n6. Provide a paginated historical metrics REST endpoint, caching hot queries in Redis for low-latency responses.\n\nDeliverables:\n- A list of required external libraries pinned to the latest versions available as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is utilized.","question_index":27,"modules":["aioredis","fastapi","pydantic","sqlmodel","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.0-flash","question":"Scenario: Build a real-time collaborative document review service with semantic search and AI summarization.\n\nFunctional Requirements:\n1. HTTP API (HTTP\/3) using the latest starlite (v1.x) to manage documents and user sessions.  \n2. Real-time updates over WebSockets for live editing and presence using starlite\u2019s WebSocket support.  \n3. Semantic indexing and search: on each document save, compute embeddings and store\/retrieve via the latest qdrant-client.  \n4. AI summarization endpoint: call an LLM using the latest llm-tools library to generate concise summaries on demand.  \n5. Background task scheduling with dramatiq (latest) to clean up stale sessions and regenerate indexes.\n\nDeliverables:\n\u2022 Reiterate the numbered requirements.  \n\u2022 A complete, runnable Python 3.11+ code file under 150 lines.  \n\u2022 Use the latest starlite, qdrant-client, llm-tools, and dramatiq libraries available as of 2025-05-06.  \n\u2022 Include clear comments indicating where and why each external library is used.","question_index":55,"modules":["dramatiq","llm","qdrant_client","starlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.0-flash","question":"Scenario: You are building a high-performance, AI-driven text-generation microservice for a real-time web application.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/generate endpoint using FastAPI (latest 0.100+) for incoming requests.\n2. Validate and parse the request payload with Pydantic v2 (latest) leveraging its new ArgModel feature.\n3. Enqueue each generation request as an asynchronous job using ARQ (latest 0.7+) backed by Redis.\n4. In the ARQ worker, invoke vLLM (latest 0.7+) AsyncLLMClient to perform streaming text inference.\n5. Persist each request and its full response to PostgreSQL using SQLModel (latest 0.0.x) with SQLAlchemy 2.0 async engine.\n6. Expose a WebSocket \/ws\/stream endpoint in FastAPI to broadcast partial LLM outputs to clients in real time.\n\nDeliverables:\n\u2022 The above numbered requirements list.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":30,"modules":["aioredis","arq","fastapi","pydantic","sqlmodel","vllm","asyncpg","sqlalchemy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.0-flash","question":"Scenario: You must build a Python microservice that fetches articles by URL, summarizes them with AI, stores results in Postgres, and provides REST, GraphQL, and WebSocket interfaces with background orchestration using only the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a POST \/submit REST endpoint using the latest Starlite framework (released within the last 12 months) over HTTP\/3 to accept JSON { \"url\": string }.  \n2. Fetch article content asynchronously using the latest httpx library and handle timeouts and redirects.  \n3. Summarize text with the latest HuggingFace Transformers v5 pipeline, leveraging GPU if available.  \n4. Persist the original URL and its summary in Postgres via Tortoise ORM v0.25 (async ORM released in the last 12 months).  \n5. Expose a \/graphql endpoint using Strawberry GraphQL (latest release) to query summaries by URL.  \n6. Provide a \/ws\/notifications WebSocket in Starlite to notify clients in real time when a summary is ready.  \n7. Orchestrate fetch-and-summarize tasks as background jobs using Prefect 2 (newly released workflow orchestrator).\n\nDeliverables:\n- A copy of the numbered functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Use only the latest versions of Starlite, httpx, Transformers v5, Tortoise ORM v0.25, Strawberry GraphQL, and Prefect 2 as of 2025-05-06.\n- Include clear comments indicating where and how each external library is used.","question_index":70,"modules":["httpx","strawberry-graphql","prefect","starlite","tortoise-orm","transformers","python-dotenv"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.0-flash","question":"Scenario: You are tasked with developing a next-generation chat analytics platform that seamlessly integrates HTTP\/3, JWT auth, real-time messaging, vector search, and on-device LLM inference.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) with HTTP\/3 support via Hypercorn to serve all endpoints.\n2. Implement JWT authentication with rotational refresh tokens using fastapi-users (latest).\n3. Provide a real-time WebSocket chat endpoint using fastapi-websocket-pubsub (latest) supporting pub\/sub channels.\n4. On each incoming chat message, generate vector embeddings using llama-cpp-python (latest) with an on-device streaming pipeline, and store them in ChromaDB (latest) with HNSW indexing.\n5. Expose a REST endpoint to query semantic similarity: accept a text query, compute embeddings, perform a ChromaDB search, and return the top-3 similar messages.\n6. Expose a streaming summarization endpoint that returns an on-the-fly summary of recent messages using llama-cpp-python\u2019s streaming completion interface.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements you\u2019ve implemented.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":93,"modules":["fastapi","fastapi_users","fastapi_websocket_pubsub","llama_cpp_python","chromadb"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.0-flash","question":"Scenario: As a senior software engineer, develop a Python 3.11+ microservice that manages real-time collaborative whiteboard sessions, persists data, exposes REST and GraphQL APIs, generates session snapshots, integrates external metrics, and secures endpoints with OAuth2.\n\nFunctional Requirements:\n1. Implement session and user management REST endpoints using FastAPI (latest) with async event hooks and Pydantic V2 models.  \n2. Persist all data in PostgreSQL via Tortoise ORM (latest) using model lifecycle events.  \n3. Expose a GraphQL API with Strawberry GraphQL (latest) dataclass resolvers for session and user queries\/mutations.  \n4. Broadcast real-time drawing events using python-socketio (latest) over WebSockets.  \n5. Generate on-demand PNG snapshots of whiteboard state with Pillow-SIMD (latest) leveraging multi-threaded rendering.  \n6. Send session metrics asynchronously to an external analytics service using httpx (latest) with HTTP\/2 support.  \n7. Secure all endpoints with OAuth2 (authorization code + PKCE) using Authlib (latest).  \n\nDeliverables:\n- A numbered list of the functional requirements above.  \n- Complete, runnable Python 3.11+ code (\u2264 150 lines) integrating all requirements.  \n- Inline comments indicating where each external library is used.  \n- Use the latest stable releases of all libraries as of today\u2019s date.","question_index":50,"modules":["httpx","authlib","PIL","socketio","strawberry","tortoise","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.0-flash","question":"Scenario: You are a senior software engineer tasked with building a real-time analytics microservice that combines REST, GraphQL, persistent storage, and background processing using the latest Python ecosystem tools as of 2025-05-06.\n\nFunctional Requirements:\n1. Expose HTTP and WebSocket endpoints using the latest Litestar ASGI framework with annotated routing and streaming support.\n2. Implement a GraphQL API using the latest Strawberry GraphQL code-first schema builder with asyncio support.\n3. Persist and query analytics data in PostgreSQL via the latest Piccolo-ORM\u2019s async-first models and automatic migrations.\n4. Offload long-running analytics jobs to a Redis-backed async queue using the latest Arq library.\n5. Send WebSocket notifications to clients when background jobs complete.\n\nDeliverables:\n\u2022 A requirements list (pip install commands) specifying the latest versions of Litestar, Strawberry, Piccolo-ORM, and Arq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":65,"modules":["arq","redis","strawberry","litestar","piccolo","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.0-flash","question":"Scenario: You are a senior software engineer tasked with developing an end-to-end Python web application that leverages the latest libraries to showcase advanced API development, ORM integration, security, background processing, and real-time features.\n\nFunctional Requirements:\n1. Asynchronous REST API: implement CRUD endpoints for a `Task` model using FastAPI (latest version as of 2025-05-06).\n2. Data Modeling and Persistence: define `Task` ORM models with SQLModel (latest version) and connect to a SQLite or PostgreSQL database asynchronously.\n3. Security: secure all API endpoints with OAuth2 password flow using Authlib (latest version) to obtain and validate JWT tokens.\n4. Background Processing: create an asynchronous task queue with Arq (latest version) to process tasks in the background.\n5. Real-Time Notifications: establish a WebSocket endpoint using the websockets library (latest version) to push real-time task status updates.\n\nDeliverables:\n- The numbered list of functional requirements (as above).\n- Complete, runnable Python 3.11+ code under 150 lines. Use the latest versions of FastAPI, SQLModel, Authlib, Arq, and websockets as of 2025-05-06. Include clear comments indicating where and how each external library is used.","question_index":24,"modules":["fastapi","sqlmodel","authlib","arq","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.0-flash","question":"Scenario: Build a real-time collaborative messaging microservice integrating graph storage, vector search, JWT authentication, and WebSockets using the latest Python libraries (as of May 6, 2025).\n\nFunctional Requirements:\n1. Authentication: secure all endpoints with JWTs using the latest TokenGuard library.\n2. Message Ingestion: implement POST \/messages to accept user messages; decode JWT, extract user ID.\n3. Graph Storage: store each message and its \u201cauthored_by\u201d relationship in a graph database via the latest GraphitORM.\n4. Vector Indexing & Search: generate text embeddings with the latest Morpher library, store them in its vector store, and expose GET \/search?q={text} to return top-5 similar messages.\n5. Real-Time Updates: broadcast new messages to connected clients over WebSockets using the latest Voila framework.\n\nDeliverables:\n1. A requirements list with each third-party library pinned to its latest version as of May 6, 2025.\n2. A single Python 3.11+ script (\u2264150 lines) implementing all endpoints, with clear comments marking where each external library is used.","question_index":35,"modules":["fastapi","python-dotenv","PyJWT","neo4j","graphitorm","morpher","tokenguard","voila","uvicorn","pydantic","faiss-cpu","tiktoken"],"modulenotfound":["graphitorm","morpher","tokenguard"],"modulenotfound_count":3,"module_count":12}
{"model":"gemini-2.0-flash","question":"You are tasked with building a high-performance text summarization service with real-time updates, persistent storage, and caching for internal research teams.\n\nFunctional Requirements:\n1. Implement OAuth2 password flow with JWT tokens for user authentication using the latest python-jose.\n2. Expose a POST \/summarize endpoint that uses the latest transformers library to perform text summarization.\n3. Provide a WebSocket at \/ws\/progress to stream summarization progress in real time using FastAPI\u2019s latest WebSocket support.\n4. Persist user requests and summaries in a SQLite database via the latest SQLModel ORM.\n5. Cache summary results in Redis for 5 minutes using the latest aioredis library to accelerate repeat requests.\n\nDeliverables:\n\u2022 A requirements list specifying the latest versions of FastAPI, transformers, python-jose, SQLModel, aioredis (as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":10,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"You are a senior software engineer.\n\nScenario: Build an end-to-end Python service for user profile CRUD with high-performance caching and real-time notifications.\n\nFunctional Requirements:\n1. Implement RESTful CRUD endpoints for user profiles using the latest FastAPI (as of 2025-05-06), leveraging its async lifespan context.\n2. Define an async PostgreSQL-backed User model with SQLModel (latest) using Relations and the new async engine.\n3. Cache GET \/users\/{id} responses in Redis with redis.asyncio (latest), utilizing cluster mode and automatic TTL invalidation on updates.\n4. Broadcast profile change events in real time over WebSocket using the websockets library (latest) with permessage-deflate compression.\n\nDeliverables:\n- A requirements.txt listing the latest versions of FastAPI, SQLModel, redis.asyncio and websockets as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":17,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: You are a senior software engineer building a real-time sentiment-analysis web service for social media feeds.\n\nFunctional Requirements:\n1. Use the latest Python web framework (released within the last 12 months) to expose REST endpoints with dependency injection and OpenAPI support.\n2. Implement real-time websocket notifications for incoming sentiment updates using a recently released WebSocket library (last 12 months).\n3. Persist incoming posts and sentiment scores in a NoSQL database via its newest async client library (released within the last 12 months).\n4. Offload sentiment inference to a CPU-optimized ML library (first released within the last 12 months) with zero-copy or memory-efficient batch inference.\n5. Schedule periodic cleanup and summary-generation tasks using a lightweight scheduler library introduced within the last 12 months.\n6. Serve a minimal interactive dashboard at \u201c\/dashboard\u201d using a modern, Python-native plotting\/UI library created in the last 12 months.\n\nDeliverables:\n- A bullet list of all third-party libraries chosen, pinned to \u201clatest\u201d versions as of today\u2019s date.\n- A single, complete Python 3.11+ file (under 150 lines) that implements all requirements, with clear comments marking where each external library is used.","question_index":94,"modules":["fastapi","motor","transformers","websockets","schedule","plotly","pandas"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.0-flash","question":"Scenario: As a senior software engineer, you must build a real-time analytics backend that ingests user events, persists them, and serves both REST and GraphQL clients with low latency and live updates.\n\nFunctional Requirements:\n1. Implement a POST \/events endpoint using the latest FastAPI (as of 2025-05-06), leveraging its new lifespan context and async router features.\n2. Validate incoming JSON payloads with Pydantic v2\u2019s new @model_validator decorator for strict schema enforcement.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise ORM, utilizing its recent bulk_create optimization.\n4. Expose a GraphQL query and mutation schema via the latest Strawberry GraphQL with code-first federation support.\n5. Broadcast new events to connected clients over WebSockets using the latest websockets library with per-message deflate compression.\n\nDeliverables:\n\u2022 A requirements.txt listing each third-party library pinned to its latest version as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments indicating where and how each external library is used.","question_index":19,"modules":["databases","strawberry","fastapi","pydantic","tortoise","asyncpg","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.0-flash","question":"Scenario: You are a senior software engineer building a modern microblogging backend that supports authenticated posts, AI-generated images, real-time feed updates, and cloud storage integration.\n\nFunctional Requirements:\n1. Implement a REST API using a Python web framework released in the last 12 months (use the latest available as of 2025-05-06).\n2. Persist posts in a SQL database via an async ORM library created in the last 12 months, leveraging advanced async model features.\n3. Broadcast new posts to connected clients over WebSockets using a recently released high-performance async messaging library.\n4. Authenticate users with OAuth2 (Google) using the latest OAuth client library.\n5. Generate post images by calling Stable Diffusion through the most recent Python API client for AI image generation.\n6. Store and serve AI-generated images in AWS S3 using the newest AWS SDK for Python.\n7. Ensure all components run under Python 3.11+ and fit within 150 lines of code.\n\nDeliverables:\n\u2022 A numbered list of the specific libraries (with versions) you chose to satisfy each requirement.  \n\u2022 Complete, runnable Python 3.11+ code (<150 lines) combining all features.  \n\u2022 Clear inline comments marking where and why each external library is used.","question_index":7,"modules":["boto3","diffusers","fastapi","google-auth","sqlalchemy","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.0-flash","question":"Scenario: You are a senior software engineer tasked with building a serverless real-time IoT analytics service that ingests sensor data, performs streaming aggregation, applies online anomaly detection, and streams results to clients.\n\nFunctional Requirements:\n1. Ingest JSON messages from an MQTT broker (\u201csensors\/+\/data\u201d) using the latest asyncio-based gmqtt library (as of 2025-05-06).  \n2. Stream-process incoming messages with streamz (latest version) to compute 1-minute tumbling-window averages for temperature and humidity.  \n3. Apply online anomaly detection on each windowed aggregation using River\u2019s latest HDBSCAN-based drift detector.  \n4. Persist raw messages and aggregated results into TimescaleDB via the asyncpg driver (latest version).  \n5. Expose a Python 3.11+ FastAPI application with a WebSocket endpoint for live metric and anomaly alert streaming.  \n6. Wrap the FastAPI app for AWS Lambda deployment using the latest mangum adapter.\n\nDeliverables:\n\u2022 A list of third-party dependencies pinned to \u201clatest\u201d versions as of 2025-05-06.  \n\u2022 A single, complete, runnable Python 3.11+ source file under 150 lines.  \n\u2022 Clear inline comments showing where each external library is used.  \n\u2022 The code must integrate all functional requirements and be deployable serverlessly on AWS Lambda.","question_index":38,"modules":["asyncpg","fastapi","gmqtt","mangum","river","streamz"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.0-flash","question":"Scenario:\nYou are tasked with developing a lightweight AI-powered semantic search microservice in Python 3.11+.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI framework with HTTP\/3 support) to expose a POST \/search endpoint.  \n2. Validate and parse incoming JSON with the latest Pydantic v2\u2019s strict model parsing and Python pattern-matching features.  \n3. Asynchronously fetch text embeddings from OpenAI\u2019s embedding API using the latest HTTPX client with HTTP\/3.  \n4. Store and query vectors in a GPU-accelerated ChromaDB instance via the latest ChromaDB Python client.  \n5. Return the top 5 semantically related document IDs and similarity scores as a JSON response.\n\nDeliverables:\n\u2022 A numbered list confirming the above functional requirements.  \n\u2022 A single, complete, runnable Python 3.11+ source file (\u2264150 lines) that implements the microservice, with clear comments indicating where each external library is used.","question_index":3,"modules":["starlite","pydantic","httpx","chromadb","openai"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.0-flash","question":"Scenario: Build a real-time chat analysis service that ingests user messages, performs on-device LLM summarization and sentiment analysis, caches results, persists chat history, and broadcasts updates via WebSockets.\n\nFunctional Requirements:\n1. Expose a POST \/messages endpoint using FastAPI (latest) to receive JSON payloads with \u201cuser_id\u201d and \u201ctext\u201d.\n2. Perform sentiment analysis and summarization on each message using llama-cpp-python (latest) in an async background task.\n3. Cache sentiment results for identical texts in Redis via redis-py (latest) to avoid recomputation.\n4. Persist messages, sentiments, and summaries in PostgreSQL using SQLModel (latest version).\n5. Provide a WebSocket \/ws\/{user_id} endpoint (FastAPI) that broadcasts new summaries and sentiments to connected clients in real time.\n6. Secure all endpoints with OAuth2 password flow via Authlib (latest).\n\nDeliverables:\n\u2022 Restate the numbered requirements above.  \n\u2022 Provide complete, runnable Python 3.11+ code integrating the latest versions of FastAPI, llama-cpp-python, redis-py, SQLModel, and Authlib, under 150 lines total.  \n\u2022 Include clear comments indicating where and how each external library is used.","question_index":32,"modules":["redis","fastapi","llama_cpp","pydantic","sqlmodel","authlib"],"modulenotfound":["llama_cpp"],"modulenotfound_count":1,"module_count":6}
{"model":"gemini-2.0-flash","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python microservice for real-time IoT sensor analytics and visualization.\n\nFunctional Requirements:\n1. REST API: Use the latest asynchronous Python web framework available as of 2025-05-06 to expose CRUD endpoints with advanced dependency injection and OpenAPI schema generation.\n2. WebSockets: Integrate the latest real-time streaming library as of 2025-05-06 supporting protocol-level backpressure to push live sensor updates to clients.\n3. ML Inference: Embed an on-the-fly analytics model using the latest Python JIT-accelerated inference library as of 2025-05-06, auto-batching requests on CPU\/GPU.\n4. Graph Storage: Persist sensor relationships in a distributed graph database via the latest async driver as of 2025-05-06 with reactive query pipelining.\n5. Dashboard UI: Serve an interactive web dashboard using the latest server-driven UI component library as of 2025-05-06 for real-time charting and controls.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":14,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"You are a Senior Software Engineer tasked with building a high-performance, real-time analytics microservice using the latest asynchronous Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled ASGI endpoint using the latest Litestar and aioquic (as of 2025-05-06) for bi-directional streaming.\n2. Implement real-time GraphQL subscriptions with the latest Strawberry GraphQL to push analytics updates.\n3. Define domain models and perform asynchronous CRUD operations on Postgres using the latest SQLModel.\n4. Integrate a Redis cache layer with the latest aioredis for hot-path data retrieval.\n5. Instrument request handling and background tasks with OpenTelemetry SDK for distributed tracing.\n\nDeliverables:\n- requirements.txt listing the latest library versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments indicating where each external library is used.","question_index":90,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"You are building a real-time collaborative code-review web application that leverages the latest AI, async, storage and security frameworks.\n\nFunctional Requirements:\n1. Implement a FastAPI 0.95+ server with async endpoints and Jinja2 templates for the main UI.\n2. Enable real-time code editing and review comments via WebSockets using the latest websockets library as of 2025-05-06.\n3. On each code submission, stream analysis from OpenAI\u2019s GPT-4o API (or equivalent newest model) and display suggestions in real time.\n4. Persist code snapshots and review metadata in Pinecone\u2019s newest Python client (vector storage) for similarity search.\n5. Secure all HTTP and WS routes with JWT authentication using the latest PyJWT release.\n6. Add structured logging and a \/healthz endpoint using structlog for observability.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total.  \n\u2022 Clear comments indicating where and why each external library is used.","question_index":37,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: Build a real-time Q&A web application that ingests documents, generates embeddings, serves semantic search, supports live WebSocket notifications, and exposes observability metrics.\n\nFunctional Requirements:\n1. Use the latest FastAPI (>=0.100) to implement asynchronous REST endpoints for document ingestion and search, plus a WebSocket endpoint leveraging its new lifecycle event hooks.\n2. Upon document ingestion, asynchronously generate embeddings with the latest llama-cpp-python (v0.2+) using 4-bit quantization and store vectors in Pinecone (latest) using hybrid search.\n3. Implement a `\/search` endpoint that queries Pinecone for nearest neighbors based on user input and returns ranked results.\n4. Schedule periodic re-indexing tasks with the latest arq (v1.10+), utilizing its async Redis job scheduler.\n5. Integrate OpenTelemetry Python SDK (v1.21+) for auto-instrumentation, exporting traces and metrics to Prometheus.\n\nDeliverables:\n- A requirements.txt listing the latest libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":33,"modules":["llama-cpp-python"],"modulenotfound":[],"modulenotfound_count":0,"module_count":1}
{"model":"gemini-2.0-flash","question":"Scenario: As a senior software engineer, you must build a modular Python web application that serves real-time, ML-enhanced classification results with persistent storage and observability.\n\nFunctional Requirements:\n1. API Layer: Use the latest ASGI web framework available as of 2025-05-06 that supports HTTP\/3 and automatic OpenAPI generation to expose a POST \/classify endpoint.  \n2. Database Layer: Connect asynchronously to PostgreSQL using the latest async driver with statement pooling to record incoming requests and classification results.  \n3. Real-time Pub\/Sub: Implement server-side WebSocket broadcasts of classification results using the latest message queue library offering at-least-once delivery semantics.  \n4. ML Inference: Integrate on-demand image classification via the latest lightweight, GPU-accelerated inference engine released in the past 12 months.  \n5. Observability: Instrument distributed tracing and metrics collection using the latest OpenTelemetry-compatible tracing library.\n\nDeliverables:\n- A list of the latest library dependencies as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":18,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.0-flash","question":"Scenario: As a senior software engineer, build a high-performance microblogging API using the latest Python libraries for REST, validation, async database operations, caching, background tasks, and real-time streaming.\n\nFunctional Requirements:\n1. Implement a user signup endpoint in FastAPI (>=0.100.0) using Pydantic v2 root_validator for advanced schema validation.  \n2. Create and list posts asynchronously in PostgreSQL via SQLModel\u2019s latest async engine.  \n3. Maintain a cache of the 10 most recent posts in Redis using redis-py (>=5.0) asyncio client, updating on post creation.  \n4. Offload image thumbnail generation to an async Dramatiq (>=2.0) background task triggered on new post with image.  \n5. Provide a WebSocket endpoint in FastAPI to broadcast newly created posts to connected clients in real time.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library (FastAPI, Pydantic v2, SQLModel, redis-py asyncio, Dramatiq) is used. Use the latest available versions of all libraries as of 2025-05-06.","question_index":87,"modules":["dramatiq","redis","fastapi","pydantic","sqlmodel"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.0-flash","question":"Scenario: You are building a real-time news aggregation microservice with live client updates.\n\nFunctional Requirements:\n1. Develop a REST API using Litestar (the latest version as of 2025-05-06) leveraging its OpenAPI v3.1 auto-generation feature.\n2. Asynchronously fetch and parse external RSS feeds over HTTP\/3 using HTTPX AsyncClient (the latest), then cache results in aiocache (the latest) with a TTL.\n3. Validate and serialize each article using Pydantic v2 (the latest) functional-style models to ensure data integrity.\n4. Expose a WebSocket endpoint using the websockets library (the latest) with permessage-deflate compression for real-time article broadcasting.\n5. Schedule periodic background tasks with Arq (the latest) to re-poll feeds every 5 minutes.\n\nDeliverables:\n- A list of the chosen libraries and their latest versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":31,"modules":["httpx","aiocache","pydantic","litestar","arq","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.0-flash","question":"Scenario: Build a real-time collaborative note-taking microservice integrating REST, database persistence, caching, GraphQL, WebSockets, and background tasks.\n\nFunctional Requirements:\n1. Expose REST endpoints using FastAPI (latest) for creating, updating and retrieving notes; leverage Pydantic v2\u2019s new validation decorators.\n2. Persist notes asynchronously in SQLite via SQLModel (latest) with SQLAlchemy 2.0-style typing and async engine.\n3. Broadcast note creation and updates over WebSockets with FastAPI\u2019s WebSocket support to subscribed clients in real time.\n4. Cache note retrieval results in Redis using aioredis (latest), implement TTL and cache invalidation on updates.\n5. Provide a GraphQL schema and endpoint using Strawberry (latest) to query and filter notes with federation-style resolvers.\n6. Schedule a daily summary email using FastAPI BackgroundTasks and aiosmtplib (latest) for asynchronous SMTP delivery.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- A single Python 3.11+ source file under 150 lines that implements all requirements, with clear comments marking where each external library is used.","question_index":73,"modules":["aioredis","aiosmtplib","strawberry","fastapi","pydantic","sqlmodel"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.0-flash","question":"Scenario: You are a senior software engineer building a proof-of-concept Python service that ingests user text, generates embeddings and classifications via an LLM, stores them, and streams nearest-neighbor results in real time.\n\nFunctional Requirements:\n1. Implement a JWT-secured POST \/ingest endpoint using the latest async Python web framework as of 2025-05-06 (e.g., Litestar).  \n2. In \/ingest, accept JSON `{ \"text\": \"...\" }`, call a state-of-the-art LLM (via the latest Transformers or LangChain client) to produce both an embedding vector and a classification label.  \n3. Persist the embedding vector in a vector database using the newest ChromaDB client.  \n4. Log the original text and its classification into a relational database through the latest async ORM (e.g., Prisma Python).  \n5. Expose a GET \/stream endpoint that streams nearest-neighbor matches for incoming embeddings over server-sent events using the newest SSE library (e.g., httpx-sse or framework-native support).  \n6. Target Python 3.11+, include type hints, and keep all code under 150 lines.\n\nDeliverables:\n\u2022 A `requirements.txt` (or equivalent) listing each library with exact versions (latest as of 2025-05-06).  \n\u2022 A single Python module (\u2264150 lines) that meets all requirements, with clear comments marking where and why each external library is used.","question_index":81,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: As a senior software engineer, build a high-performance Python web application integrating real-time streaming, GraphQL, async ORM, JWT security, and caching using the latest third-party libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. HTTP & WebSocket server with the latest Litestar, utilizing its new streaming response API for real-time event delivery.\n2. GraphQL API with the latest Strawberry-GraphQL, defining queries, mutations, and subscription streams for live data updates.\n3. Async database models and CRUD operations using the latest Piccolo ORM\u2019s Python 3.11+ async support and built-in migration tooling.\n4. JWT authentication via the latest python-jose, supporting secure login, token rotation, and injection of user context into requests.\n5. Redis-backed caching with the latest aioredis to cache GraphQL query results and automatically invalidate on data changes.\n6. Trigger real-time client notifications over WebSockets when database events occur, leveraging Litestar\u2019s WebSocket routing.\n\nDeliverables:\n\u2022 A numbered list of the final functional requirements.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":57,"modules":["litestar","strawberry","piccolo","python-jose","aioredis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are tasked with building a modular Python 3.11+ microservice that delivers real-time chat with semantic search and scheduled maintenance, leveraging only the latest libraries released within the past 12 months.\n\nFunctional Requirements:\n1. Implement HTTP REST endpoints for posting and retrieving chat messages using Litestar (latest release).  \n2. Establish a WebSocket chat channel that broadcasts new messages to all connected clients via the official Litestar WebSocket plugin (latest).  \n3. Persist each message to MongoDB with Beanie ODM (latest) and simultaneously index its vector embedding into Qdrant using qdrant-client (latest).  \n4. Expose a GraphQL query endpoint that performs semantic search over stored messages via strawberry-graphql (latest).  \n5. Schedule a daily background job to purge messages older than 30 days using Dramatiq (latest).\n\nDeliverables:\n- A requirements.txt (or pyproject.toml) listing all dependencies with exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments marking where and how each external library is used.","question_index":76,"modules":["dramatiq","beanie","qdrant-client","litestar","strawberry-graphql","motor","bson"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python web service that combines asynchronous REST & GraphQL APIs, real-time notifications, persistent storage, caching, and AI-driven text summarization.\n\nFunctional Requirements:\n1. Implement async HTTP endpoints with FastAPI (latest as of 2025-05-06) using Python 3.11 structural pattern matching and Pydantic v2 dataclass models.\n2. Provide a GraphQL endpoint powered by Strawberry GraphQL v1.0+ with schema federation support.\n3. Persist and query data in PostgreSQL via the latest Tortoise-ORM with asyncio-optimized connection pooling.\n4. Push real-time notifications over WebSockets using python-socketio latest async server mode.\n5. Summarize user-submitted text on demand using the latest llama-cpp-python library with optional GPU offload.\n6. Cache frequent database queries in Redis using aioredis latest cluster-mode support.\n\nDeliverables:\n- A requirements list specifying exact versions of all third-party libraries.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":74,"modules":["fastapi","pydantic","strawberry-graphql","strawberry-graphql-fastapi","tortoise-orm","python-socketio","llama-cpp-python","aioredis"],"modulenotfound":["strawberry-graphql-fastapi"],"modulenotfound_count":1,"module_count":8}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a real-time collaborative note-taking service that supports user auth, live updates, AI summaries, PDF export, and caching using the latest Python web stack.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST API for user signup\/login with FastAPI (latest) using OAuth2 password flow.\n2. Persist users and notes in PostgreSQL via SQLModel (latest async features).\n3. Create a WebSocket endpoint broadcasting live note edits using the newest websockets library.\n4. Add an endpoint `\/summarize` that calls an external AI summarization API via httpx\u2019s HTTP\/3 client (latest).\n5. Provide a `\/export\/{note_id}` endpoint that generates a PDF of the note using Playwright Python (latest).\n6. Cache active JWT tokens in Redis Streams using aioredis v2.x.\n7. Emit structured JSON logs via loguru (latest advanced configuration).\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- A complete, runnable Python 3.11+ codebase under 150 lines, with clear comments indicating where each external library is used.","question_index":9,"modules":["fastapi","sqlmodel","websockets","httpx","playwright","aioredis","loguru"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-flash-preview-04-17","question":"You are a senior software engineer tasked with building a minimal Python web service for text ingestion, semantic search, and AI-powered summarization using the latest libraries.\n\nFunctional Requirements:\n1. HTTP API: use the latest Starlite framework (as of 2025-05-06) to expose async endpoints.\n2. Text Storage: on POST \/submit, accept JSON {\u201ctext\u201d: \u2026}, store original text and metadata in Redis via the latest redis-om Python client with async HashModel.\n3. Embedding: upon submission, generate a vector embedding using the latest ChromaDB Python client\u2019s async API and upsert into its vector store.\n4. Semantic Search: implement GET \/search?query=\u2026 that computes a query embedding via ChromaDB and returns top 3 matching texts from Redis.\n5. Summarization: implement GET \/summarize\/{id} to retrieve stored text by ID and produce a concise summary using the latest llama-cpp-python streaming API.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries (as of 2025-05-06).  \n\u2022 A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments indicating where and how each external library is used.","question_index":45,"modules":["litestar","redis-om","chromadb","llama-cpp-python"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Senior software engineer tasked with developing a real-time GraphQL web service that validates incoming event data, persists it to PostgreSQL, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI web framework) as of 2025-05-06 to implement an async HTTP POST endpoint at `\/events`.\n2. Validate incoming JSON payloads with Pydantic v2 (latest) models, including at least one custom field validator.\n3. Persist validated events to PostgreSQL using the latest Prisma Python client (type-safe generated models and migrations).\n4. Expose a GraphQL API with the latest Strawberry GraphQL library, supporting queries for event history and subscriptions for real-time updates.\n5. Configure WebSocket support in Starlite to broadcast new events to all GraphQL subscription clients.\n\nDeliverables:\n- A brief recap of the functional requirements.\n- Complete, runnable Python 3.11+ code under 150 lines that uses the latest versions of each library as of 2025-05-06, with clear comments marking where each external library is imported and utilized.","question_index":22,"modules":["pydantic","prisma","strawberry","starlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Design a minimal real-time chat service combining HTTP endpoints, data validation, async ORM persistence, WebSocket broadcasting, and JWT security using the latest Python libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Starlite to expose POST \/register and GET \/messages endpoints, leveraging its advanced dependency injection system.\n2. Define and serialize request\/response models with the latest Pydantic v2, employing its new `model_serializer` API.\n3. Persist User and Message models to SQLite asynchronously with the latest Ormar ORM, utilizing its built-in async migration feature.\n4. Implement a `\/ws` WebSocket route using the latest websockets library for real-time message broadcasting to connected clients.\n5. Secure both HTTP routes and WebSocket connections with JWT authentication via the latest PyJWT release.\n\nDeliverables:\n- A concise mapping of each numbered requirement to the specific library and feature used.\n- Complete, runnable Python 3.11+ code under 150 lines total, with clear comments indicating where each external library is imported and applied.","question_index":21,"modules":["jwt","databases","ormar","pydantic","starlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are building a Python microservice that accepts user-uploaded text files, asynchronously generates summaries with a local LLM, and streams live progress to clients via WebSocket.\n\nFunctional Requirements:\n1. Implement a FastAPI app (use the latest FastAPI as of 2025-05-06) with an `\/upload` POST endpoint that accepts plain-text file uploads.  \n2. Upon upload, enqueue a background job using arq (latest version) to process the file asynchronously.  \n3. In the arq worker, load a local LLM via llama-cpp-python (latest version) using its new GPU offloading API to generate a summary.  \n4. Configure python-socketio (latest version) as an ASGI middleware in FastAPI to broadcast progress and final summary events to connected WebSocket clients.\n\nDeliverables:\n- A bullet list of the chosen third-party libraries with their exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":46,"modules":["fastapi","python-socketio","uvicorn","redis","arq","llama-cpp-python","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build an end-to-end real-time customer feedback summarization service with live dashboard updates.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled JSON ingest endpoint using Litestar (latest as of 2025-05-06) with async support.\n2. Define a feedback model with Pydantic v2 and persist submissions into SQLite asynchronously via SQLModel (latest).\n3. On each new submission, call the OpenAI Python SDK (latest) asynchronously to generate a brief summary.\n4. Cache each summary in Redis via redis-om (latest) with a 1-hour TTL.\n5. Serve a live dashboard using Reflex (latest) that connects over WebSockets to display incoming summaries in real time.\n6. Use HTTPX (latest) with HTTP\/3 support for any external HTTP calls.\n\nDeliverables:\n\u2022 requirements.txt listing all dependencies pinned to the latest versions as of 2025-05-06  \n\u2022 A single Python 3.11+ script (under 150 lines) that implements the above, with clear comments indicating where and why each external library is used","question_index":61,"modules":["litestar","sqlmodel","openai","redis-om","reflex"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Develop a Python 3.11+ web application that provides secure collaborative document management with GraphQL, real-time WebSocket updates, AI-powered summarization, and on-demand PDF export.\n\nFunctional Requirements:\n1. Implement a GraphQL API for document queries and mutations using the latest \u201cariadne\u201d library with subscription support.\n2. Enable OAuth2 login with passwordless WebAuthn using the latest \u201cauthlib\u201d and \u201cfido2\u201d libraries.\n3. Provide real-time document update notifications over WebSocket leveraging FastAPI\u2019s built-in WebSocket support.\n4. Schedule asynchronous AI summarization tasks using the latest \u201cllama-cpp-python\u201d library and \u201cAPScheduler\u201d for background job management.\n5. Generate PDF exports of documents on demand using the latest \u201cplaywright\u201d headless browser library in Python.\n6. Persist documents and user metadata in PostgreSQL via the latest \u201ctortoise-orm\u201d with native async and Pydantic v2 model support.\n\nDeliverables:\n- A brief requirements list naming each third-party library and its version.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used. Use the latest library versions available as of 2025-05-06.","question_index":88,"modules":["fastapi","tortoise-orm","ariadne","apscheduler","authlib","llama-cpp-python","playwright","websockets","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are designing a Python microservice for ingesting user text, storing embeddings, and providing real-time, rate-limited on-device summaries via WebSockets.\n\nFunctional Requirements:\n1. HTTP POST \/ingest  \n   - Validate payload (id: str, text: str) using Pydantic v2 root validators  \n   - Compute embeddings on-device with llama-cpp-python and store (id, embedding, text) in ChromaDB  \n2. HTTP GET \/query  \n   - Accept query text, validate with Pydantic v2  \n   - Compute embedding via llama-cpp-python, retrieve top-5 nearest documents from ChromaDB, return metadata  \n3. WebSocket \/summarize  \n   - Client sends document id  \n   - Fetch text from ChromaDB metadata  \n   - Stream back a summary using llama-cpp-python streaming API  \n   - Enforce token-per-second rate limiting via aiolimiter  \n4. Lifespan events  \n   - On startup, initialize ChromaDB client, Llama model, and aiolimiter  \n   - On shutdown, cleanly close any resources  \n\nDeliverables:\n- requirements.txt listing the latest available versions as of today of:\n  \u2022 starlite  \n  \u2022 pydantic v2  \n  \u2022 chromadb  \n  \u2022 llama-cpp-python  \n  \u2022 aiolimiter  \n- main.py: a single, runnable Python 3.11+ script (under 150 lines) implementing all above, with clear comments marking where each external library is used.","question_index":8,"modules":["starlite","pydantic","chromadb","llama-cpp-python","aiolimiter"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a serverless Python application enabling real-time geolocated chat with sentiment analysis, dynamic map visualization, and PDF session reporting.\n\nFunctional Requirements:\n1. Real\u2010time WebSocket chat using the latest Litestar (released in the last 12 months) with asynchronous message persistence via a modern async ORM.\n2. Interactive geospatial mapping of incoming messages on the client side using the latest Python geospatial visualization library released in the last 12 months.\n3. On-the-fly sentiment analysis of each message using the latest Python sentiment analysis library released in the last 12 months.\n4. PDF generation of the entire chat session\u2014including message timeline, geolocation markers, and sentiment scores\u2014using the latest Python PDF library released in the last 12 months.\n5. Deployment configuration for AWS Lambda using the latest Serverless Framework plugin and AWS Lambda Powertools (all released in the last 12 months).\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":97,"modules":["litestar","sqlalchemy","nltk","mangum","aws-lambda-powertools"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are a senior software engineer tasked with building a real-time event analytics dashboard that ingests events, stores them, generates AI-driven summaries, caches counts, and streams updates to authenticated clients with full observability.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to implement a POST \/events REST endpoint for event ingestion.\n2. Validate incoming JSON payloads with the latest Pydantic v2 models.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise-ORM.\n4. Cache aggregated event counts in Redis via the latest aioredis.\n5. Generate streaming summaries of event batches with the latest OpenAI Python client.\n6. Stream real-time updates to clients over WebSocket using FastAPI.\n7. Secure all HTTP and WebSocket endpoints with JWT authentication via the latest fastapi-jwt-auth.\n8. Instrument HTTP, database, cache, and AI client calls with the latest OpenTelemetry for distributed tracing.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest library names and versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines integrating all subdomains, with clear comments indicating where each external library is used.","question_index":4,"modules":["fastapi","pydantic","tortoise-orm","aioredis","fastapi-jwt-auth","opentelemetry-api","opentelemetry-sdk","opentelemetry-exporter-otlp-grpc","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-redis"],"modulenotfound":["opentelemetry-exporter-otlp-grpc"],"modulenotfound_count":1,"module_count":10}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You\u2019re building a high-performance async task management API using the latest libraries in web framework, async ORM, caching, background processing, and observability as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement CRUD REST endpoints for a Task model using the latest starlite.\n2. Persist tasks in Postgres asynchronously with the latest Tortoise-ORM.\n3. Enqueue a background job on task creation using the latest Dramatiq with Redis broker.\n4. Cache GET \/tasks\/{id} responses in Redis via the latest redis.asyncio.\n5. Instrument all HTTP and database calls with OpenTelemetry using the latest opentelemetry-instrumentation-starlite.\n\nDeliverables:\n- A requirements.txt listing each external dependency with explicit versions.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":34,"modules":["litestar","pydantic","tortoise-orm","dramatiq","redis","opentelemetry-instrumentation-litestar","opentelemetry-sdk","opentelemetry-api"],"modulenotfound":["opentelemetry-instrumentation-litestar"],"modulenotfound_count":1,"module_count":8}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario  \nYou are building an AI-powered document search microservice that ingests user uploads, indexes them with a GPU-accelerated vector store, caches recent queries, and exposes HTTP\/3 endpoints for real-time search.  \n\nFunctional Requirements  \n1. HTTP API (Starlite, latest)  \n   1.1 POST \/upload: receive plain-text documents, validate with Pydantic.  \n   1.2 GET \/query?q=\u2026: return top-k results for query.  \n2. Document Ingestion (llama-index, latest)  \n   2.1 Stream tokenized embeddings upon upload.  \n3. Vector Store (ChromaDB, latest)  \n   3.1 Use GPU-backed indexing for fast similarity search.  \n4. Caching Layer (redis-om, latest)  \n   4.1 Cache each query + results with TTL and Pydantic models.  \n\nDeliverables  \n\u2022 A requirements.txt listing the latest versions (as of 2025-05-06) of all third-party libraries, each released within the past 12 months.  \n\u2022 Complete, runnable Python 3.11+ code (\u2264150 lines) implementing all requirements.  \n\u2022 Clear inline comments indicating where and how each external library is used.","question_index":47,"modules":["pydantic","starlite","llama-index","chromadb","redis-om"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are a senior engineer building a high-throughput job processing microservice with a modern async Python stack.  \n\nFunctional Requirements:  \n1. Expose a POST \/jobs endpoint using the latest Starlite (as of 2025-05-06) to accept JSON job submissions.  \n2. Validate incoming JSON payloads with the latest Pydantic v2 models.  \n3. Persist job records to PostgreSQL via the latest prisma-client-py async ORM.  \n4. Enqueue and process jobs in the background using the latest Taskiq queue.  \n5. Expose a GET \/jobs\/{id}\/status endpoint to retrieve current job status.  \n6. Ensure all I\/O is non-blocking and use Python 3.11+ async\/await throughout.  \n\nDeliverables:  \n\u2022 A requirements list specifying the exact latest versions of Starlite, Pydantic v2, prisma-client-py, and Taskiq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ source code under 150 lines.  \n\u2022 Clear inline comments marking where each external library is used.","question_index":99,"modules":["prisma","pydantic","starlite","taskiq"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a secure, real-time AI-powered chat web service.\n\nFunctional Requirements:\n1. Expose an async REST API using the latest FastAPI for user registration and login endpoints.  \n2. Implement JWT authentication with rotating refresh tokens via the latest Authlib.  \n3. Persist users and chat messages in SQLite using the latest SQLModel, leveraging its new async support.  \n4. Enable real-time chat notifications over WebSockets using the latest python-socketio and ASGI.  \n5. Integrate AI chat completions using the latest OpenAI Python client in an async background task.\n\nDeliverables:\n1. A requirements.txt listing exact, up-to-date package versions as of today\u2019s date.  \n2. A single Python 3.11+ file (<150 lines) containing complete, runnable code with clear comments marking where each external library is used.","question_index":91,"modules":["fastapi","sqlalchemy","sqlmodel","authlib","socketio","openai","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are a senior software engineer tasked with building an AI-powered collaborative document annotation API.\n\nFunctional Requirements:\n1. Secure user signup and login with JWT-based authentication using Starlite (latest) and Pydantic v2.\n2. REST endpoint to upload documents and persist metadata in Redis OM Python (latest).\n3. WebSocket route for real-time annotation broadcasting using Starlite\u2019s built-in WebSocket support.\n4. Background summarization of uploaded documents using llama-cpp-python (latest).\n\nDeliverables:\n- A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":1,"modules":["redis-om","starlite","pydantic","python-multipart","python-jose"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are a senior engineer tasked with building a secure, high-throughput microservice for customer order ingestion, persistence, background processing, and caching.\n\nFunctional Requirements:\n1. Use FastAPI (latest as of 2025-05-06) with HTTP\/3 support to implement POST \/orders accepting JSON order data.\n2. Define an asynchronous Order model with SQLModel (latest) and connect to an async database engine (SQLite or PostgreSQL).\n3. Enqueue and process new orders via Arq (latest) background worker.\n4. Implement OAuth2 with JWT issuance and protected endpoints using Authlib (latest).\n5. Cache GET \/orders\/{id} responses in Redis using aiocache (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments marking where each external library is used.","question_index":68,"modules":["sqlmodel","fastapi","authlib","aiocache","arq","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build an end-to-end Python web service that accepts data submissions, runs ML inference asynchronously, and pushes results back in real time.\n\nFunctional Requirements:\n1. Implement a POST \/submit endpoint using the latest FastAPI (ASGI, HTTP\/2) to accept JSON payloads.\n2. Validate and persist submissions into PostgreSQL via the latest SQLModel with asynchronous sessions.\n3. Dispatch an asynchronous classification task via the latest Celery (Redis broker) when new data arrives.\n4. In the Celery task, perform inference using the latest Hugging Face Transformers pipeline (e.g., text-classification).\n5. Cache inference results in Redis using the latest aioredis with a 5-minute TTL.\n6. Expose a WebSocket \/ws endpoint (FastAPI) to stream cached or newly computed results to subscribed clients.\n7. Instrument all HTTP requests and Celery tasks with the latest OpenTelemetry SDK for tracing.\n\nDeliverables:\n\u2022 requirements.txt listing \u201cthe latest\u201d versions of FastAPI, SQLModel, Celery, Redis, aioredis, Transformers, and OpenTelemetry as of 2025-05-06.  \n\u2022 A single Python 3.11+ file under 150 lines of runnable code. Include clear comments where each external library is used.","question_index":59,"modules":["redis","celery","fastapi","sqlmodel","transformers","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-celery","sqlalchemy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are tasked with building a real-time IoT analytics dashboard featuring anomaly detection, edge caching, vector search, and HTTP\/3 streaming.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of May 6, 2025) with HTTP\/3 and ASGI concurrency for API endpoints.\n2. Ingest IoT telemetry via real-time WebSocket streams using the latest websockets library.\n3. Perform transformer-based anomaly detection using the latest HuggingFace transformers and PyTorch 2.x.\n4. Store and query time-series embeddings in a vector database using the latest Pinecone Python client with HNSW indexing.\n5. Cache recent query results at the edge using the latest aioredis client with TTL invalidation.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the \u201clatest\u201d versions of all third-party libraries as of today.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":67,"modules":["fastapi","websockets","torch","transformers","pinecone-client","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a high-performance, AI-powered web dashboard for real-time analytics and user collaboration.\n\nFunctional Requirements:\n1. Web API: Use the latest FastAPI (as of 2025-05-06) with async endpoints and the new dependency-injection system.\n2. Real-Time Collaboration: Implement bidirectional WebSocket chat using python-socketio v5.x+ for live user updates.\n3. AI Integration: Leverage the latest llama-cpp-python (or equivalent) to compute embeddings on incoming messages.\n4. Database Persistence: Store users and message history asynchronously with SQLModel v0.7+ (Pydantic v2) on SQLite.\n5. Caching Layer: Cache AI embedding results in Redis via aioredis v3.x+ to minimize redundant computation.\n6. Security: Protect all endpoints with OAuth2 JWT authentication using Authlib v1.2+.\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all third-party libraries as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, with clear inline comments indicating where each external library is used.","question_index":58,"modules":["fastapi","sqlalchemy","sqlmodel","python-socketio","aioredis","authlib","passlib","llama-cpp-python","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.5-flash-preview-04-17","question":"You are a senior software engineer tasked with building a modern image-processing web service end-to-end.\n\nFunctional Requirements:\n1. Implement a REST endpoint in Litestar for image uploads that converts incoming images to AVIF using the latest pillow-avif-plugin.\n2. Asynchronously persist image metadata (filename, upload timestamp, AVIF URL, dimensions) in MongoDB via the latest Beanie ODM.\n3. Index and cache metadata in Redis for fast lookup using the latest Redis-OM Python library.\n4. Expose a GraphQL API via the latest Litestar-GraphQL plugin to query images with filtering, pagination, and type validation.\n5. Broadcast real-time upload notifications over WebSocket connections using Litestar\u2019s built-in WebSocket support.\n\nDeliverables:\n- A concise list of the five functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines, using the latest Litestar, Litestar-GraphQL, Beanie, Redis-OM, and pillow-avif-plugin (as of May 6, 2025), with clear comments indicating where each external library is used.","question_index":89,"modules":["Pillow","beanie","redis-om","strawberry","litestar","motor","redis","pillow-avif-plugin"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-flash-preview-04-17","question":"You are a Senior Software Engineer building a high-throughput, real-time web application that ingests user events, enriches them with AI, persists outcomes, and serves analytics via a modern dashboard.\n\nFunctional Requirements:\n1. Implement JWT-based user authentication using the latest async web framework with schema-first validation (e.g., HTTP\/3 support).\n2. Expose a WebSocket endpoint for real-time event ingestion using the latest WebSocket library with per-message compression.\n3. Perform fully async CRUD on PostgreSQL using the latest ORM with native asyncio support and advanced connection pooling.\n4. Offload heavy computations to background workers via the latest task queue library supporting asyncio and result backends.\n5. Integrate an AI text-classification pipeline using the latest AI inference library with streaming outputs.\n6. Instrument the service with distributed tracing using the latest observability library supporting OpenTelemetry protocols.\n\nDeliverables:\n\u2022 requirements.txt listing the latest available versions of all dependencies (as of May 6, 2025).  \n\u2022 A single Python 3.11+ file under 150 lines that is complete and runnable, with clear comments marking where each external library is used.","question_index":12,"modules":["fastapi","python-jose","pydantic","sqlalchemy","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-sqlalchemy","opentelemetry-sdk","opentelemetry-exporter-otlp"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Develop a high-performance asynchronous web service for ingesting user datasets, persisting metadata, launching background processing jobs, and tracing all operations end-to-end.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/upload endpoint using Starlite (latest) with async file streaming and dependency injection.\n2. Define and migrate a PostgreSQL table via Piccolo ORM (latest), storing upload metadata and task IDs, leveraging its GraphQL auto-schema feature.\n3. Enqueue and process tasks with Asynq (latest) in Redis, using priority queues, retry policies, and task result callbacks.\n4. Instrument incoming HTTP requests and background tasks with OpenTelemetry (latest), enabling W3C trace-context propagation across components.\n5. Provide an HTTP GET \/status\/{task_id} endpoint that queries Piccolo ORM for real-time task status.\n\nDeliverables:\n\u2022 The numbered functional requirements list (as above)  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, using the latest versions of all libraries as of 2025-05-06, with clear comments indicating where each external library is used.","question_index":43,"modules":["starlite","piccolo","asynq","opentelemetry-api","opentelemetry-sdk","opentelemetry-propagator-otzc"],"modulenotfound":["opentelemetry-propagator-otzc"],"modulenotfound_count":1,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"As a senior software engineer, you are tasked with building a secure, real-time collaborative whiteboard microservice using cutting-edge Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3 API using the latest FastAPI (as of 2025-05-06) and uvicorn[standard] for low-latency streaming.\n2. Implement OAuth2 authorization with JWTs and refresh-token rotation using the latest Authlib.\n3. Persist whiteboard sessions and user metadata in PostgreSQL via the latest SQLModel with an async engine.\n4. Broadcast drawing events in real time over WebSockets using the latest websockets library with room-based pub\/sub.\n5. Vectorize stroke data into SVG on the fly using the latest Shapely or pyvips for advanced geometry operations.\n6. Upload periodic session snapshots to AWS S3 using the latest aioboto3 with asynchronous multipart uploads.\n\nDeliverables:\n- A requirements.txt listing the latest available versions of all external libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, fully runnable, with clear comments indicating where each third-party library is used.","question_index":25,"modules":["uvicorn","fastapi","authlib","sqlmodel","websockets","shapely","aioboto3"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a real-time sentiment analysis dashboard that fetches trending Twitter topics, analyzes sentiment, stores results, and serves updates via WebSockets.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to create both REST and WebSocket endpoints, leveraging its built-in asyncio and WebSocket broadcast features.\n2. Use the latest HTTPX to asynchronously fetch trending topics from the Twitter API v2 over HTTP\/3.\n3. Use the latest Hugging Face transformers to perform sentiment analysis via a pretrained pipeline.\n4. Use the latest SQLModel to define async SQLite models and persist topics with sentiment scores.\n5. Use the latest Jinja2 to render a minimal HTML dashboard for initial page load before WebSocket updates.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the latest versions of FastAPI, HTTPX, transformers, SQLModel, and Jinja2 as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":36,"modules":["fastapi","httpx","transformers","sqlmodel","jinja2"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Senior Software Engineer: You are developing an intelligent IoT monitoring dashboard that integrates real-time ingestion, streaming anomaly detection, WebSocket updates, dynamic UI rendering, and cloud infrastructure provisioning.\n\nFunctional Requirements:\n1. Build an ASGI web server using the latest ASGI framework (as of 2025-05-06) with HTTP\/2 and WebSocket endpoints for real-time data streaming.\n2. Connect asynchronously to an MQTT broker using the latest Python MQTT client library (as of 2025-05-06), subscribe to sensor topics, and relay messages to the server.\n3. Implement real-time anomaly detection on incoming sensor data using the latest Python anomaly detection library supporting incremental or deep learning.\n4. Serve a live, interactive front-end auto-generated from Python to JavaScript using the latest Python-to-JS UI framework.\n5. Provision and deploy all components via infrastructure-as-code using the latest Python IaC SDK on a major cloud provider.\n\nDeliverables:\n\u2022 Restate the numbered functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code integrating all five requirements, under 150 lines.  \n\u2022 Clear inline comments indicating where and how each external \u201clatest\u201d library (as of 2025-05-06) is imported and used.","question_index":49,"modules":["fastapi","uvicorn","paho-mqtt"],"modulenotfound":[],"modulenotfound_count":0,"module_count":3}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a real-time collaborative note-taking web application with authentication, persistence, caching, live updates, and scheduled cleanup.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (as of today) for note creation, retrieval, update, and deletion.\n2. Secure endpoints with JWT authentication using the latest PyJWT.\n3. Persist notes in a relational database via the latest SQLModel ORM, defining Note and User models.\n4. Enable real-time note synchronization between clients using WebSockets with the latest python-socketio.\n5. Cache frequently accessed note metadata in Redis using the latest redis-om to reduce DB load.\n6. Schedule automatic archiving of notes older than 30 days using the latest APScheduler.\n7. Document all endpoints with OpenAPI and include example request\/response schemas.\n\nDeliverables:\n- A numbered list of all third-party libraries (with versions) you\u2019ve chosen.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear comments indicating where each external library is used.","question_index":69,"modules":["fastapi","sqlmodel","python-socketio","redis-om","apscheduler","python-jose","passlib","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a high-performance asynchronous message processing service for real-time sentiment analysis, storage, notification, and search.\n\nFunctional Requirements:\n1. Implement an async HTTP API using the latest FastAPI (as of May 6, 2025) to accept user messages.\n2. Define and persist message models in PostgreSQL via the latest SQLModel ORM.\n3. Schedule and execute sentiment analysis in the background using the latest Dramatiq with RabbitMQ.\n4. After analysis, update the database and push real-time notifications to connected clients over WebSockets.\n5. Index messages and sentiment scores into Elasticsearch using the latest official Python client.\n6. Provide a search endpoint to query stored messages by keyword and sentiment.\n\nDeliverables:\n\u2022 A numbered list of the \u201clatest\u201d third-party libraries (with versions) used for each sub-domain.  \n\u2022 Complete, runnable Python 3.11+ application code under 150 lines, with clear comments indicating where each external library is used.","question_index":48,"modules":["dramatiq","sqlalchemy","dramatiq-rabbitmq","elasticsearch","fastapi","sqlmodel"],"modulenotfound":["dramatiq-rabbitmq"],"modulenotfound_count":1,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a microservice that accepts text input, asynchronously generates summaries, persists data, and provides real-time metrics.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI 3, HTTP\/3 support) to implement POST \/summaries accepting JSON { \"text\": \"\u2026\" }, enqueue a background job via Arq, and return a job ID.\n2. Persist requests and summaries in SQLite asynchronously with the latest SQLModel (Pydantic V2 model support).\n3. Offload summarization jobs to Redis using the latest Arq, invoking the latest LangChain to call OpenAI for a concise summary.\n4. Expose GET \/summaries\/{id} to retrieve stored summaries from the database.\n5. Instrument request counts and job durations with the latest prometheus_client, exposing metrics on GET \/metrics.\n\nDeliverables:\n- A bulleted list of chosen \u201clatest\u201d libraries with exact version numbers.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear inline comments indicating where each external library is used.\n- Self-contained implementation: after installing dependencies, code runs without further setup.","question_index":95,"modules":["sqlmodel","starlite","arq","langchain","openai","prometheus_client","aiosqlite","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Create a serverless real-time analytics API that ingests WebSocket events, performs on-the-fly NLP summarization, indexes embeddings, exposes a GraphQL endpoint, and deploys to AWS Lambda.\n\nFunctional Requirements:\n1. Use the latest Litestar (as of 2025-05-06) to build an HTTP\/2 GraphQL endpoint with schema-based typing.\n2. Implement a real-time WebSocket consumer using wsproto or litestar-websockets for event ingestion.\n3. Offload text summarization to an asynchronous background task leveraging the latest Transformers library.\n4. Generate embeddings with the newest OpenAI Embeddings API or sentence-transformers and store them in the latest ChromaDB vector store.\n5. Secure all endpoints with passwordless authentication via the latest fastapi-users or equivalent.\n6. Package and deploy the entire application to AWS Lambda using the latest Mangum adapter.\n\nDeliverables:\n- A numbered list of chosen \u201clatest\u201d library versions (as of 2025-05-06) mapping to each requirement.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":77,"modules":["litestar","sentence-transformers","chromadb","strawberry"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Develop a modern, high-performance AI-powered web service for real-time text processing.\n\nFunctional Requirements:\n1. Build a JSON REST API with Litestar using its latest dependency-injection and ASGI lifespan hooks.\n2. Integrate Tortoise ORM (async) to manage a PostgreSQL \u201cjobs\u201d table with migrations.\n3. Use llama-cpp-python to generate text embeddings or responses via a local LLaMA model.\n4. Expose a WebSocket endpoint with python-socketio (asyncio mode) for live client notifications.\n5. Cache recent embeddings in Redis using redis-py\u2019s asyncio client.\n6. Schedule background tasks (e.g., stale job cleanup) leveraging asyncio\u2019s latest task groups.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":60,"modules":["socketio","litestar","tortoise-orm","llama-cpp-python","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are a senior software engineer building a real-time social media monitoring application that ingests tweets, analyzes sentiment, classifies images, displays live charts, and deploys as a serverless microservice using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Ingest tweets in real time from Twitter API v2 using the latest HTTPX async streaming client.\n2. Perform NLP sentiment analysis on tweet text with the latest Hugging Face Transformers pipeline.\n3. Classify embedded images using the latest Ultralytics YOLOv8 model.\n4. Render an interactive, live-updating dashboard using the latest Plotly Dash framework.\n5. Scaffold serverless deployment with the latest AWS Lambda Powertools for Python.\n\nDeliverables:\n- A requirements list of all external dependencies.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":98,"modules":["dash","plotly","aws-lambda-powertools"],"modulenotfound":[],"modulenotfound_count":0,"module_count":3}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: A knowledge management platform must allow users to upload PDFs, index their content, and perform real-time similarity searches via a high-performance API.\n\nFunctional Requirements:\n1. Implement a PDF upload REST endpoint using the latest unstructured-sdk to extract text.\n2. Use the latest ChromaDB Python client to generate and store embeddings for each document.\n3. Provide a search REST endpoint that queries ChromaDB for top-k similar documents given a text query.\n4. Create a real-time WebSocket over HTTP\/3 endpoint using the latest aioquic to stream incremental search results.\n5. Assemble the application as an asynchronous service using the latest Starlite web framework.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":6,"modules":["starlite","unstructured","chromadb"],"modulenotfound":[],"modulenotfound_count":0,"module_count":3}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: As a senior software engineer, build an asyncio-based Python microservice showcasing HTTP\/3 REST, GraphQL, async ORM, real-time WebSockets, and scheduled tasks using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST endpoint at `\/api\/items` using FastAPI (latest) and Hypercorn (latest) with HTTP\/3 support.\n2. Expose a GraphQL API at `\/graphql` using Strawberry (latest) with asynchronous resolvers fetching from the database.\n3. Integrate PostgreSQL via Tortoise ORM (latest), define an `Item` model, and apply programmatic migrations on startup.\n4. Provide WebSocket notifications at `\/ws\/updates` using python-socketio (latest), broadcasting item creation and deletion events.\n5. Schedule an hourly cleanup job with APScheduler (latest) in asyncio mode to remove `Item` records older than 7 days.\n\nDeliverables:\n- A `requirements.txt` listing the latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is imported and utilized.","question_index":66,"modules":["fastapi","hypercorn","python-socketio","strawberry-graphql","tortoise-orm","apscheduler"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Develop a real-time AI-driven sentiment analysis web service that ingests text, processes it via an on-device LLM, persists results, and streams live analytics to clients.\n\nFunctional Requirements:\n1. Expose RESTful and WebSocket endpoints using the latest Litestar Python framework (released within the last 12 months).\n2. Perform local LLM inference for sentiment analysis with the latest llama-cpp-python library (released within the last 12 months).\n3. Asynchronously persist raw text and sentiment scores in MongoDB using the latest async MongoDB driver (released within the last 12 months) with code-first schema.\n4. Offload sentiment analysis to a distributed background worker using the latest Dramatiq (or equivalent) async task queue released in the last 12 months.\n5. Broadcast real-time sentiment updates to connected clients over WebSockets.\n6. Render an interactive dashboard in server-side templates with the latest Plotly Python library (released within the last 12 months).\n7. Instrument the entire application with distributed tracing via the latest OpenTelemetry Python SDK (released within the last 12 months).\n\nDeliverables:\n- A requirements list specifying each third-party library and its exact latest version as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines total.\n- Clear inline comments indicating where and how each external library is used.","question_index":5,"modules":["dramatiq","litestar","motor","opentelemetry","plotly","pydantic"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"You are a senior software engineer tasked with building a real-time analytics dashboard with AI-driven anomaly alerts using the latest Python libraries as of 2025-05-06.  \n\nFunctional Requirements:  \n1. Implement asynchronous REST API endpoints using the latest FastAPI.  \n2. Add WebSocket-based real-time alert streaming using the latest websockets.  \n3. Persist incoming metrics in PostgreSQL via async ORM operations using the latest SQLModel.  \n4. Cache expensive queries in Redis for high throughput using the latest aioredis.  \n5. Perform streaming anomaly detection on incoming metrics using the latest river library.  \n6. Enable file uploads of detailed reports to S3-compatible storage via the latest minio SDK.  \n\nDeliverables:  \n\u2022 A numbered list of required Python libraries (with versions as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":20,"modules":["aioredis","asyncpg","fastapi","minio","river","sqlalchemy","sqlmodel"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-flash-preview-04-17","question":"Build a real-time analytics microservice that lets authenticated users ingest sales data, stores it in PostgreSQL, streams live updates to connected clients, and generates PDF reports with embedded charts.\n\n1. Implement OAuth2 JWT authentication using the latest fastapi-jwt-auth as of May 6, 2025.  \n2. Create async CRUD endpoints for sales records with SQLModel and asyncpg.  \n3. Broadcast new sales events over WebSocket using python-socketio (latest).  \n4. Generate interactive charts in HTML via Plotly (latest) and convert them to PDF using WeasyPrint (latest).  \n5. Expose generated PDFs via an endpoint, serving files asynchronously with aiofiles.  \n\nDeliverables:  \n- A numbered confirmation of the functional requirements above.  \n- A single Python 3.11+ file under 150 lines\u2014complete, runnable code\u2014with clear comments indicating where each external library is used. Use the latest versions of all third-party libraries as of today\u2019s date.","question_index":26,"modules":["fastapi","uvicorn","python-socketio","python-jose"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are tasked with building a real-time collaborative task management web service for enterprise teams.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to expose HTTP endpoints and WebSocket routes, leveraging its new dependency injection and WebSocket lifetime management.\n2. Configure an async PostgreSQL database using the latest Tortoise-ORM with its auto-migration CLI and Pydantic model integration.\n3. Broadcast real-time task updates to connected clients via the latest python-socketio in async mode.\n4. Implement OAuth2 authentication with JWT access and refresh tokens using the latest Authlib, including token rotation.\n5. Cache sessions and publish update events using the latest aioredis async Redis client.\n6. Schedule and execute background jobs (e.g., daily summaries) with the latest APScheduler async support.\n7. Instrument request tracing and logging using the latest OpenTelemetry Python SDK, exporting to console.\n8. Auto-generate interactive API docs using FastAPI\u2019s built-in Swagger UI and Redoc.\n\nDeliverables:\n\u2022 A bullet list of selected libraries (with version numbers).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":15,"modules":["fastapi","tortoise-orm","python-socketio","aioredis","apscheduler","opentelemetry-api","opentelemetry-sdk","opentelemetry-exporter-console","authlib"],"modulenotfound":["opentelemetry-exporter-console"],"modulenotfound_count":1,"module_count":9}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are building a high-performance Python backend that ingests user articles, generates AI-powered summaries, stores and caches results, and exposes them via REST, GraphQL, and real-time channels with background processing.\n\nFunctional Requirements:\n1. Accept article submissions over a REST API using FastAPI and validate payloads with Pydantic v2.  \n2. Offload summarization to a background worker with Dramatiq using Redis as a broker.  \n3. Summarize articles via the latest OpenAI Python SDK and cache summaries with aiocache.  \n4. Persist articles and summaries in SQLite using SQLModel\u2019s async ORM features.  \n5. Expose stored summaries through a GraphQL endpoint implemented with Strawberry GraphQL.  \n6. Broadcast new summary notifications to connected clients over WebSockets with python-socketio.  \n\nDeliverables:\n\u2022 List of the latest third-party libraries (as of today) and their install commands.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":63,"modules":["sqlmodel","pydantic","fastapi","dramatiq","openai","aiocache","strawberry","python-socketio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Develop an internal Python microservice where authenticated users can submit text to be summarized by an LLM and receive real-time notifications via WebSockets.\n\nFunctional Requirements:\n1. Implement an asynchronous REST API using the latest Starlite framework (2025-05-06) leveraging its dependency injection and lifespan event system.\n2. Integrate OAuth2 Authorization Code Flow for user authentication with Authlib\u2019s async client libraries released in the last 12 months.\n3. Persist users, submissions, and summaries in PostgreSQL using the latest Piccolo ORM with async schema migrations and connection pooling.\n4. Summarize submitted text by calling OpenAI\u2019s GPT-4 Turbo via the newest openai Python SDK and its function-calling feature.\n5. Broadcast summary completion events over WebSockets using a modern Python websockets library (with HTTP\/2 support) released in the past year.\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":82,"modules":["dotenv","starlite","piccolo","authlib","openai","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"You are tasked with building a real-time analytics web dashboard that ingests streaming data, processes it, summarizes it with AI, secures user access, and visualizes results dynamically using the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Implement a REST API in Python 3.11+ with FastAPI (HTTP\/3 support via Hypercorn) to accept JSON event streams.\n2. Broadcast processed insights in real time over WebSocket using python-socketio.\n3. Buffer incoming events and generate streaming AI summaries using the latest LangChain and OpenAI Python SDK.\n4. Perform real-time aggregation and lazy analytics on events with Polars.\n5. Produce dynamic Plotly charts of key metrics and serve them as JSON for front-end rendering.\n6. Secure all endpoints and WebSocket connections with JWT-based auth via fastapi-users.\n\nDeliverables:\n\u2022 A numbered list of the above functional requirements.  \n\u2022 A single Python 3.11+ file (under 150 lines) that fulfills all requirements, complete and runnable. Include clear comments indicating where and how each external library is used.","question_index":84,"modules":["fastapi","polars","plotly","python-socketio","fastapi-users"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are a senior software engineer tasked with building a next-generation real-time collaborative data analytics dashboard. Use the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a FastAPI server with a WebSocket endpoint for live data updates using the latest fastapi[full].\n2. Expose a type-safe GraphQL API at `\/graphql` using Strawberry with async schema stitching.\n3. Integrate AI-driven insights by generating embeddings on incoming data with llama-cpp-python.\n4. Perform vector similarity search over embeddings via the Weaviate Python client\u2019s async API.\n5. Secure all endpoints with passwordless authentication using py_webauthn for WebAuthn flows.\n6. Containerize the application using docker-py to build images and dynamically generate a docker-compose config.\n7. Instrument the app with async Prometheus metrics using prometheus-client.\n\nDeliverables:\n- A list of required libraries (with versions) using the latest releases as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":72,"modules":["fastapi","strawberry-graphql","llama-cpp-python","weaviate-client","py_webauthn","docker","prometheus-client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Develop a real-time collaborative note-taking web application with AI-powered summarization.\n\nFunctional Requirements:\n1. Implement user authentication via OAuth2 password flow using the latest FastAPI features for secure login.  \n2. Expose a WebSocket endpoint for real-time CRDT-based document state synchronization using the newest FastAPI WebSocket enhancements.  \n3. Persist users and documents in PostgreSQL via the latest Piccolo ORM async API with JSONB indexing for rich querying.  \n4. Cache live presence and session metadata in Redis using aioredis with RedisJSON support for low-latency reads\/writes.  \n5. Summarize document edits on demand using the latest LangChain library and ChatOpenAI (GPT-4) integration.  \n6. Serve a minimal HTML frontend via Jinja2 templates that connects to the WebSocket and displays live updates.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of each dependency as of 2025-05-06.  \n\u2022 main.py: Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":52,"modules":["fastapi","piccolo","aioredis","langchain-openai","langchain"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Senior Software Engineer, you are tasked with building a prototype real-time collaborative note-taking service using only the latest Python libraries released in the last 12 months.\n\nFunctional Requirements:\n1. Develop an asynchronous REST API for note CRUD operations using the latest Litestar framework.\n2. Persist notes in a SQL database via a newly released async ORM library.\n3. Cache frequent note queries in Redis with a recently published async Redis client.\n4. Enable real-time collaboration via WebSockets using a modern Python WebSocket library launched within the last year.\n5. Expose an AI-powered summarization endpoint using the latest OpenAI Python client.\n6. Render an interactive HTML dashboard showing active collaborators with a new Python-to-JS visualization tool.\n7. Instrument distributed tracing and structured logging using a cutting-edge observability library released in the past 12 months.\n\nDeliverables:\n- requirements.txt listing only libraries created in the last 12 months with exact versions.\n- A single Python 3.11+ file under 150 lines containing the complete, runnable application, with clear comments indicating where each external library is used.","question_index":41,"modules":["litestar","sqlmodel","redis","openai","htmx-python","opentelemetry","opentelemetry-instrumentation-litestar","sqlalchemy"],"modulenotfound":["htmx-python","opentelemetry","opentelemetry-instrumentation-litestar"],"modulenotfound_count":3,"module_count":8}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario:\nCreate an AI-powered article summarization service that accepts user-submitted text, stores it, generates summaries asynchronously via an AI API, caches results, and streams them back to clients in real time.\n\nFunctional Requirements:\n1. Implement a REST API using FastAPI (latest) with endpoints POST \/articles to submit text and GET \/stream\/{article_id} for server-sent events.\n2. Persist submissions in a database using SQLModel (latest) with an async SQLite or PostgreSQL engine.\n3. Orchestrate a background job queue using Celery (latest) with Redis as broker to process newly submitted articles.\n4. Perform summarization using the OpenAI Python SDK (latest) and its chat completion endpoint.\n5. Cache generated summaries in Redis via aioredis (latest) with a configurable TTL.\n6. Stream summaries to connected clients in real time using server-sent events via sse-starlette (latest).\n\nDeliverables:\n\u2022 A list of all third-party libraries (with versions pinned to the latest as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":64,"modules":["sqlmodel","sqlalchemy","fastapi","sse-starlette","celery","redis","openai","aioredis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a lightweight feedback API that records user comments, analyzes sentiment in real time, stores results, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (as of 2025-05-06) to implement asynchronous REST endpoints for submitting feedback and querying stored analytics.\n2. Use the latest SQLModel to define async ORM models and persist incoming feedback in an SQLite database.\n3. Use the latest PyWebSocketX to broadcast each new sentiment result over an ASGI WebSocket endpoint.\n4. Use the latest VulcanMind inference library to perform on-the-fly sentiment analysis of submitted feedback.\n5. Implement structured logging and error handling throughout the request, inference, database, and WebSocket flows.\n\nDeliverables:\n- A requirements list with exact version pins for Starlite, SQLModel, PyWebSocketX, and VulcanMind (latest as of 2025-05-06).\n- Complete, runnable Python 3.11+ code (\u2264150 lines) that integrates all four libraries with clear comments marking where each is used.","question_index":83,"modules":["aiosqlite","starlite","sqlmodel","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-flash-preview-04-17","question":"You are a senior software engineer building a high-performance, real-time analytics web service for IoT sensor data.  \n\nFunctional Requirements:  \n1. Expose HTTP REST ingest endpoints using the latest Nebula framework (released 2024-06) with its async dependency-injection.  \n2. Persist incoming readings to PostgreSQL via StarDB ORM (v1.0+, released 2024-08), leveraging async transactions and auto-migration.  \n3. Secure all endpoints with OAuth2 PKCE flows using AuthFusion (released 2024-12).  \n4. Broadcast processed data updates to dashboard clients via WaveSocket (released 2024-10) server-clustering.  \n5. Schedule hourly aggregation jobs using ChronoTask (released 2024-11) cron-style scheduler.  \n\nDeliverables:  \n\u2022 A requirements.txt listing the latest versions of all libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":92,"modules":["nebula","stardb","authfusion","wavesocket","chronotask"],"modulenotfound":["nebula","stardb","authfusion","wavesocket"],"modulenotfound_count":4,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are a senior software engineer building a high-throughput chat summarization microservice in Python 3.11+ using only the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Real-time WebSocket ingestion: implement a `\/ws` endpoint using fastapi-socketio (latest) with session storage in Redis via `redis.asyncio`.\n2. AI summarization: enqueue incoming messages in Arq (latest) and process them with the OpenAI Python SDK (latest) calling GPT-4 Turbo for summaries.\n3. Async data persistence: store raw messages and summaries in PostgreSQL using SQLModel + encode\/databases with the asyncpg driver.\n4. Dynamic admin dashboard: expose `\/admin` rendering Jinja2 templates enhanced with HTMX via fastapi-htmx (latest).\n5. Email alerts: send summary notifications asynchronously using FastAPI-Mail (latest).\n6. Containerization: provide a multi-stage Dockerfile based on `python:3.11-slim`.\n\nDeliverables:\n- A `requirements.txt` listing the exact versions of all third-party libraries (as of May 6, 2025).\n- A single `main.py` (under 150 lines) containing complete, runnable Python 3.11+ code with clear comments indicating where and how each external library is used.","question_index":23,"modules":["redis","sqlmodel","openai","arq","fastapi","fastapi-socketio","fastapi-mail","fastapi-htmx","jinja2","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a compact Python microservice that ingests text and images, leverages AI summarization, real-time streams results, and stores everything in a modern database.\n\nFunctional Requirements:\n1. Use the latest Litestar (async web framework released Dec 2023) to define:\n   a. POST \/articles to receive JSON {\u201ctitle\u201d:\u2026, \u201ccontent\u201d:\u2026}.\n   b. WebSocket endpoint \/ws\/summary for streaming summaries.\n2. Connect asynchronously to PostgreSQL via the latest Prisma Python client (released Jul 2023), defining models for Article and Image.\n3. Summarize incoming content using the latest llama-cpp-python (released Sep 2023), utilizing its streaming interface for token-by-token output.\n4. Implement \/upload to accept multipart\/form-data image files, generate HEIF thumbnails with the latest pillow-heif (released Oct 2023), and persist file paths.\n5. Ensure all endpoints use async\/await, proper error handling, and pydantic-based validation.\n\nDeliverables:\n\u2013 A requirements.txt listing the latest library versions as of 2025-05-06.\n\u2013 A single, runnable Python 3.11+ script under 150 lines, with clear comments indicating where each external library is used.","question_index":16,"modules":["prisma","litestar","pydantic","Pillow","pillow-heif","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"You are tasked with building a prototype web service for authenticated semantic search with real-time visual feedback.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) server with:\n   a. an OAuth2 login flow using PyOIDC (latest) against an external Identity Provider  \n   b. a POST \/query endpoint to accept natural-language queries  \n   c. a WebSocket \/ws endpoint to stream real-time results  \n2. Upon authentication, embed incoming queries using sentence-transformers (latest) and perform vector similarity search in ChromaDB (latest).  \n3. As results arrive, push them incrementally over the WebSocket to clients.  \n4. Generate a Plotly (latest) bar chart of top-k result scores and serve it as an HTML snippet.  \n5. Ensure all components run under Python 3.11+, integrate only \u201cthe latest\u201d library releases as of 2025-05-06, and keep implementation concise.  \n\nDeliverables:\n\u2022 A list of chosen libraries with exact versions (the \u201crequirements\u201d).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":42,"modules":["uvicorn","fastapi","sentence-transformers","chromadb","plotly"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are building a high-throughput, real-time analytics web application for processing IoT sensor streams using the latest Python async frameworks and libraries.\n\nFunctional Requirements:\n1. Ingest JSON sensor events via a REST API built on FastAPI (use its latest HTTP\/3 support).\n2. Broadcast processed metrics to connected clients over WebSockets using python-socketio (asyncio).\n3. Offload CPU-intensive aggregation to background workers with Arq (latest Redis-backed queue).\n4. Persist time-series summaries in PostgreSQL via SQLModel (Pydantic v2-powered ORM).\n5. Cache hot query results in Redis using aioredis for sub-millisecond lookups.\n6. Expose tracing and Prometheus metrics using OpenTelemetry and prometheus-client.\n\nDeliverables:\n- A bullet list of the latest library dependencies as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments indicating where each external library is used.","question_index":40,"modules":["fastapi","socketio","arq","sqlmodel","redis","opentelemetry","prometheus_client"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":7}
{"model":"gemini-2.5-flash-preview-04-17","question":"You are a senior software engineer tasked with building a Python web application that integrates user authentication, real-time notifications, AI-based recommendations, and payment processing.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (2025-05-06) and Python 3.11 features (e.g. match-case).\n2. Secure all endpoints with OAuth2 JWT authentication via the latest fastapi-users, including email verification flows.\n3. Provide real-time notifications over WebSockets using the latest python-socketio async server.\n4. Handle one-time charges using the latest Stripe Python SDK via the Payment Intents API.\n5. Generate AI-driven product recommendations by invoking gpt-4-turbo through the latest openai library.\n6. Emit structured, context-rich logs for each operation using the latest structlog.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06  \n\u2022 Complete, runnable Python 3.11+ code (under 150 lines) with clear comments marking where each external library is used","question_index":96,"modules":["uvicorn","python-socketio","fastapi","structlog","fastapi-users","openai","stripe"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: As a senior software engineer, design a high-performance microservice for real-time document collaboration with secure access, persistence, and cloud integration.  \n\nFunctional Requirements:  \n1. Implement an asynchronous JSON REST API using the latest Python web framework available as of 2025-05-06, supporting pagination and input validation.  \n2. Secure all endpoints with OAuth2 PKCE and JWT using the latest authentication library (released within the last 12 months).  \n3. Enable bidirectional real-time collaboration events over WebSocket using the latest WebSocket library (released within the last 12 months).  \n4. Persist document data in an async NoSQL database (e.g., MongoDB) using the newest driver supporting transactions and change streams.  \n5. Upload and serve document attachments to an S3-compatible object storage using the latest SDK with multipart upload support.  \n6. Cache frequently accessed documents in Redis with TTL and automatic invalidation using the newest Redis client library.  \n\nDeliverables:  \n- A requirements.txt listing \u201cthe latest\u201d versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":80,"modules":["fastapi","pydantic","motor","redis","aiobotocore","jose"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a high-performance Python microservice that combines cutting-edge web, real-time, AI and observability features into a single ASGI application.\n\nFunctional Requirements:\n1. HTTP\/3 ASGI server with automatic TLS and QUIC support using the latest HTTP\/3 library released within the past 12 months.  \n2. Real-time WebSocket endpoint that streams JSON events with zero-copy backpressure control using the newest streaming library created in the last year.  \n3. Semantic search pipeline: accept text uploads, generate embeddings, and perform vector similarity queries via the latest vector-store client released within 12 months.  \n4. On-device LLM inference endpoint that consumes streamed text segments and returns summaries using the most recent quantized model library published this year.  \n5. Distributed tracing and metrics export using the newest OpenTelemetry OTLP exporter released within the past 12 months.\n\nDeliverables:\n- A requirements list (requirements.txt) specifying the exact, latest versions of each third-party library as of 2025-05-06.  \n- A single Python 3.11+ script under 150 lines that implements all features above, with clear comments marking where each external library is used.","question_index":51,"modules":["uvicorn","fastapi","sentence-transformers","qdrant-client","llama-cpp-python","opentelemetry-api","opentelemetry-instrumentation-fastapi","opentelemetry-sdk","opentelemetry-exporter-otlp"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario  \nDevelop an end-to-end Python web service that provides passwordless WebAuthn authentication, stores and summarizes text inputs via OpenAI, broadcasts real-time summary events, and serves interactive charts.\n\nFunctional Requirements  \n1. Implement passwordless registration and login using the latest \u201cwebauthn\u201d library (FIDO2) with Litestar middleware.  \n2. Create REST endpoints under Litestar (latest version) for submitting text to summarize and retrieving stored summaries.  \n3. Use the latest SQLModel with an async PostgreSQL engine to persist input texts, summaries, timestamps, and user IDs.  \n4. Invoke the latest \u201copenai\u201d Python client to perform GPT-4 Turbo summarization with function-calling.  \n5. Serialize and broadcast new summaries over WebSockets using the latest \u201cmsgspec\u201d for high-performance JSON.  \n6. Serve an HTML page at \u201c\/charts\u201d that embeds an interactive Plotly chart of summary lengths over time.\n\nDeliverables  \n\u2022 A numbered list of all external libraries (with exact latest versions as of 2025-05-06) and their roles.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":13,"modules":["litestar","litestar-webauthn","sqlmodel","openai","msgspec","plotly","psycopg","uvicorn","python-jose"],"modulenotfound":["litestar-webauthn"],"modulenotfound_count":1,"module_count":9}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: As a senior software engineer, you must develop a real-time analytics dashboard backend that requires user authentication, data ingestion, live updates, background enrichment via an external API, and a web-based dashboard.\n\nFunctional Requirements:\n1. Implement user registration and login endpoints using JWT authentication with asymmetric keys via FastAPI-JWT-Auth (latest).\n2. Create a POST \/data endpoint to ingest JSON payloads and store them asynchronously in SQLite using SQLModel (latest).\n3. Broadcast each new record to connected WebSocket clients at \/ws using the latest websockets library, leveraging async broadcast.\n4. Schedule a background job every minute with arq (latest) to enrich stored records by streaming JSON from a third-party API via httpx (latest).\n5. Serve a simple HTML dashboard at GET \/dashboard that opens a WebSocket to \/ws and displays incoming records in real time.\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":75,"modules":["fastapi","uvicorn","sqlmodel","fastapi_jwt_auth","arq","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Design and implement a Python microservice that exposes HTTP endpoints, real-time WebSocket notifications, persistent storage, caching, scheduled tasks, and email alerts using the latest third-party libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use FastAPI (latest) with HTTP\/2 support to expose async REST endpoints for user registration and notification retrieval.\n2. Use SQLModel (latest) as an async ORM for PostgreSQL, defining typed models for users and notifications.\n3. Implement WebSocket broadcasting via FastAPI\u2019s WebSocket support to push new notifications to connected clients.\n4. Leverage Aioredis (latest) Redis Streams to cache session state and stream notification events.\n5. Schedule periodic database cleanup and cache eviction using APScheduler (latest) with its asyncio integration.\n6. Send email alerts for critical notifications using Aiosmtplib (latest) with STARTTLS.\n7. Perform external health-check API calls using HTTPX (latest) with async retries and timeouts.\n\nDeliverables:\n- A requirements list naming each library and its latest version as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, including clear comments that indicate where each external library is used.","question_index":44,"modules":["fastapi","sqlmodel","aioredis","apscheduler","aiosmtplib","httpx","backoff"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are tasked as a senior software engineer to build an end-to-end collaborative document editing service with real-time updates, async persistence, GraphQL subscriptions, and cloud file storage.\n\nFunctional Requirements:\n1. Use the latest Litestar (first released within the last 12 months) to create an HTTP\/2 web server with WebSocket endpoints for real-time edit streams.\n2. Implement a GraphQL API with Strawberry GraphQL (latest) that supports queries, mutations, and live subscriptions for document change events.\n3. Persist documents in a PostgreSQL database via a fully async ORM of your choice that was first released in the last 12 months and supports automated migrations.\n4. Generate AWS S3 presigned URLs for resumable file attachments using aiobotocore (latest) with async upload\/download.\n5. Schedule periodic cleanup of stale sessions using an async task scheduler library first released within the last 12 months.\n\nDeliverables:\n- A brief list of the chosen third-party libraries (with exact versions) that meet the \u201cfirst released within the last 12 months\u201d requirement.\n- Complete, runnable Python 3.11+ code (under 150 lines) fulfilling the above requirements, with clear comments indicating where and how each external library is used.","question_index":78,"modules":["aiobotocore","aiosqlite","asyncpg","litestar","sqlmodel","strawberry-graphql","strawberry-graphql-litestar"],"modulenotfound":["strawberry-graphql-litestar"],"modulenotfound_count":1,"module_count":7}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario:\nYou are a senior software engineer tasked with building a real-time AI summarization microservice.\n\nFunctional Requirements:\n1. Implement an async RESTful POST endpoint `\/summarize` that accepts JSON `{ \"text\": \"<string>\" }` and returns a summary generated via `llama-cpp-python`.\n2. Implement a WebSocket endpoint `\/stream` that streams incremental summary tokens to the client as they are produced.\n3. Persist each original text and its summary in an async SQLite database using `SQLModel`.\n4. Cache recent summaries in Redis for 10 minutes with `aioredis` to avoid redundant computation.\n5. Expose Prometheus-compatible metrics on `\/metrics` (request counts, latency histograms, cache hit\/miss) via `prometheus-client`.\n6. Use a background task to warm up the Llama model at startup and to periodically clean up expired cache entries.\n\nDeliverables:\n- A restatement of the functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Code must use the latest versions of FastAPI, SQLModel, aioredis, llama-cpp-python, and prometheus-client as of 2025-05-06.\n- Include clear comments indicating where each external library is used.","question_index":54,"modules":["fastapi","uvicorn","sqlmodel","sqlalchemy","redis","llama-cpp-python","prometheus-client","aiosqlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-flash-preview-04-17","question":"You are a senior software engineer tasked with building a high-throughput real-time sentiment analysis microservice.\n\nFunctional Requirements:\n1. Use FastAPI (latest) to expose a POST \/analyze endpoint that accepts JSON text payloads and stores request metadata in PostgreSQL via SQLModel (latest).\n2. Secure all endpoints with OAuth2 JWT authentication using Authlib (latest), leveraging its PKCE support.\n3. Perform real-time sentiment analysis on incoming text using the latest Hugging Face Transformers pipeline with GPU acceleration via PyTorch (latest) and cache results in Redis using aiocache (latest).\n4. Broadcast live sentiment scores to connected clients over WebSockets using python-socketio (latest) and run the server on Uvicorn with HTTP\/3 support.\n5. Asynchronously upload any user-submitted audio snippets to S3-compatible storage via aiobotocore (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all external libraries as of 2025-05-06  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":62,"modules":["fastapi","sqlmodel","sqlalchemy","python-jose","transformers","torch","aiocache","python-socketio","aiobotocore","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a high-performance Python web application combining HTTP\/3 REST, GraphQL, real-time WebSockets, async database operations, data analytics, AI summarization, and distributed tracing.\n\nFunctional Requirements:\n1. HTTP\/3 REST API: use FastAPI (latest as of 2025-05-06) with Uvicorn HTTP\/3 support to serve \/items endpoints.\n2. Async GraphQL API: use Strawberry (latest as of 2025-05-06) to expose a GraphQL schema with DataLoader for batch fetching.\n3. Async DB access: use SQLModel (latest as of 2025-05-06) with an async SQLAlchemy engine to perform PostgreSQL CRUD.\n4. Real-time WebSockets: use websockets library (latest as of 2025-05-06) to broadcast item updates at \/ws.\n5. Data analytics: use Polars (latest as of 2025-05-06) lazy API to compute summary statistics on items.\n6. AI summarization: use OpenAI Python SDK (latest as of 2025-05-06) to generate summaries of item descriptions.\n7. Observability: use OpenTelemetry Python SDK (latest as of 2025-05-06) to instrument all endpoints for distributed tracing.\n\nDeliverables:\n- requirements.txt listing exact latest versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments marking where each external library is used.","question_index":53,"modules":["fastapi","uvicorn","sqlmodel","strawberry-graphql","websockets","polars","openai","opentelemetry-instrumentation-fastapi","opentelemetry-sdk","opentelemetry-exporter-otlp-proto-grpc","opentelemetry-api","asyncpg","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":13}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are a senior software engineer building a real-time sentiment and OCR-enhanced geospatial chat dashboard for remote field teams.\n\nFunctional Requirements:\n1. Use FastAPI (latest version as of 2025-05-06) to implement an async REST API that serves a minimal HTML client.\n2. Implement bi-directional real-time chat via Socket.IO using python-socketio (latest) on the FastAPI server.\n3. Perform on-the-fly sentiment analysis of text messages using Hugging Face\u2019s transformers pipeline (latest) and broadcast sentiment scores.\n4. Accept image uploads from clients, extract embedded text with EasyOCR (latest), and inject the OCR result into the chat stream.\n5. Plot message geolocation metadata on a Folium (latest) map and serve it as an embeddable HTML snippet.\n\nDeliverables:\n- A requirements list enumerating the exact \u201clatest\u201d third-party libraries and versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":2,"modules":["python-socketio","uvicorn","fastapi","transformers","easyocr","folium","python-multipart","torch","Pillow"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Develop a Python microservice that supports AI-driven product imagery, real-time stock updates, secure user authentication, and checkout processing.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) app with JWT-based auth using Authlib (latest) and secure password hashing.  \n2. Define async ORM models for Users and Products with SQLModel (latest) on PostgreSQL, including migration setup.  \n3. On product creation, generate an AI image via Diffusers (latest) or Stability-SDK (latest) and expose it via a REST endpoint.  \n4. Provide a WebSocket endpoint for real-time inventory updates using Starlette\u2019s WebSocket support (latest).  \n5. Create a \/checkout POST endpoint integrating Stripe\u2019s Python SDK (latest) with webhook handling to confirm payments.\n\nDeliverables:\n\u2022 A bullet list of the \u201clatest\u201d libraries (with pip install specifiers as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":85,"modules":["authlib","fastapi","passlib","sqlalchemy","sqlmodel","starlette","stripe","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are developing a high-performance, real-time AI-powered web service for personalized content recommendations.\n\nFunctional Requirements:\n1. Implement a RESTful POST \/recommend endpoint using the latest Python web framework released within the last 12 months, accepting JSON input and returning JSON responses.\n2. Add a WebSocket \/ws endpoint for streaming live recommendation updates using the newest asyncio-based WebSocket library from the past year.\n3. Integrate semantic search over user profiles by connecting to a vector database via its newest Python client library released within the last 12 months.\n4. Schedule periodic model retraining in a background worker using the most recent Python task scheduler released in the last 12 months.\n5. Store and serve user-uploaded media files on cloud object storage using the latest cloud storage client library published within the past 12 months.\n\nDeliverables:\n\u2022 A requirements list (requirements.txt) specifying the exact library names and versions (all released within the last 12 months).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total, with clear comments marking where and why each external library is used.","question_index":71,"modules":["fastapi","pydantic","uvicorn","qdrant-client","rocketry","minio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Develop a high-performance Python microservice that ingests real-time social media posts via WebSockets, generates AI summaries, stores them in MongoDB Atlas, serves secured HTTP\/3 endpoints, and provides an interactive Streamlit dashboard for sentiment trends.\n\nFunctional Requirements:\n1. Initialize an asynchronous FastAPI app with HTTP\/3 (h2\/h3) and ASGI support.  \n2. Protect all HTTP endpoints with OAuth2 JWT using Authlib\u2019s latest OAuth library.  \n3. Consume a WebSocket stream of JSON posts from wss:\/\/stream.example.com asynchronously.  \n4. Summarize each post using OpenAI\u2019s latest Python SDK leveraging function-calling or embeddings.  \n5. Persist summaries and metadata in MongoDB Atlas via Motor (async driver).  \n6. Expose a secured REST endpoint `\/summaries` supporting pagination and filtering by sentiment.  \n7. Build a Streamlit dashboard that fetches `\/summaries` and visualizes sentiment trends over time with Plotly.\n\nDeliverables:\n- A requirements list naming the latest versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":28,"modules":["fastapi","authlib","motor","openai","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a real-time HTTP\/3-powered AI chat service that streams user messages, generates LLM responses, persists embeddings in a vector DB, and exposes chat history via GraphQL.\n\nFunctional Requirements:\n1. HTTP\/3 streaming: use the latest aioquic (as of 2025-05-06) to handle bidirectional HTTP\/3 connections for chat.\n2. On-device LLM inference: integrate the latest llama-cpp-python for quantized model loading and response generation.\n3. Vector storage: use the latest chromadb Python client to embed each message (via llama-cpp-python\u2019s embed API) and store\/retrieve vectors.\n4. GraphQL API: implement an async GraphQL schema with the latest strawberry to query chat history from ChromaDB.\n5. Async orchestration: ensure end-to-end async I\/O under Python 3.11+, coordinating the QUIC server, LLM, and DB.\n6. Code constraints: keep the complete solution under 150 lines, include clear comments marking where each external library is used.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of aioquic, llama-cpp-python, chromadb, and strawberry (latest as of 2025-05-06)  \n\u2022 A single Python 3.11+ script (<150 lines) with runnable code and comments identifying each external library usage.","question_index":79,"modules":["strawberry","chromadb","llama-cpp-python"],"modulenotfound":[],"modulenotfound_count":0,"module_count":3}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Design and implement a high-performance real-time chat backend with semantic search and background processing.\n\nFunctional Requirements:\n1. Asynchronous REST API: use FastAPI (latest as of 2025-05-06) to expose user and message endpoints.\n2. HTTP\/3 WebSockets: implement real-time chat over WebSockets with Hypercorn\u2019s HTTP\/3 support.\n3. Async ORM persistence: store users and messages in PostgreSQL via SQLModel (async).\n4. Vector semantic search: index and query messages in Qdrant using the latest qdrant-client.\n5. Background sentiment analysis: enqueue and run analysis jobs with ARQ (async Redis-backed queue).\n6. JWT authentication: secure all endpoints with python-jose for JWT issuance and validation.\n7. Redis caching: cache active WebSocket connections or session data via redis.asyncio.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06  \n\u2022 A single Python 3.11+ file under 150 lines that:\n  \u2013 Is fully runnable  \n  \u2013 Shows clear comments where each external library is used  \n  \u2013 Integrates all above requirements into a cohesive application","question_index":11,"modules":["fastapi","python-jose","pydantic","sqlmodel","qdrant-client","numpy","redis","arq","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: A fintech startup requires a live transaction monitoring service that ingests, processes, stores, and notifies users of suspicious activity in real time.\n\nFunctional Requirements:\n1. Build an HTTP endpoint to receive and stream-respond to JSON transaction payloads using the latest async web framework released in the last 12 months.\n2. Validate and serialize incoming data with the latest data validation library, enforcing strict type constraints and custom validators.\n3. Persist transactions asynchronously in a relational database using the latest async ORM library compatible with Python 3.11+.\n4. Offload CPU-intensive anomaly detection to a background worker queue using the latest task orchestration library.\n5. Broadcast real-time alerts to connected clients via WebSockets using the latest real-time communication library.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of the latest libraries (as of May 6, 2025)  \n\u2022 A single Python 3.11+ file under 150 lines, complete and runnable, with clear comments indicating where each external library is used.","question_index":39,"modules":["fastapi","pydantic","sqlalchemy","aiosqlite","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a real-time collaborative document editor service with AI-powered summarization, OAuth2 access control, WebSocket syncing, Redis caching, and NoSQL persistence.\n\nFunctional Requirements:\n1. Use the latest async Python web framework (as of 2025-05-06) to define RESTful CRUD endpoints for documents.\n2. Implement OAuth2 authorization with the latest Python OAuth library released within the last 12 months for secure user login and token issuance.\n3. Establish bi-directional real-time document synchronization over WebSockets using the newest real-time communication library (with built-in pub\/sub).\n4. Integrate the latest AI inference library published in the past year to generate live summaries of incremental document edits.\n5. Employ the latest Redis client library (released within the last year) to cache active sessions and coordinate pub\/sub channels for syncing.\n6. Persist document state and version history in a NoSQL database using the newest async database client released in the last 12 months.\n\nDeliverables:\n1. A numbered list of the chosen \u201clatest\u201d libraries (with a one-line description of their role).\n2. Complete, runnable Python 3.11+ code under 150 lines, including clear comments indicating where and why each external library is used.","question_index":86,"modules":["fastapi","redis","motor","python-jose"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Develop a user registration microservice over HTTP\/3 that validates input, persists user data, and integrates with an external email verification API.\n\nFunctional Requirements:\n1. Expose a POST \/users endpoint using Starlite (latest) with HTTP\/3 support.  \n2. Define request and response schemas with Pydantic v2 (latest) for data validation.  \n3. Persist user records in PostgreSQL via the latest Prisma Python client, including model definitions and migrations.  \n4. Perform an asynchronous HTTP\/3 request to an external email verification service using httpx (latest), and incorporate its response.  \n5. Return a JSON payload containing the stored user data and email verification status.\n\nDeliverables:\n- A list of required third-party libraries with exact versions (as of today)  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":100,"modules":["httpx","pydantic","starlite","prisma"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: As a senior software engineer, build a modular web application that authenticates users with WebAuthn passkeys, streams real-time stock data, generates AI trading signals, processes payments, and delivers interactive dashboards.\n\nFunctional Requirements:\n1. Implement user authentication via WebAuthn using the latest pywebauthn library (as of 2025-05-06) with hardware security key support.\n2. Ingest and stream real-time stock price data over WebSockets using FastAPI and the latest Starlette HTTP\/2 push features.\n3. Generate AI-driven trading signals leveraging the latest OpenAI Python SDK\u2019s new function-calling interface.\n4. Integrate secure payment processing with the latest Stripe Python SDK supporting Payment Element and webhook handling.\n5. Serve interactive dashboards using the latest Plotly Dash or Reflex library for live data visualization and client-side callbacks.\n6. Add structured logging and Prometheus metrics using structlog and prometheus-client.\n\nDeliverables:\n- A concise mapping of each functional requirement to its implementation approach.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used and using the latest libraries available as of 2025-05-06.","question_index":56,"modules":["fastapi","pywebauthn","openai","stripe","structlog","prometheus-client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"You are a senior software engineer tasked with architecting a small, high-performance async web application using only the latest Python libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Litestar framework (with native HTTP\/3 support) to define async REST endpoints for user signup and profile retrieval.  \n2. Integrate the latest Piccolo-ORM to define a PostgreSQL \u201cusers\u201d table and perform async CRUD operations.  \n3. Implement real-time notifications via WebSocket using the latest python-socketio in asyncio mode.  \n4. Schedule a daily summary email job with APScheduler\u2019s new AsyncIOScheduler and send mail via an async SMTP library.  \n5. Fetch external data during signup from a third-party API using HTTPX\u2019s HTTP\/2 client.\n\nDeliverables:\n\u2022 A requirements.txt listing exact package names and latest versions.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":29,"modules":["litestar","piccolo","python-socketio","apscheduler","aiosmtplib","httpx","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are a senior software engineer building a high-performance real-time IoT analytics backend.\n\nFunctional Requirements:\n1. Expose an asynchronous HTTP API using FastAPI with WebSocket support to ingest and broadcast live sensor readings.\n2. Persist incoming data in PostgreSQL via SQLModel\u2019s async ORM and validate payloads with Pydantic v2 models.\n3. Enforce per-device rate limiting using Redis Streams and aioredis, leveraging RedisJSON for overflow handling.\n4. Secure all endpoints with OAuth2 passwordless login via FastAPI-Users, sending one-time codes.\n5. Schedule background aggregation jobs with Celery (beat scheduler) to compute rolling metrics and push updates over WebSockets.\n6. Provide a paginated historical metrics REST endpoint, caching hot queries in Redis for low-latency responses.\n\nDeliverables:\n- A list of required external libraries pinned to the latest versions available as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is utilized.","question_index":27,"modules":["fastapi","sqlmodel","pydantic","redis","fastapi-users","celery"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a real-time collaborative document review service with semantic search and AI summarization.\n\nFunctional Requirements:\n1. HTTP API (HTTP\/3) using the latest starlite (v1.x) to manage documents and user sessions.  \n2. Real-time updates over WebSockets for live editing and presence using starlite\u2019s WebSocket support.  \n3. Semantic indexing and search: on each document save, compute embeddings and store\/retrieve via the latest qdrant-client.  \n4. AI summarization endpoint: call an LLM using the latest llm-tools library to generate concise summaries on demand.  \n5. Background task scheduling with dramatiq (latest) to clean up stale sessions and regenerate indexes.\n\nDeliverables:\n\u2022 Reiterate the numbered requirements.  \n\u2022 A complete, runnable Python 3.11+ code file under 150 lines.  \n\u2022 Use the latest starlite, qdrant-client, llm-tools, and dramatiq libraries available as of 2025-05-06.  \n\u2022 Include clear comments indicating where and why each external library is used.","question_index":55,"modules":["starlite","qdrant-client","dramatiq"],"modulenotfound":[],"modulenotfound_count":0,"module_count":3}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are building a high-performance, AI-driven text-generation microservice for a real-time web application.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/generate endpoint using FastAPI (latest 0.100+) for incoming requests.\n2. Validate and parse the request payload with Pydantic v2 (latest) leveraging its new ArgModel feature.\n3. Enqueue each generation request as an asynchronous job using ARQ (latest 0.7+) backed by Redis.\n4. In the ARQ worker, invoke vLLM (latest 0.7+) AsyncLLMClient to perform streaming text inference.\n5. Persist each request and its full response to PostgreSQL using SQLModel (latest 0.0.x) with SQLAlchemy 2.0 async engine.\n6. Expose a WebSocket \/ws\/stream endpoint in FastAPI to broadcast partial LLM outputs to clients in real time.\n\nDeliverables:\n\u2022 The above numbered requirements list.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":30,"modules":["fastapi","pydantic","sqlalchemy","sqlmodel","arq","vllm","asyncpg","redis","torch","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: As a senior software engineer, build a high-performance microblogging API using the latest Python libraries for REST, validation, async database operations, caching, background tasks, and real-time streaming.\n\nFunctional Requirements:\n1. Implement a user signup endpoint in FastAPI (>=0.100.0) using Pydantic v2 root_validator for advanced schema validation.  \n2. Create and list posts asynchronously in PostgreSQL via SQLModel\u2019s latest async engine.  \n3. Maintain a cache of the 10 most recent posts in Redis using redis-py (>=5.0) asyncio client, updating on post creation.  \n4. Offload image thumbnail generation to an async Dramatiq (>=2.0) background task triggered on new post with image.  \n5. Provide a WebSocket endpoint in FastAPI to broadcast newly created posts to connected clients in real time.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library (FastAPI, Pydantic v2, SQLModel, redis-py asyncio, Dramatiq) is used. Use the latest available versions of all libraries as of 2025-05-06.","question_index":87,"modules":["dramatiq","dramatiq-redis","fastapi","pydantic","redis","sqlmodel"],"modulenotfound":["dramatiq-redis"],"modulenotfound_count":1,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are a senior software engineer tasked with building a real-time analytics microservice that combines REST, GraphQL, persistent storage, and background processing using the latest Python ecosystem tools as of 2025-05-06.\n\nFunctional Requirements:\n1. Expose HTTP and WebSocket endpoints using the latest Litestar ASGI framework with annotated routing and streaming support.\n2. Implement a GraphQL API using the latest Strawberry GraphQL code-first schema builder with asyncio support.\n3. Persist and query analytics data in PostgreSQL via the latest Piccolo-ORM\u2019s async-first models and automatic migrations.\n4. Offload long-running analytics jobs to a Redis-backed async queue using the latest Arq library.\n5. Send WebSocket notifications to clients when background jobs complete.\n\nDeliverables:\n\u2022 A requirements list (pip install commands) specifying the latest versions of Litestar, Strawberry, Piccolo-ORM, and Arq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":65,"modules":["piccolo","strawberry-graphql","arq","litestar","httpx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You must build a Python microservice that fetches articles by URL, summarizes them with AI, stores results in Postgres, and provides REST, GraphQL, and WebSocket interfaces with background orchestration using only the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a POST \/submit REST endpoint using the latest Starlite framework (released within the last 12 months) over HTTP\/3 to accept JSON { \"url\": string }.  \n2. Fetch article content asynchronously using the latest httpx library and handle timeouts and redirects.  \n3. Summarize text with the latest HuggingFace Transformers v5 pipeline, leveraging GPU if available.  \n4. Persist the original URL and its summary in Postgres via Tortoise ORM v0.25 (async ORM released in the last 12 months).  \n5. Expose a \/graphql endpoint using Strawberry GraphQL (latest release) to query summaries by URL.  \n6. Provide a \/ws\/notifications WebSocket in Starlite to notify clients in real time when a summary is ready.  \n7. Orchestrate fetch-and-summarize tasks as background jobs using Prefect 2 (newly released workflow orchestrator).\n\nDeliverables:\n- A copy of the numbered functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Use only the latest versions of Starlite, httpx, Transformers v5, Tortoise ORM v0.25, Strawberry GraphQL, and Prefect 2 as of 2025-05-06.\n- Include clear comments indicating where and how each external library is used.","question_index":70,"modules":["starlite","starlite-graphql","httpx","transformers","tortoise-orm","strawberry-graphql","prefect"],"modulenotfound":["starlite-graphql"],"modulenotfound_count":1,"module_count":7}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a real-time collaborative messaging microservice integrating graph storage, vector search, JWT authentication, and WebSockets using the latest Python libraries (as of May 6, 2025).\n\nFunctional Requirements:\n1. Authentication: secure all endpoints with JWTs using the latest TokenGuard library.\n2. Message Ingestion: implement POST \/messages to accept user messages; decode JWT, extract user ID.\n3. Graph Storage: store each message and its \u201cauthored_by\u201d relationship in a graph database via the latest GraphitORM.\n4. Vector Indexing & Search: generate text embeddings with the latest Morpher library, store them in its vector store, and expose GET \/search?q={text} to return top-5 similar messages.\n5. Real-Time Updates: broadcast new messages to connected clients over WebSockets using the latest Voila framework.\n\nDeliverables:\n1. A requirements list with each third-party library pinned to its latest version as of May 6, 2025.\n2. A single Python 3.11+ script (\u2264150 lines) implementing all endpoints, with clear comments marking where each external library is used.","question_index":35,"modules":["fastapi","jose","sentence-transformers","chromadb","networkx"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are a senior software engineer tasked with building a serverless real-time IoT analytics service that ingests sensor data, performs streaming aggregation, applies online anomaly detection, and streams results to clients.\n\nFunctional Requirements:\n1. Ingest JSON messages from an MQTT broker (\u201csensors\/+\/data\u201d) using the latest asyncio-based gmqtt library (as of 2025-05-06).  \n2. Stream-process incoming messages with streamz (latest version) to compute 1-minute tumbling-window averages for temperature and humidity.  \n3. Apply online anomaly detection on each windowed aggregation using River\u2019s latest HDBSCAN-based drift detector.  \n4. Persist raw messages and aggregated results into TimescaleDB via the asyncpg driver (latest version).  \n5. Expose a Python 3.11+ FastAPI application with a WebSocket endpoint for live metric and anomaly alert streaming.  \n6. Wrap the FastAPI app for AWS Lambda deployment using the latest mangum adapter.\n\nDeliverables:\n\u2022 A list of third-party dependencies pinned to \u201clatest\u201d versions as of 2025-05-06.  \n\u2022 A single, complete, runnable Python 3.11+ source file under 150 lines.  \n\u2022 Clear inline comments showing where each external library is used.  \n\u2022 The code must integrate all functional requirements and be deployable serverlessly on AWS Lambda.","question_index":38,"modules":["pandas","fastapi","gmqtt","mangum","asyncpg","streamz","river","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario:\nYou are tasked with developing a lightweight AI-powered semantic search microservice in Python 3.11+.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI framework with HTTP\/3 support) to expose a POST \/search endpoint.  \n2. Validate and parse incoming JSON with the latest Pydantic v2\u2019s strict model parsing and Python pattern-matching features.  \n3. Asynchronously fetch text embeddings from OpenAI\u2019s embedding API using the latest HTTPX client with HTTP\/3.  \n4. Store and query vectors in a GPU-accelerated ChromaDB instance via the latest ChromaDB Python client.  \n5. Return the top 5 semantically related document IDs and similarity scores as a JSON response.\n\nDeliverables:\n\u2022 A numbered list confirming the above functional requirements.  \n\u2022 A single, complete, runnable Python 3.11+ source file (\u2264150 lines) that implements the microservice, with clear comments indicating where each external library is used.","question_index":3,"modules":["httpx","starlite","pydantic","chromadb","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a real-time collaborative note-taking microservice integrating REST, database persistence, caching, GraphQL, WebSockets, and background tasks.\n\nFunctional Requirements:\n1. Expose REST endpoints using FastAPI (latest) for creating, updating and retrieving notes; leverage Pydantic v2\u2019s new validation decorators.\n2. Persist notes asynchronously in SQLite via SQLModel (latest) with SQLAlchemy 2.0-style typing and async engine.\n3. Broadcast note creation and updates over WebSockets with FastAPI\u2019s WebSocket support to subscribed clients in real time.\n4. Cache note retrieval results in Redis using aioredis (latest), implement TTL and cache invalidation on updates.\n5. Provide a GraphQL schema and endpoint using Strawberry (latest) to query and filter notes with federation-style resolvers.\n6. Schedule a daily summary email using FastAPI BackgroundTasks and aiosmtplib (latest) for asynchronous SMTP delivery.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- A single Python 3.11+ source file under 150 lines that implements all requirements, with clear comments marking where each external library is used.","question_index":73,"modules":["fastapi","pydantic","sqlmodel","sqlalchemy","aioredis","strawberry-graphql"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are building a real-time news aggregation microservice with live client updates.\n\nFunctional Requirements:\n1. Develop a REST API using Litestar (the latest version as of 2025-05-06) leveraging its OpenAPI v3.1 auto-generation feature.\n2. Asynchronously fetch and parse external RSS feeds over HTTP\/3 using HTTPX AsyncClient (the latest), then cache results in aiocache (the latest) with a TTL.\n3. Validate and serialize each article using Pydantic v2 (the latest) functional-style models to ensure data integrity.\n4. Expose a WebSocket endpoint using the websockets library (the latest) with permessage-deflate compression for real-time article broadcasting.\n5. Schedule periodic background tasks with Arq (the latest) to re-poll feeds every 5 minutes.\n\nDeliverables:\n- A list of the chosen libraries and their latest versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":31,"modules":["pydantic","litestar","httpx","aiocache","websockets","arq"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a real-time chat analysis service that ingests user messages, performs on-device LLM summarization and sentiment analysis, caches results, persists chat history, and broadcasts updates via WebSockets.\n\nFunctional Requirements:\n1. Expose a POST \/messages endpoint using FastAPI (latest) to receive JSON payloads with \u201cuser_id\u201d and \u201ctext\u201d.\n2. Perform sentiment analysis and summarization on each message using llama-cpp-python (latest) in an async background task.\n3. Cache sentiment results for identical texts in Redis via redis-py (latest) to avoid recomputation.\n4. Persist messages, sentiments, and summaries in PostgreSQL using SQLModel (latest version).\n5. Provide a WebSocket \/ws\/{user_id} endpoint (FastAPI) that broadcasts new summaries and sentiments to connected clients in real time.\n6. Secure all endpoints with OAuth2 password flow via Authlib (latest).\n\nDeliverables:\n\u2022 Restate the numbered requirements above.  \n\u2022 Provide complete, runnable Python 3.11+ code integrating the latest versions of FastAPI, llama-cpp-python, redis-py, SQLModel, and Authlib, under 150 lines total.  \n\u2022 Include clear comments indicating where and how each external library is used.","question_index":32,"modules":["fastapi","sqlmodel","redis","llama-cpp-python","authlib"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"You are a senior software engineer.\n\nScenario: Build an end-to-end Python service for user profile CRUD with high-performance caching and real-time notifications.\n\nFunctional Requirements:\n1. Implement RESTful CRUD endpoints for user profiles using the latest FastAPI (as of 2025-05-06), leveraging its async lifespan context.\n2. Define an async PostgreSQL-backed User model with SQLModel (latest) using Relations and the new async engine.\n3. Cache GET \/users\/{id} responses in Redis with redis.asyncio (latest), utilizing cluster mode and automatic TTL invalidation on updates.\n4. Broadcast profile change events in real time over WebSocket using the websockets library (latest) with permessage-deflate compression.\n\nDeliverables:\n- A requirements.txt listing the latest versions of FastAPI, SQLModel, redis.asyncio and websockets as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":17,"modules":["sqlmodel","redis","websockets","fastapi"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are a senior software engineer building a proof-of-concept Python service that ingests user text, generates embeddings and classifications via an LLM, stores them, and streams nearest-neighbor results in real time.\n\nFunctional Requirements:\n1. Implement a JWT-secured POST \/ingest endpoint using the latest async Python web framework as of 2025-05-06 (e.g., Litestar).  \n2. In \/ingest, accept JSON `{ \"text\": \"...\" }`, call a state-of-the-art LLM (via the latest Transformers or LangChain client) to produce both an embedding vector and a classification label.  \n3. Persist the embedding vector in a vector database using the newest ChromaDB client.  \n4. Log the original text and its classification into a relational database through the latest async ORM (e.g., Prisma Python).  \n5. Expose a GET \/stream endpoint that streams nearest-neighbor matches for incoming embeddings over server-sent events using the newest SSE library (e.g., httpx-sse or framework-native support).  \n6. Target Python 3.11+, include type hints, and keep all code under 150 lines.\n\nDeliverables:\n\u2022 A `requirements.txt` (or equivalent) listing each library with exact versions (latest as of 2025-05-06).  \n\u2022 A single Python module (\u2264150 lines) that meets all requirements, with clear comments marking where and why each external library is used.","question_index":81,"modules":["anyio","chromadb","litestar","prisma","pydantic","python-jose","sentence-transformers"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are tasked with developing a next-generation chat analytics platform that seamlessly integrates HTTP\/3, JWT auth, real-time messaging, vector search, and on-device LLM inference.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) with HTTP\/3 support via Hypercorn to serve all endpoints.\n2. Implement JWT authentication with rotational refresh tokens using fastapi-users (latest).\n3. Provide a real-time WebSocket chat endpoint using fastapi-websocket-pubsub (latest) supporting pub\/sub channels.\n4. On each incoming chat message, generate vector embeddings using llama-cpp-python (latest) with an on-device streaming pipeline, and store them in ChromaDB (latest) with HNSW indexing.\n5. Expose a REST endpoint to query semantic similarity: accept a text query, compute embeddings, perform a ChromaDB search, and return the top-3 similar messages.\n6. Expose a streaming summarization endpoint that returns an on-the-fly summary of recent messages using llama-cpp-python\u2019s streaming completion interface.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements you\u2019ve implemented.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":93,"modules":["fastapi","fastapi-users","pydantic","fastapi-websocket-pubsub","chromadb","llama-cpp-python"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"You are tasked with building a high-performance text summarization service with real-time updates, persistent storage, and caching for internal research teams.\n\nFunctional Requirements:\n1. Implement OAuth2 password flow with JWT tokens for user authentication using the latest python-jose.\n2. Expose a POST \/summarize endpoint that uses the latest transformers library to perform text summarization.\n3. Provide a WebSocket at \/ws\/progress to stream summarization progress in real time using FastAPI\u2019s latest WebSocket support.\n4. Persist user requests and summaries in a SQLite database via the latest SQLModel ORM.\n5. Cache summary results in Redis for 5 minutes using the latest aioredis library to accelerate repeat requests.\n\nDeliverables:\n\u2022 A requirements list specifying the latest versions of FastAPI, transformers, python-jose, SQLModel, aioredis (as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":10,"modules":["fastapi","python-jose","pydantic","transformers","sqlmodel","aioredis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python microservice for real-time IoT sensor analytics and visualization.\n\nFunctional Requirements:\n1. REST API: Use the latest asynchronous Python web framework available as of 2025-05-06 to expose CRUD endpoints with advanced dependency injection and OpenAPI schema generation.\n2. WebSockets: Integrate the latest real-time streaming library as of 2025-05-06 supporting protocol-level backpressure to push live sensor updates to clients.\n3. ML Inference: Embed an on-the-fly analytics model using the latest Python JIT-accelerated inference library as of 2025-05-06, auto-batching requests on CPU\/GPU.\n4. Graph Storage: Persist sensor relationships in a distributed graph database via the latest async driver as of 2025-05-06 with reactive query pipelining.\n5. Dashboard UI: Serve an interactive web dashboard using the latest server-driven UI component library as of 2025-05-06 for real-time charting and controls.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":14,"modules":["fastapi","uvicorn","neo4j","torch","dash"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"You are a Senior Software Engineer tasked with building a high-performance, real-time analytics microservice using the latest asynchronous Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled ASGI endpoint using the latest Litestar and aioquic (as of 2025-05-06) for bi-directional streaming.\n2. Implement real-time GraphQL subscriptions with the latest Strawberry GraphQL to push analytics updates.\n3. Define domain models and perform asynchronous CRUD operations on Postgres using the latest SQLModel.\n4. Integrate a Redis cache layer with the latest aioredis for hot-path data retrieval.\n5. Instrument request handling and background tasks with OpenTelemetry SDK for distributed tracing.\n\nDeliverables:\n- requirements.txt listing the latest library versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments indicating where each external library is used.","question_index":90,"modules":["aioredis","litestar","opentelemetry-api","opentelemetry-instrumentation-litestar","opentelemetry-sdk","sqlmodel","strawberry-graphql"],"modulenotfound":["opentelemetry-instrumentation-litestar"],"modulenotfound_count":1,"module_count":7}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: As a senior software engineer, develop a Python 3.11+ microservice that manages real-time collaborative whiteboard sessions, persists data, exposes REST and GraphQL APIs, generates session snapshots, integrates external metrics, and secures endpoints with OAuth2.\n\nFunctional Requirements:\n1. Implement session and user management REST endpoints using FastAPI (latest) with async event hooks and Pydantic V2 models.  \n2. Persist all data in PostgreSQL via Tortoise ORM (latest) using model lifecycle events.  \n3. Expose a GraphQL API with Strawberry GraphQL (latest) dataclass resolvers for session and user queries\/mutations.  \n4. Broadcast real-time drawing events using python-socketio (latest) over WebSockets.  \n5. Generate on-demand PNG snapshots of whiteboard state with Pillow-SIMD (latest) leveraging multi-threaded rendering.  \n6. Send session metrics asynchronously to an external analytics service using httpx (latest) with HTTP\/2 support.  \n7. Secure all endpoints with OAuth2 (authorization code + PKCE) using Authlib (latest).  \n\nDeliverables:\n- A numbered list of the functional requirements above.  \n- Complete, runnable Python 3.11+ code (\u2264 150 lines) integrating all requirements.  \n- Inline comments indicating where each external library is used.  \n- Use the latest stable releases of all libraries as of today\u2019s date.","question_index":50,"modules":["httpx","Pillow","tortoise-orm","fastapi","pydantic","python-socketio","strawberry-graphql"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-flash-preview-04-17","question":"You are building a real-time collaborative code-review web application that leverages the latest AI, async, storage and security frameworks.\n\nFunctional Requirements:\n1. Implement a FastAPI 0.95+ server with async endpoints and Jinja2 templates for the main UI.\n2. Enable real-time code editing and review comments via WebSockets using the latest websockets library as of 2025-05-06.\n3. On each code submission, stream analysis from OpenAI\u2019s GPT-4o API (or equivalent newest model) and display suggestions in real time.\n4. Persist code snapshots and review metadata in Pinecone\u2019s newest Python client (vector storage) for similarity search.\n5. Secure all HTTP and WS routes with JWT authentication using the latest PyJWT release.\n6. Add structured logging and a \/healthz endpoint using structlog for observability.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total.  \n\u2022 Clear comments indicating where and why each external library is used.","question_index":37,"modules":["structlog","uvicorn","fastapi","PyJWT"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are a senior software engineer building a real-time sentiment-analysis web service for social media feeds.\n\nFunctional Requirements:\n1. Use the latest Python web framework (released within the last 12 months) to expose REST endpoints with dependency injection and OpenAPI support.\n2. Implement real-time websocket notifications for incoming sentiment updates using a recently released WebSocket library (last 12 months).\n3. Persist incoming posts and sentiment scores in a NoSQL database via its newest async client library (released within the last 12 months).\n4. Offload sentiment inference to a CPU-optimized ML library (first released within the last 12 months) with zero-copy or memory-efficient batch inference.\n5. Schedule periodic cleanup and summary-generation tasks using a lightweight scheduler library introduced within the last 12 months.\n6. Serve a minimal interactive dashboard at \u201c\/dashboard\u201d using a modern, Python-native plotting\/UI library created in the last 12 months.\n\nDeliverables:\n- A bullet list of all third-party libraries chosen, pinned to \u201clatest\u201d versions as of today\u2019s date.\n- A single, complete Python 3.11+ file (under 150 lines) that implements all requirements, with clear comments marking where each external library is used.","question_index":94,"modules":["esmerald","pydantic","motor","transformers","torch","aioschedule","nicegui","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: Build a real-time Q&A web application that ingests documents, generates embeddings, serves semantic search, supports live WebSocket notifications, and exposes observability metrics.\n\nFunctional Requirements:\n1. Use the latest FastAPI (>=0.100) to implement asynchronous REST endpoints for document ingestion and search, plus a WebSocket endpoint leveraging its new lifecycle event hooks.\n2. Upon document ingestion, asynchronously generate embeddings with the latest llama-cpp-python (v0.2+) using 4-bit quantization and store vectors in Pinecone (latest) using hybrid search.\n3. Implement a `\/search` endpoint that queries Pinecone for nearest neighbors based on user input and returns ranked results.\n4. Schedule periodic re-indexing tasks with the latest arq (v1.10+), utilizing its async Redis job scheduler.\n5. Integrate OpenTelemetry Python SDK (v1.21+) for auto-instrumentation, exporting traces and metrics to Prometheus.\n\nDeliverables:\n- A requirements.txt listing the latest libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":33,"modules":["arq","fastapi","llama-cpp-python","opentelemetry-distro","opentelemetry-exporter-prometheus","opentelemetry-instrumentation-fastapi","pinecone-client","pydantic","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: As a senior software engineer, you must build a real-time analytics backend that ingests user events, persists them, and serves both REST and GraphQL clients with low latency and live updates.\n\nFunctional Requirements:\n1. Implement a POST \/events endpoint using the latest FastAPI (as of 2025-05-06), leveraging its new lifespan context and async router features.\n2. Validate incoming JSON payloads with Pydantic v2\u2019s new @model_validator decorator for strict schema enforcement.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise ORM, utilizing its recent bulk_create optimization.\n4. Expose a GraphQL query and mutation schema via the latest Strawberry GraphQL with code-first federation support.\n5. Broadcast new events to connected clients over WebSockets using the latest websockets library with per-message deflate compression.\n\nDeliverables:\n\u2022 A requirements.txt listing each third-party library pinned to its latest version as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments indicating where and how each external library is used.","question_index":19,"modules":["fastapi","pydantic","tortoise-orm","strawberry-graphql","strawberry-graphql-fastapi","websockets"],"modulenotfound":["strawberry-graphql-fastapi"],"modulenotfound_count":1,"module_count":6}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: As a senior software engineer, you must build a modular Python web application that serves real-time, ML-enhanced classification results with persistent storage and observability.\n\nFunctional Requirements:\n1. API Layer: Use the latest ASGI web framework available as of 2025-05-06 that supports HTTP\/3 and automatic OpenAPI generation to expose a POST \/classify endpoint.  \n2. Database Layer: Connect asynchronously to PostgreSQL using the latest async driver with statement pooling to record incoming requests and classification results.  \n3. Real-time Pub\/Sub: Implement server-side WebSocket broadcasts of classification results using the latest message queue library offering at-least-once delivery semantics.  \n4. ML Inference: Integrate on-demand image classification via the latest lightweight, GPU-accelerated inference engine released in the past 12 months.  \n5. Observability: Instrument distributed tracing and metrics collection using the latest OpenTelemetry-compatible tracing library.\n\nDeliverables:\n- A list of the latest library dependencies as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":18,"modules":["fastapi","asyncpg","aio-pika","onnxruntime","Pillow","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are a senior software engineer building a modern microblogging backend that supports authenticated posts, AI-generated images, real-time feed updates, and cloud storage integration.\n\nFunctional Requirements:\n1. Implement a REST API using a Python web framework released in the last 12 months (use the latest available as of 2025-05-06).\n2. Persist posts in a SQL database via an async ORM library created in the last 12 months, leveraging advanced async model features.\n3. Broadcast new posts to connected clients over WebSockets using a recently released high-performance async messaging library.\n4. Authenticate users with OAuth2 (Google) using the latest OAuth client library.\n5. Generate post images by calling Stable Diffusion through the most recent Python API client for AI image generation.\n6. Store and serve AI-generated images in AWS S3 using the newest AWS SDK for Python.\n7. Ensure all components run under Python 3.11+ and fit within 150 lines of code.\n\nDeliverables:\n\u2022 A numbered list of the specific libraries (with versions) you chose to satisfy each requirement.  \n\u2022 Complete, runnable Python 3.11+ code (<150 lines) combining all features.  \n\u2022 Clear inline comments marking where and why each external library is used.","question_index":7,"modules":["fastapi","pydantic","sqlalchemy","aioboto3","aiosqlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-flash-preview-04-17","question":"Scenario: You are a senior software engineer tasked with developing an end-to-end Python web application that leverages the latest libraries to showcase advanced API development, ORM integration, security, background processing, and real-time features.\n\nFunctional Requirements:\n1. Asynchronous REST API: implement CRUD endpoints for a `Task` model using FastAPI (latest version as of 2025-05-06).\n2. Data Modeling and Persistence: define `Task` ORM models with SQLModel (latest version) and connect to a SQLite or PostgreSQL database asynchronously.\n3. Security: secure all API endpoints with OAuth2 password flow using Authlib (latest version) to obtain and validate JWT tokens.\n4. Background Processing: create an asynchronous task queue with Arq (latest version) to process tasks in the background.\n5. Real-Time Notifications: establish a WebSocket endpoint using the websockets library (latest version) to push real-time task status updates.\n\nDeliverables:\n- The numbered list of functional requirements (as above).\n- Complete, runnable Python 3.11+ code under 150 lines. Use the latest versions of FastAPI, SQLModel, Authlib, Arq, and websockets as of 2025-05-06. Include clear comments indicating where and how each external library is used.","question_index":24,"modules":["arq","fastapi","python-jose","pydantic","sqlalchemy","sqlmodel","aiosqlite","websockets","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: As a senior software engineer, build a high-performance Python web application integrating real-time streaming, GraphQL, async ORM, JWT security, and caching using the latest third-party libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. HTTP & WebSocket server with the latest Litestar, utilizing its new streaming response API for real-time event delivery.\n2. GraphQL API with the latest Strawberry-GraphQL, defining queries, mutations, and subscription streams for live data updates.\n3. Async database models and CRUD operations using the latest Piccolo ORM\u2019s Python 3.11+ async support and built-in migration tooling.\n4. JWT authentication via the latest python-jose, supporting secure login, token rotation, and injection of user context into requests.\n5. Redis-backed caching with the latest aioredis to cache GraphQL query results and automatically invalidate on data changes.\n6. Trigger real-time client notifications over WebSockets when database events occur, leveraging Litestar\u2019s WebSocket routing.\n\nDeliverables:\n\u2022 A numbered list of the final functional requirements.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":57,"modules":["aioredis","litestar","piccolo","pydantic","python-jose","strawberry-graphql"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are tasked with building a modular Python 3.11+ microservice that delivers real-time chat with semantic search and scheduled maintenance, leveraging only the latest libraries released within the past 12 months.\n\nFunctional Requirements:\n1. Implement HTTP REST endpoints for posting and retrieving chat messages using Litestar (latest release).  \n2. Establish a WebSocket chat channel that broadcasts new messages to all connected clients via the official Litestar WebSocket plugin (latest).  \n3. Persist each message to MongoDB with Beanie ODM (latest) and simultaneously index its vector embedding into Qdrant using qdrant-client (latest).  \n4. Expose a GraphQL query endpoint that performs semantic search over stored messages via strawberry-graphql (latest).  \n5. Schedule a daily background job to purge messages older than 30 days using Dramatiq (latest).\n\nDeliverables:\n- A requirements.txt (or pyproject.toml) listing all dependencies with exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments marking where and how each external library is used.","question_index":76,"modules":["beanie","dramatiq","litestar","motor","pydantic","python-dotenv","qdrant-client","sentence-transformers","strawberry-graphql"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python web service that combines asynchronous REST & GraphQL APIs, real-time notifications, persistent storage, caching, and AI-driven text summarization.\n\nFunctional Requirements:\n1. Implement async HTTP endpoints with FastAPI (latest as of 2025-05-06) using Python 3.11 structural pattern matching and Pydantic v2 dataclass models.\n2. Provide a GraphQL endpoint powered by Strawberry GraphQL v1.0+ with schema federation support.\n3. Persist and query data in PostgreSQL via the latest Tortoise-ORM with asyncio-optimized connection pooling.\n4. Push real-time notifications over WebSockets using python-socketio latest async server mode.\n5. Summarize user-submitted text on demand using the latest llama-cpp-python library with optional GPU offload.\n6. Cache frequent database queries in Redis using aioredis latest cluster-mode support.\n\nDeliverables:\n- A requirements list specifying exact versions of all third-party libraries.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":74,"modules":["aioredis","aiosqlite","fastapi","pydantic","python-socketio","strawberry-graphql","tortoise-orm"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a real-time collaborative note-taking service that supports user auth, live updates, AI summaries, PDF export, and caching using the latest Python web stack.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST API for user signup\/login with FastAPI (latest) using OAuth2 password flow.\n2. Persist users and notes in PostgreSQL via SQLModel (latest async features).\n3. Create a WebSocket endpoint broadcasting live note edits using the newest websockets library.\n4. Add an endpoint `\/summarize` that calls an external AI summarization API via httpx\u2019s HTTP\/3 client (latest).\n5. Provide a `\/export\/{note_id}` endpoint that generates a PDF of the note using Playwright Python (latest).\n6. Cache active JWT tokens in Redis Streams using aioredis v2.x.\n7. Emit structured JSON logs via loguru (latest advanced configuration).\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- A complete, runnable Python 3.11+ codebase under 150 lines, with clear comments indicating where each external library is used.","question_index":9,"modules":["aioredis","asyncpg","bcrypt","fastapi","httpx","loguru","passlib","playwright","python-jose","sqlalchemy","sqlmodel","uvicorn","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":13}
{"model":"gemini-2.5-pro-preview-05-06","question":"You are a senior software engineer tasked with building a minimal Python web service for text ingestion, semantic search, and AI-powered summarization using the latest libraries.\n\nFunctional Requirements:\n1. HTTP API: use the latest Starlite framework (as of 2025-05-06) to expose async endpoints.\n2. Text Storage: on POST \/submit, accept JSON {\u201ctext\u201d: \u2026}, store original text and metadata in Redis via the latest redis-om Python client with async HashModel.\n3. Embedding: upon submission, generate a vector embedding using the latest ChromaDB Python client\u2019s async API and upsert into its vector store.\n4. Semantic Search: implement GET \/search?query=\u2026 that computes a query embedding via ChromaDB and returns top 3 matching texts from Redis.\n5. Summarization: implement GET \/summarize\/{id} to retrieve stored text by ID and produce a concise summary using the latest llama-cpp-python streaming API.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries (as of 2025-05-06).  \n\u2022 A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments indicating where and how each external library is used.","question_index":45,"modules":["chromadb","litestar","llama-cpp-python","pydantic","redis-om"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Senior software engineer tasked with developing a real-time GraphQL web service that validates incoming event data, persists it to PostgreSQL, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI web framework) as of 2025-05-06 to implement an async HTTP POST endpoint at `\/events`.\n2. Validate incoming JSON payloads with Pydantic v2 (latest) models, including at least one custom field validator.\n3. Persist validated events to PostgreSQL using the latest Prisma Python client (type-safe generated models and migrations).\n4. Expose a GraphQL API with the latest Strawberry GraphQL library, supporting queries for event history and subscriptions for real-time updates.\n5. Configure WebSocket support in Starlite to broadcast new events to all GraphQL subscription clients.\n\nDeliverables:\n- A brief recap of the functional requirements.\n- Complete, runnable Python 3.11+ code under 150 lines that uses the latest versions of each library as of 2025-05-06, with clear comments marking where each external library is imported and utilized.","question_index":22,"modules":["litestar","pydantic","strawberry"],"modulenotfound":[],"modulenotfound_count":0,"module_count":3}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Design a minimal real-time chat service combining HTTP endpoints, data validation, async ORM persistence, WebSocket broadcasting, and JWT security using the latest Python libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Starlite to expose POST \/register and GET \/messages endpoints, leveraging its advanced dependency injection system.\n2. Define and serialize request\/response models with the latest Pydantic v2, employing its new `model_serializer` API.\n3. Persist User and Message models to SQLite asynchronously with the latest Ormar ORM, utilizing its built-in async migration feature.\n4. Implement a `\/ws` WebSocket route using the latest websockets library for real-time message broadcasting to connected clients.\n5. Secure both HTTP routes and WebSocket connections with JWT authentication via the latest PyJWT release.\n\nDeliverables:\n- A concise mapping of each numbered requirement to the specific library and feature used.\n- Complete, runnable Python 3.11+ code under 150 lines total, with clear comments indicating where each external library is imported and applied.","question_index":21,"modules":["PyJWT","ormar","pydantic","starlite","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are building a Python microservice that accepts user-uploaded text files, asynchronously generates summaries with a local LLM, and streams live progress to clients via WebSocket.\n\nFunctional Requirements:\n1. Implement a FastAPI app (use the latest FastAPI as of 2025-05-06) with an `\/upload` POST endpoint that accepts plain-text file uploads.  \n2. Upon upload, enqueue a background job using arq (latest version) to process the file asynchronously.  \n3. In the arq worker, load a local LLM via llama-cpp-python (latest version) using its new GPU offloading API to generate a summary.  \n4. Configure python-socketio (latest version) as an ASGI middleware in FastAPI to broadcast progress and final summary events to connected WebSocket clients.\n\nDeliverables:\n- A bullet list of the chosen third-party libraries with their exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":46,"modules":["arq","fastapi","python-socketio","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build an end-to-end real-time customer feedback summarization service with live dashboard updates.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled JSON ingest endpoint using Litestar (latest as of 2025-05-06) with async support.\n2. Define a feedback model with Pydantic v2 and persist submissions into SQLite asynchronously via SQLModel (latest).\n3. On each new submission, call the OpenAI Python SDK (latest) asynchronously to generate a brief summary.\n4. Cache each summary in Redis via redis-om (latest) with a 1-hour TTL.\n5. Serve a live dashboard using Reflex (latest) that connects over WebSockets to display incoming summaries in real time.\n6. Use HTTPX (latest) with HTTP\/3 support for any external HTTP calls.\n\nDeliverables:\n\u2022 requirements.txt listing all dependencies pinned to the latest versions as of 2025-05-06  \n\u2022 A single Python 3.11+ script (under 150 lines) that implements the above, with clear comments indicating where and why each external library is used","question_index":61,"modules":["aiosqlite","httpx","litestar","openai","pydantic","python-dotenv","redis-om","reflex","sqlalchemy","sqlmodel"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Develop a Python 3.11+ web application that provides secure collaborative document management with GraphQL, real-time WebSocket updates, AI-powered summarization, and on-demand PDF export.\n\nFunctional Requirements:\n1. Implement a GraphQL API for document queries and mutations using the latest \u201cariadne\u201d library with subscription support.\n2. Enable OAuth2 login with passwordless WebAuthn using the latest \u201cauthlib\u201d and \u201cfido2\u201d libraries.\n3. Provide real-time document update notifications over WebSocket leveraging FastAPI\u2019s built-in WebSocket support.\n4. Schedule asynchronous AI summarization tasks using the latest \u201cllama-cpp-python\u201d library and \u201cAPScheduler\u201d for background job management.\n5. Generate PDF exports of documents on demand using the latest \u201cplaywright\u201d headless browser library in Python.\n6. Persist documents and user metadata in PostgreSQL via the latest \u201ctortoise-orm\u201d with native async and Pydantic v2 model support.\n\nDeliverables:\n- A brief requirements list naming each third-party library and its version.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used. Use the latest library versions available as of 2025-05-06.","question_index":88,"modules":["APScheduler","ariadne","authlib","fastapi","llama-cpp-python","playwright","pydantic","python-fido2","tortoise-orm","uvicorn"],"modulenotfound":["python-fido2"],"modulenotfound_count":1,"module_count":10}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are designing a Python microservice for ingesting user text, storing embeddings, and providing real-time, rate-limited on-device summaries via WebSockets.\n\nFunctional Requirements:\n1. HTTP POST \/ingest  \n   - Validate payload (id: str, text: str) using Pydantic v2 root validators  \n   - Compute embeddings on-device with llama-cpp-python and store (id, embedding, text) in ChromaDB  \n2. HTTP GET \/query  \n   - Accept query text, validate with Pydantic v2  \n   - Compute embedding via llama-cpp-python, retrieve top-5 nearest documents from ChromaDB, return metadata  \n3. WebSocket \/summarize  \n   - Client sends document id  \n   - Fetch text from ChromaDB metadata  \n   - Stream back a summary using llama-cpp-python streaming API  \n   - Enforce token-per-second rate limiting via aiolimiter  \n4. Lifespan events  \n   - On startup, initialize ChromaDB client, Llama model, and aiolimiter  \n   - On shutdown, cleanly close any resources  \n\nDeliverables:\n- requirements.txt listing the latest available versions as of today of:\n  \u2022 starlite  \n  \u2022 pydantic v2  \n  \u2022 chromadb  \n  \u2022 llama-cpp-python  \n  \u2022 aiolimiter  \n- main.py: a single, runnable Python 3.11+ script (under 150 lines) implementing all above, with clear comments marking where each external library is used.","question_index":8,"modules":["starlite","pydantic","chromadb","llama-cpp-python","aiolimiter","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a serverless Python application enabling real-time geolocated chat with sentiment analysis, dynamic map visualization, and PDF session reporting.\n\nFunctional Requirements:\n1. Real\u2010time WebSocket chat using the latest Litestar (released in the last 12 months) with asynchronous message persistence via a modern async ORM.\n2. Interactive geospatial mapping of incoming messages on the client side using the latest Python geospatial visualization library released in the last 12 months.\n3. On-the-fly sentiment analysis of each message using the latest Python sentiment analysis library released in the last 12 months.\n4. PDF generation of the entire chat session\u2014including message timeline, geolocation markers, and sentiment scores\u2014using the latest Python PDF library released in the last 12 months.\n5. Deployment configuration for AWS Lambda using the latest Serverless Framework plugin and AWS Lambda Powertools (all released in the last 12 months).\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":97,"modules":["litestar","tortoise-orm","aiosqlite","vaderSentiment-py","fpdf2","aws-lambda-powertools","mangum"],"modulenotfound":["vaderSentiment-py"],"modulenotfound_count":1,"module_count":7}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are a senior software engineer tasked with building a real-time event analytics dashboard that ingests events, stores them, generates AI-driven summaries, caches counts, and streams updates to authenticated clients with full observability.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to implement a POST \/events REST endpoint for event ingestion.\n2. Validate incoming JSON payloads with the latest Pydantic v2 models.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise-ORM.\n4. Cache aggregated event counts in Redis via the latest aioredis.\n5. Generate streaming summaries of event batches with the latest OpenAI Python client.\n6. Stream real-time updates to clients over WebSocket using FastAPI.\n7. Secure all HTTP and WebSocket endpoints with JWT authentication via the latest fastapi-jwt-auth.\n8. Instrument HTTP, database, cache, and AI client calls with the latest OpenTelemetry for distributed tracing.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest library names and versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines integrating all subdomains, with clear comments indicating where each external library is used.","question_index":4,"modules":["aioredis","asyncpg","fastapi","fastapi-jwt-auth","openai","opentelemetry-api","opentelemetry-instrumentation-aioredis","opentelemetry-instrumentation-asyncpg","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-httpx","opentelemetry-sdk","pydantic","python-dotenv","tortoise-orm","uvicorn"],"modulenotfound":["opentelemetry-instrumentation-aioredis"],"modulenotfound_count":1,"module_count":15}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You\u2019re building a high-performance async task management API using the latest libraries in web framework, async ORM, caching, background processing, and observability as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement CRUD REST endpoints for a Task model using the latest starlite.\n2. Persist tasks in Postgres asynchronously with the latest Tortoise-ORM.\n3. Enqueue a background job on task creation using the latest Dramatiq with Redis broker.\n4. Cache GET \/tasks\/{id} responses in Redis via the latest redis.asyncio.\n5. Instrument all HTTP and database calls with OpenTelemetry using the latest opentelemetry-instrumentation-starlite.\n\nDeliverables:\n- A requirements.txt listing each external dependency with explicit versions.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":34,"modules":["asyncpg","dramatiq","litestar","opentelemetry-api","opentelemetry-instrumentation-litestar","opentelemetry-instrumentation-tortoiseorm","opentelemetry-sdk","pydantic","redis","tortoise-orm"],"modulenotfound":["opentelemetry-instrumentation-litestar"],"modulenotfound_count":1,"module_count":10}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario  \nYou are building an AI-powered document search microservice that ingests user uploads, indexes them with a GPU-accelerated vector store, caches recent queries, and exposes HTTP\/3 endpoints for real-time search.  \n\nFunctional Requirements  \n1. HTTP API (Starlite, latest)  \n   1.1 POST \/upload: receive plain-text documents, validate with Pydantic.  \n   1.2 GET \/query?q=\u2026: return top-k results for query.  \n2. Document Ingestion (llama-index, latest)  \n   2.1 Stream tokenized embeddings upon upload.  \n3. Vector Store (ChromaDB, latest)  \n   3.1 Use GPU-backed indexing for fast similarity search.  \n4. Caching Layer (redis-om, latest)  \n   4.1 Cache each query + results with TTL and Pydantic models.  \n\nDeliverables  \n\u2022 A requirements.txt listing the latest versions (as of 2025-05-06) of all third-party libraries, each released within the past 12 months.  \n\u2022 Complete, runnable Python 3.11+ code (\u2264150 lines) implementing all requirements.  \n\u2022 Clear inline comments indicating where and how each external library is used.","question_index":47,"modules":["chromadb","litestar","llama_index","pydantic","redis_om","torch"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are a senior engineer building a high-throughput job processing microservice with a modern async Python stack.  \n\nFunctional Requirements:  \n1. Expose a POST \/jobs endpoint using the latest Starlite (as of 2025-05-06) to accept JSON job submissions.  \n2. Validate incoming JSON payloads with the latest Pydantic v2 models.  \n3. Persist job records to PostgreSQL via the latest prisma-client-py async ORM.  \n4. Enqueue and process jobs in the background using the latest Taskiq queue.  \n5. Expose a GET \/jobs\/{id}\/status endpoint to retrieve current job status.  \n6. Ensure all I\/O is non-blocking and use Python 3.11+ async\/await throughout.  \n\nDeliverables:  \n\u2022 A requirements list specifying the exact latest versions of Starlite, Pydantic v2, prisma-client-py, and Taskiq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ source code under 150 lines.  \n\u2022 Clear inline comments marking where each external library is used.","question_index":99,"modules":["litestar","pydantic","prisma","taskiq"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a secure, real-time AI-powered chat web service.\n\nFunctional Requirements:\n1. Expose an async REST API using the latest FastAPI for user registration and login endpoints.  \n2. Implement JWT authentication with rotating refresh tokens via the latest Authlib.  \n3. Persist users and chat messages in SQLite using the latest SQLModel, leveraging its new async support.  \n4. Enable real-time chat notifications over WebSockets using the latest python-socketio and ASGI.  \n5. Integrate AI chat completions using the latest OpenAI Python client in an async background task.\n\nDeliverables:\n1. A requirements.txt listing exact, up-to-date package versions as of today\u2019s date.  \n2. A single Python 3.11+ file (<150 lines) containing complete, runnable code with clear comments marking where each external library is used.","question_index":91,"modules":["aiosqlite","authlib","fastapi","openai","passlib","pydantic","python-dotenv","python-socketio","sqlmodel","sqlalchemy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are a senior software engineer tasked with building an AI-powered collaborative document annotation API.\n\nFunctional Requirements:\n1. Secure user signup and login with JWT-based authentication using Starlite (latest) and Pydantic v2.\n2. REST endpoint to upload documents and persist metadata in Redis OM Python (latest).\n3. WebSocket route for real-time annotation broadcasting using Starlite\u2019s built-in WebSocket support.\n4. Background summarization of uploaded documents using llama-cpp-python (latest).\n\nDeliverables:\n- A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":1,"modules":["bcrypt","litestar","pydantic","python-jose","redis-om"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are a senior engineer tasked with building a secure, high-throughput microservice for customer order ingestion, persistence, background processing, and caching.\n\nFunctional Requirements:\n1. Use FastAPI (latest as of 2025-05-06) with HTTP\/3 support to implement POST \/orders accepting JSON order data.\n2. Define an asynchronous Order model with SQLModel (latest) and connect to an async database engine (SQLite or PostgreSQL).\n3. Enqueue and process new orders via Arq (latest) background worker.\n4. Implement OAuth2 with JWT issuance and protected endpoints using Authlib (latest).\n5. Cache GET \/orders\/{id} responses in Redis using aiocache (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments marking where each external library is used.","question_index":68,"modules":["fastapi","pydantic","sqlalchemy","sqlmodel","authlib","arq","aiocache","aiosqlite","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build an end-to-end Python web service that accepts data submissions, runs ML inference asynchronously, and pushes results back in real time.\n\nFunctional Requirements:\n1. Implement a POST \/submit endpoint using the latest FastAPI (ASGI, HTTP\/2) to accept JSON payloads.\n2. Validate and persist submissions into PostgreSQL via the latest SQLModel with asynchronous sessions.\n3. Dispatch an asynchronous classification task via the latest Celery (Redis broker) when new data arrives.\n4. In the Celery task, perform inference using the latest Hugging Face Transformers pipeline (e.g., text-classification).\n5. Cache inference results in Redis using the latest aioredis with a 5-minute TTL.\n6. Expose a WebSocket \/ws endpoint (FastAPI) to stream cached or newly computed results to subscribed clients.\n7. Instrument all HTTP requests and Celery tasks with the latest OpenTelemetry SDK for tracing.\n\nDeliverables:\n\u2022 requirements.txt listing \u201cthe latest\u201d versions of FastAPI, SQLModel, Celery, Redis, aioredis, Transformers, and OpenTelemetry as of 2025-05-06.  \n\u2022 A single Python 3.11+ file under 150 lines of runnable code. Include clear comments where each external library is used.","question_index":59,"modules":["aioredis","asyncpg","celery","fastapi","opentelemetry-api","opentelemetry-instrumentation-celery","opentelemetry-instrumentation-fastapi","opentelemetry-sdk","sqlmodel","torch","transformers"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are tasked with building a real-time IoT analytics dashboard featuring anomaly detection, edge caching, vector search, and HTTP\/3 streaming.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of May 6, 2025) with HTTP\/3 and ASGI concurrency for API endpoints.\n2. Ingest IoT telemetry via real-time WebSocket streams using the latest websockets library.\n3. Perform transformer-based anomaly detection using the latest HuggingFace transformers and PyTorch 2.x.\n4. Store and query time-series embeddings in a vector database using the latest Pinecone Python client with HNSW indexing.\n5. Cache recent query results at the edge using the latest aioredis client with TTL invalidation.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the \u201clatest\u201d versions of all third-party libraries as of today.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":67,"modules":["aioredis","fastapi","pinecone-client","python-dotenv","torch","transformers"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a high-performance, AI-powered web dashboard for real-time analytics and user collaboration.\n\nFunctional Requirements:\n1. Web API: Use the latest FastAPI (as of 2025-05-06) with async endpoints and the new dependency-injection system.\n2. Real-Time Collaboration: Implement bidirectional WebSocket chat using python-socketio v5.x+ for live user updates.\n3. AI Integration: Leverage the latest llama-cpp-python (or equivalent) to compute embeddings on incoming messages.\n4. Database Persistence: Store users and message history asynchronously with SQLModel v0.7+ (Pydantic v2) on SQLite.\n5. Caching Layer: Cache AI embedding results in Redis via aioredis v3.x+ to minimize redundant computation.\n6. Security: Protect all endpoints with OAuth2 JWT authentication using Authlib v1.2+.\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all third-party libraries as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, with clear inline comments indicating where each external library is used.","question_index":58,"modules":["aioredis","aiosqlite","fastapi","pydantic","python-jose","python-socketio","sqlmodel","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-pro-preview-05-06","question":"You are a senior software engineer tasked with building a modern image-processing web service end-to-end.\n\nFunctional Requirements:\n1. Implement a REST endpoint in Litestar for image uploads that converts incoming images to AVIF using the latest pillow-avif-plugin.\n2. Asynchronously persist image metadata (filename, upload timestamp, AVIF URL, dimensions) in MongoDB via the latest Beanie ODM.\n3. Index and cache metadata in Redis for fast lookup using the latest Redis-OM Python library.\n4. Expose a GraphQL API via the latest Litestar-GraphQL plugin to query images with filtering, pagination, and type validation.\n5. Broadcast real-time upload notifications over WebSocket connections using Litestar\u2019s built-in WebSocket support.\n\nDeliverables:\n- A concise list of the five functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines, using the latest Litestar, Litestar-GraphQL, Beanie, Redis-OM, and pillow-avif-plugin (as of May 6, 2025), with clear comments indicating where each external library is used.","question_index":89,"modules":["beanie","litestar","motor","Pillow","pillow-avif-plugin","pydantic","redis-om","strawberry-graphql"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-pro-preview-05-06","question":"You are a Senior Software Engineer building a high-throughput, real-time web application that ingests user events, enriches them with AI, persists outcomes, and serves analytics via a modern dashboard.\n\nFunctional Requirements:\n1. Implement JWT-based user authentication using the latest async web framework with schema-first validation (e.g., HTTP\/3 support).\n2. Expose a WebSocket endpoint for real-time event ingestion using the latest WebSocket library with per-message compression.\n3. Perform fully async CRUD on PostgreSQL using the latest ORM with native asyncio support and advanced connection pooling.\n4. Offload heavy computations to background workers via the latest task queue library supporting asyncio and result backends.\n5. Integrate an AI text-classification pipeline using the latest AI inference library with streaming outputs.\n6. Instrument the service with distributed tracing using the latest observability library supporting OpenTelemetry protocols.\n\nDeliverables:\n\u2022 requirements.txt listing the latest available versions of all dependencies (as of May 6, 2025).  \n\u2022 A single Python 3.11+ file under 150 lines that is complete and runnable, with clear comments marking where each external library is used.","question_index":12,"modules":["arq","asyncpg","fastapi","opentelemetry-api","opentelemetry-exporter-stdout","opentelemetry-instrumentation-fastapi","opentelemetry-sdk","passlib","pydantic","python-jose","redis","sqlmodel","uvicorn"],"modulenotfound":["opentelemetry-exporter-stdout"],"modulenotfound_count":1,"module_count":13}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Develop a high-performance asynchronous web service for ingesting user datasets, persisting metadata, launching background processing jobs, and tracing all operations end-to-end.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/upload endpoint using Starlite (latest) with async file streaming and dependency injection.\n2. Define and migrate a PostgreSQL table via Piccolo ORM (latest), storing upload metadata and task IDs, leveraging its GraphQL auto-schema feature.\n3. Enqueue and process tasks with Asynq (latest) in Redis, using priority queues, retry policies, and task result callbacks.\n4. Instrument incoming HTTP requests and background tasks with OpenTelemetry (latest), enabling W3C trace-context propagation across components.\n5. Provide an HTTP GET \/status\/{task_id} endpoint that queries Piccolo ORM for real-time task status.\n\nDeliverables:\n\u2022 The numbered functional requirements list (as above)  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, using the latest versions of all libraries as of 2025-05-06, with clear comments indicating where each external library is used.","question_index":43,"modules":["asynq","litestar","opentelemetry-api","opentelemetry-instrumentation-litestar","opentelemetry-propagators-opentelemetry-tracecontext","opentelemetry-sdk","piccolo","uvicorn"],"modulenotfound":["opentelemetry-instrumentation-litestar","opentelemetry-propagators-opentelemetry-tracecontext"],"modulenotfound_count":2,"module_count":8}
{"model":"gemini-2.5-pro-preview-05-06","question":"As a senior software engineer, you are tasked with building a secure, real-time collaborative whiteboard microservice using cutting-edge Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3 API using the latest FastAPI (as of 2025-05-06) and uvicorn[standard] for low-latency streaming.\n2. Implement OAuth2 authorization with JWTs and refresh-token rotation using the latest Authlib.\n3. Persist whiteboard sessions and user metadata in PostgreSQL via the latest SQLModel with an async engine.\n4. Broadcast drawing events in real time over WebSockets using the latest websockets library with room-based pub\/sub.\n5. Vectorize stroke data into SVG on the fly using the latest Shapely or pyvips for advanced geometry operations.\n6. Upload periodic session snapshots to AWS S3 using the latest aioboto3 with asynchronous multipart uploads.\n\nDeliverables:\n- A requirements.txt listing the latest available versions of all external libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, fully runnable, with clear comments indicating where each third-party library is used.","question_index":25,"modules":["aioboto3","asyncpg","authlib","fastapi","h3","pydantic","python-multipart","shapely","sqlalchemy","sqlmodel","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a real-time sentiment analysis dashboard that fetches trending Twitter topics, analyzes sentiment, stores results, and serves updates via WebSockets.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to create both REST and WebSocket endpoints, leveraging its built-in asyncio and WebSocket broadcast features.\n2. Use the latest HTTPX to asynchronously fetch trending topics from the Twitter API v2 over HTTP\/3.\n3. Use the latest Hugging Face transformers to perform sentiment analysis via a pretrained pipeline.\n4. Use the latest SQLModel to define async SQLite models and persist topics with sentiment scores.\n5. Use the latest Jinja2 to render a minimal HTML dashboard for initial page load before WebSocket updates.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the latest versions of FastAPI, HTTPX, transformers, SQLModel, and Jinja2 as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":36,"modules":["aiosqlite","fastapi","jinja2","sqlmodel","torch","transformers"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"Senior Software Engineer: You are developing an intelligent IoT monitoring dashboard that integrates real-time ingestion, streaming anomaly detection, WebSocket updates, dynamic UI rendering, and cloud infrastructure provisioning.\n\nFunctional Requirements:\n1. Build an ASGI web server using the latest ASGI framework (as of 2025-05-06) with HTTP\/2 and WebSocket endpoints for real-time data streaming.\n2. Connect asynchronously to an MQTT broker using the latest Python MQTT client library (as of 2025-05-06), subscribe to sensor topics, and relay messages to the server.\n3. Implement real-time anomaly detection on incoming sensor data using the latest Python anomaly detection library supporting incremental or deep learning.\n4. Serve a live, interactive front-end auto-generated from Python to JavaScript using the latest Python-to-JS UI framework.\n5. Provision and deploy all components via infrastructure-as-code using the latest Python IaC SDK on a major cloud provider.\n\nDeliverables:\n\u2022 Restate the numbered functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code integrating all five requirements, under 150 lines.  \n\u2022 Clear inline comments indicating where and how each external \u201clatest\u201d library (as of 2025-05-06) is imported and used.","question_index":49,"modules":["nicegui","starlette","asyncio-mqtt","river","pulumi"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a real-time collaborative note-taking web application with authentication, persistence, caching, live updates, and scheduled cleanup.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (as of today) for note creation, retrieval, update, and deletion.\n2. Secure endpoints with JWT authentication using the latest PyJWT.\n3. Persist notes in a relational database via the latest SQLModel ORM, defining Note and User models.\n4. Enable real-time note synchronization between clients using WebSockets with the latest python-socketio.\n5. Cache frequently accessed note metadata in Redis using the latest redis-om to reduce DB load.\n6. Schedule automatic archiving of notes older than 30 days using the latest APScheduler.\n7. Document all endpoints with OpenAPI and include example request\/response schemas.\n\nDeliverables:\n- A numbered list of all third-party libraries (with versions) you\u2019ve chosen.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear comments indicating where each external library is used.","question_index":69,"modules":["APScheduler","PyJWT","fastapi","passlib","pydantic","python-socketio","redis-om","sqlmodel","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a high-performance asynchronous message processing service for real-time sentiment analysis, storage, notification, and search.\n\nFunctional Requirements:\n1. Implement an async HTTP API using the latest FastAPI (as of May 6, 2025) to accept user messages.\n2. Define and persist message models in PostgreSQL via the latest SQLModel ORM.\n3. Schedule and execute sentiment analysis in the background using the latest Dramatiq with RabbitMQ.\n4. After analysis, update the database and push real-time notifications to connected clients over WebSockets.\n5. Index messages and sentiment scores into Elasticsearch using the latest official Python client.\n6. Provide a search endpoint to query stored messages by keyword and sentiment.\n\nDeliverables:\n\u2022 A numbered list of the \u201clatest\u201d third-party libraries (with versions) used for each sub-domain.  \n\u2022 Complete, runnable Python 3.11+ application code under 150 lines, with clear comments indicating where each external library is used.","question_index":48,"modules":["asyncpg","dramatiq","elasticsearch","fastapi","httpx","sqlmodel","sqlalchemy","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a microservice that accepts text input, asynchronously generates summaries, persists data, and provides real-time metrics.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI 3, HTTP\/3 support) to implement POST \/summaries accepting JSON { \"text\": \"\u2026\" }, enqueue a background job via Arq, and return a job ID.\n2. Persist requests and summaries in SQLite asynchronously with the latest SQLModel (Pydantic V2 model support).\n3. Offload summarization jobs to Redis using the latest Arq, invoking the latest LangChain to call OpenAI for a concise summary.\n4. Expose GET \/summaries\/{id} to retrieve stored summaries from the database.\n5. Instrument request counts and job durations with the latest prometheus_client, exposing metrics on GET \/metrics.\n\nDeliverables:\n- A bulleted list of chosen \u201clatest\u201d libraries with exact version numbers.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear inline comments indicating where each external library is used.\n- Self-contained implementation: after installing dependencies, code runs without further setup.","question_index":95,"modules":["aiosqlite","arq","langchain","langchain-openai","litestar","pydantic","prometheus-client","sqlmodel","sqlalchemy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Create a serverless real-time analytics API that ingests WebSocket events, performs on-the-fly NLP summarization, indexes embeddings, exposes a GraphQL endpoint, and deploys to AWS Lambda.\n\nFunctional Requirements:\n1. Use the latest Litestar (as of 2025-05-06) to build an HTTP\/2 GraphQL endpoint with schema-based typing.\n2. Implement a real-time WebSocket consumer using wsproto or litestar-websockets for event ingestion.\n3. Offload text summarization to an asynchronous background task leveraging the latest Transformers library.\n4. Generate embeddings with the newest OpenAI Embeddings API or sentence-transformers and store them in the latest ChromaDB vector store.\n5. Secure all endpoints with passwordless authentication via the latest fastapi-users or equivalent.\n6. Package and deploy the entire application to AWS Lambda using the latest Mangum adapter.\n\nDeliverables:\n- A numbered list of chosen \u201clatest\u201d library versions (as of 2025-05-06) mapping to each requirement.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":77,"modules":["litestar","mangum","strawberry","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Develop a modern, high-performance AI-powered web service for real-time text processing.\n\nFunctional Requirements:\n1. Build a JSON REST API with Litestar using its latest dependency-injection and ASGI lifespan hooks.\n2. Integrate Tortoise ORM (async) to manage a PostgreSQL \u201cjobs\u201d table with migrations.\n3. Use llama-cpp-python to generate text embeddings or responses via a local LLaMA model.\n4. Expose a WebSocket endpoint with python-socketio (asyncio mode) for live client notifications.\n5. Cache recent embeddings in Redis using redis-py\u2019s asyncio client.\n6. Schedule background tasks (e.g., stale job cleanup) leveraging asyncio\u2019s latest task groups.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":60,"modules":["litestar","tortoise-orm","redis","python-socketio","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are a senior software engineer building a real-time social media monitoring application that ingests tweets, analyzes sentiment, classifies images, displays live charts, and deploys as a serverless microservice using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Ingest tweets in real time from Twitter API v2 using the latest HTTPX async streaming client.\n2. Perform NLP sentiment analysis on tweet text with the latest Hugging Face Transformers pipeline.\n3. Classify embedded images using the latest Ultralytics YOLOv8 model.\n4. Render an interactive, live-updating dashboard using the latest Plotly Dash framework.\n5. Scaffold serverless deployment with the latest AWS Lambda Powertools for Python.\n\nDeliverables:\n- A requirements list of all external dependencies.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":98,"modules":["dash","plotly","httpx","transformers","ultralytics","aws-lambda-powertools"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: A knowledge management platform must allow users to upload PDFs, index their content, and perform real-time similarity searches via a high-performance API.\n\nFunctional Requirements:\n1. Implement a PDF upload REST endpoint using the latest unstructured-sdk to extract text.\n2. Use the latest ChromaDB Python client to generate and store embeddings for each document.\n3. Provide a search REST endpoint that queries ChromaDB for top-k similar documents given a text query.\n4. Create a real-time WebSocket over HTTP\/3 endpoint using the latest aioquic to stream incremental search results.\n5. Assemble the application as an asynchronous service using the latest Starlite web framework.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":6,"modules":["chromadb","litestar","python-multipart","sentence-transformers","unstructured","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: As a senior software engineer, build an asyncio-based Python microservice showcasing HTTP\/3 REST, GraphQL, async ORM, real-time WebSockets, and scheduled tasks using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST endpoint at `\/api\/items` using FastAPI (latest) and Hypercorn (latest) with HTTP\/3 support.\n2. Expose a GraphQL API at `\/graphql` using Strawberry (latest) with asynchronous resolvers fetching from the database.\n3. Integrate PostgreSQL via Tortoise ORM (latest), define an `Item` model, and apply programmatic migrations on startup.\n4. Provide WebSocket notifications at `\/ws\/updates` using python-socketio (latest), broadcasting item creation and deletion events.\n5. Schedule an hourly cleanup job with APScheduler (latest) in asyncio mode to remove `Item` records older than 7 days.\n\nDeliverables:\n- A `requirements.txt` listing the latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is imported and utilized.","question_index":66,"modules":["aiosqlite","apscheduler","fastapi","pydantic","python-socketio","strawberry-graphql","tortoise-orm"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Develop a real-time AI-driven sentiment analysis web service that ingests text, processes it via an on-device LLM, persists results, and streams live analytics to clients.\n\nFunctional Requirements:\n1. Expose RESTful and WebSocket endpoints using the latest Litestar Python framework (released within the last 12 months).\n2. Perform local LLM inference for sentiment analysis with the latest llama-cpp-python library (released within the last 12 months).\n3. Asynchronously persist raw text and sentiment scores in MongoDB using the latest async MongoDB driver (released within the last 12 months) with code-first schema.\n4. Offload sentiment analysis to a distributed background worker using the latest Dramatiq (or equivalent) async task queue released in the last 12 months.\n5. Broadcast real-time sentiment updates to connected clients over WebSockets.\n6. Render an interactive dashboard in server-side templates with the latest Plotly Python library (released within the last 12 months).\n7. Instrument the entire application with distributed tracing via the latest OpenTelemetry Python SDK (released within the last 12 months).\n\nDeliverables:\n- A requirements list specifying each third-party library and its exact latest version as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines total.\n- Clear inline comments indicating where and how each external library is used.","question_index":5,"modules":["dramatiq","jinja2","litestar","motor","opentelemetry-api","opentelemetry-instrumentation-dramatiq","opentelemetry-instrumentation-litestar","opentelemetry-sdk","plotly","pydantic","uvicorn"],"modulenotfound":["opentelemetry-instrumentation-dramatiq","opentelemetry-instrumentation-litestar"],"modulenotfound_count":2,"module_count":11}
{"model":"gemini-2.5-pro-preview-05-06","question":"You are a senior software engineer tasked with building a real-time analytics dashboard with AI-driven anomaly alerts using the latest Python libraries as of 2025-05-06.  \n\nFunctional Requirements:  \n1. Implement asynchronous REST API endpoints using the latest FastAPI.  \n2. Add WebSocket-based real-time alert streaming using the latest websockets.  \n3. Persist incoming metrics in PostgreSQL via async ORM operations using the latest SQLModel.  \n4. Cache expensive queries in Redis for high throughput using the latest aioredis.  \n5. Perform streaming anomaly detection on incoming metrics using the latest river library.  \n6. Enable file uploads of detailed reports to S3-compatible storage via the latest minio SDK.  \n\nDeliverables:  \n\u2022 A numbered list of required Python libraries (with versions as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":20,"modules":["aioredis","fastapi","minio","psycopg","river","sqlmodel","uvicorn","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-pro-preview-05-06","question":"Build a real-time analytics microservice that lets authenticated users ingest sales data, stores it in PostgreSQL, streams live updates to connected clients, and generates PDF reports with embedded charts.\n\n1. Implement OAuth2 JWT authentication using the latest fastapi-jwt-auth as of May 6, 2025.  \n2. Create async CRUD endpoints for sales records with SQLModel and asyncpg.  \n3. Broadcast new sales events over WebSocket using python-socketio (latest).  \n4. Generate interactive charts in HTML via Plotly (latest) and convert them to PDF using WeasyPrint (latest).  \n5. Expose generated PDFs via an endpoint, serving files asynchronously with aiofiles.  \n\nDeliverables:  \n- A numbered confirmation of the functional requirements above.  \n- A single Python 3.11+ file under 150 lines\u2014complete, runnable code\u2014with clear comments indicating where each external library is used. Use the latest versions of all third-party libraries as of today\u2019s date.","question_index":26,"modules":["aiofiles","asyncpg","fastapi","fastapi-jwt-auth","pydantic","python-socketio","sqlmodel","uvicorn","weasyprint"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are tasked with building a real-time collaborative task management web service for enterprise teams.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to expose HTTP endpoints and WebSocket routes, leveraging its new dependency injection and WebSocket lifetime management.\n2. Configure an async PostgreSQL database using the latest Tortoise-ORM with its auto-migration CLI and Pydantic model integration.\n3. Broadcast real-time task updates to connected clients via the latest python-socketio in async mode.\n4. Implement OAuth2 authentication with JWT access and refresh tokens using the latest Authlib, including token rotation.\n5. Cache sessions and publish update events using the latest aioredis async Redis client.\n6. Schedule and execute background jobs (e.g., daily summaries) with the latest APScheduler async support.\n7. Instrument request tracing and logging using the latest OpenTelemetry Python SDK, exporting to console.\n8. Auto-generate interactive API docs using FastAPI\u2019s built-in Swagger UI and Redoc.\n\nDeliverables:\n\u2022 A bullet list of selected libraries (with version numbers).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":15,"modules":["apscheduler","fastapi","opentelemetry-api","opentelemetry-instrumentation-fastapi","opentelemetry-sdk","passlib","pydantic","python-jose","python-socketio","redis","tortoise-orm"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are building a high-performance Python backend that ingests user articles, generates AI-powered summaries, stores and caches results, and exposes them via REST, GraphQL, and real-time channels with background processing.\n\nFunctional Requirements:\n1. Accept article submissions over a REST API using FastAPI and validate payloads with Pydantic v2.  \n2. Offload summarization to a background worker with Dramatiq using Redis as a broker.  \n3. Summarize articles via the latest OpenAI Python SDK and cache summaries with aiocache.  \n4. Persist articles and summaries in SQLite using SQLModel\u2019s async ORM features.  \n5. Expose stored summaries through a GraphQL endpoint implemented with Strawberry GraphQL.  \n6. Broadcast new summary notifications to connected clients over WebSockets with python-socketio.  \n\nDeliverables:\n\u2022 List of the latest third-party libraries (as of today) and their install commands.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":63,"modules":["uvicorn","fastapi","pydantic","dramatiq","redis","aiocache","sqlmodel","aiosqlite","strawberry","socketio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Develop an internal Python microservice where authenticated users can submit text to be summarized by an LLM and receive real-time notifications via WebSockets.\n\nFunctional Requirements:\n1. Implement an asynchronous REST API using the latest Starlite framework (2025-05-06) leveraging its dependency injection and lifespan event system.\n2. Integrate OAuth2 Authorization Code Flow for user authentication with Authlib\u2019s async client libraries released in the last 12 months.\n3. Persist users, submissions, and summaries in PostgreSQL using the latest Piccolo ORM with async schema migrations and connection pooling.\n4. Summarize submitted text by calling OpenAI\u2019s GPT-4 Turbo via the newest openai Python SDK and its function-calling feature.\n5. Broadcast summary completion events over WebSockets using a modern Python websockets library (with HTTP\/2 support) released in the past year.\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":82,"modules":["asyncpg","authlib","httpx","litestar","openai","piccolo","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-pro-preview-05-06","question":"You are tasked with building a real-time analytics web dashboard that ingests streaming data, processes it, summarizes it with AI, secures user access, and visualizes results dynamically using the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Implement a REST API in Python 3.11+ with FastAPI (HTTP\/3 support via Hypercorn) to accept JSON event streams.\n2. Broadcast processed insights in real time over WebSocket using python-socketio.\n3. Buffer incoming events and generate streaming AI summaries using the latest LangChain and OpenAI Python SDK.\n4. Perform real-time aggregation and lazy analytics on events with Polars.\n5. Produce dynamic Plotly charts of key metrics and serve them as JSON for front-end rendering.\n6. Secure all endpoints and WebSocket connections with JWT-based auth via fastapi-users.\n\nDeliverables:\n\u2022 A numbered list of the above functional requirements.  \n\u2022 A single Python 3.11+ file (under 150 lines) that fulfills all requirements, complete and runnable. Include clear comments indicating where and how each external library is used.","question_index":84,"modules":["fastapi","fastapi-users","plotly","polars","pydantic","python-socketio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are a senior software engineer tasked with building a next-generation real-time collaborative data analytics dashboard. Use the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a FastAPI server with a WebSocket endpoint for live data updates using the latest fastapi[full].\n2. Expose a type-safe GraphQL API at `\/graphql` using Strawberry with async schema stitching.\n3. Integrate AI-driven insights by generating embeddings on incoming data with llama-cpp-python.\n4. Perform vector similarity search over embeddings via the Weaviate Python client\u2019s async API.\n5. Secure all endpoints with passwordless authentication using py_webauthn for WebAuthn flows.\n6. Containerize the application using docker-py to build images and dynamically generate a docker-compose config.\n7. Instrument the app with async Prometheus metrics using prometheus-client.\n\nDeliverables:\n- A list of required libraries (with versions) using the latest releases as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":72,"modules":["fastapi","starlette-prometheus","strawberry-graphql"],"modulenotfound":[],"modulenotfound_count":0,"module_count":3}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Develop a real-time collaborative note-taking web application with AI-powered summarization.\n\nFunctional Requirements:\n1. Implement user authentication via OAuth2 password flow using the latest FastAPI features for secure login.  \n2. Expose a WebSocket endpoint for real-time CRDT-based document state synchronization using the newest FastAPI WebSocket enhancements.  \n3. Persist users and documents in PostgreSQL via the latest Piccolo ORM async API with JSONB indexing for rich querying.  \n4. Cache live presence and session metadata in Redis using aioredis with RedisJSON support for low-latency reads\/writes.  \n5. Summarize document edits on demand using the latest LangChain library and ChatOpenAI (GPT-4) integration.  \n6. Serve a minimal HTML frontend via Jinja2 templates that connects to the WebSocket and displays live updates.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of each dependency as of 2025-05-06.  \n\u2022 main.py: Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":52,"modules":["fastapi","pydantic","piccolo","aioredis","langchain-openai","langchain","python-jose","passlib","python-multipart","psycopg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Senior Software Engineer, you are tasked with building a prototype real-time collaborative note-taking service using only the latest Python libraries released in the last 12 months.\n\nFunctional Requirements:\n1. Develop an asynchronous REST API for note CRUD operations using the latest Litestar framework.\n2. Persist notes in a SQL database via a newly released async ORM library.\n3. Cache frequent note queries in Redis with a recently published async Redis client.\n4. Enable real-time collaboration via WebSockets using a modern Python WebSocket library launched within the last year.\n5. Expose an AI-powered summarization endpoint using the latest OpenAI Python client.\n6. Render an interactive HTML dashboard showing active collaborators with a new Python-to-JS visualization tool.\n7. Instrument distributed tracing and structured logging using a cutting-edge observability library released in the past 12 months.\n\nDeliverables:\n- requirements.txt listing only libraries created in the last 12 months with exact versions.\n- A single Python 3.11+ file under 150 lines containing the complete, runnable application, with clear comments indicating where each external library is used.","question_index":41,"modules":["aiosqlite","litestar","nicegui","openai","pydantic","python-dotenv","redis","sqlalchemy","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario:\nCreate an AI-powered article summarization service that accepts user-submitted text, stores it, generates summaries asynchronously via an AI API, caches results, and streams them back to clients in real time.\n\nFunctional Requirements:\n1. Implement a REST API using FastAPI (latest) with endpoints POST \/articles to submit text and GET \/stream\/{article_id} for server-sent events.\n2. Persist submissions in a database using SQLModel (latest) with an async SQLite or PostgreSQL engine.\n3. Orchestrate a background job queue using Celery (latest) with Redis as broker to process newly submitted articles.\n4. Perform summarization using the OpenAI Python SDK (latest) and its chat completion endpoint.\n5. Cache generated summaries in Redis via aioredis (latest) with a configurable TTL.\n6. Stream summaries to connected clients in real time using server-sent events via sse-starlette (latest).\n\nDeliverables:\n\u2022 A list of all third-party libraries (with versions pinned to the latest as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":64,"modules":["fastapi","pydantic","sqlalchemy","sqlmodel","celery","aioredis","openai","sse-starlette","aiosqlite","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a lightweight feedback API that records user comments, analyzes sentiment in real time, stores results, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (as of 2025-05-06) to implement asynchronous REST endpoints for submitting feedback and querying stored analytics.\n2. Use the latest SQLModel to define async ORM models and persist incoming feedback in an SQLite database.\n3. Use the latest PyWebSocketX to broadcast each new sentiment result over an ASGI WebSocket endpoint.\n4. Use the latest VulcanMind inference library to perform on-the-fly sentiment analysis of submitted feedback.\n5. Implement structured logging and error handling throughout the request, inference, database, and WebSocket flows.\n\nDeliverables:\n- A requirements list with exact version pins for Starlite, SQLModel, PyWebSocketX, and VulcanMind (latest as of 2025-05-06).\n- Complete, runnable Python 3.11+ code (\u2264150 lines) that integrates all four libraries with clear comments marking where each is used.","question_index":83,"modules":["aiosqlite","litestar","sqlmodel"],"modulenotfound":[],"modulenotfound_count":0,"module_count":3}
{"model":"gemini-2.5-pro-preview-05-06","question":"You are a senior software engineer building a high-performance, real-time analytics web service for IoT sensor data.  \n\nFunctional Requirements:  \n1. Expose HTTP REST ingest endpoints using the latest Nebula framework (released 2024-06) with its async dependency-injection.  \n2. Persist incoming readings to PostgreSQL via StarDB ORM (v1.0+, released 2024-08), leveraging async transactions and auto-migration.  \n3. Secure all endpoints with OAuth2 PKCE flows using AuthFusion (released 2024-12).  \n4. Broadcast processed data updates to dashboard clients via WaveSocket (released 2024-10) server-clustering.  \n5. Schedule hourly aggregation jobs using ChronoTask (released 2024-11) cron-style scheduler.  \n\nDeliverables:  \n\u2022 A requirements.txt listing the latest versions of all libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":92,"modules":["fastapi","pydantic","sqlalchemy","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are a senior software engineer building a high-throughput chat summarization microservice in Python 3.11+ using only the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Real-time WebSocket ingestion: implement a `\/ws` endpoint using fastapi-socketio (latest) with session storage in Redis via `redis.asyncio`.\n2. AI summarization: enqueue incoming messages in Arq (latest) and process them with the OpenAI Python SDK (latest) calling GPT-4 Turbo for summaries.\n3. Async data persistence: store raw messages and summaries in PostgreSQL using SQLModel + encode\/databases with the asyncpg driver.\n4. Dynamic admin dashboard: expose `\/admin` rendering Jinja2 templates enhanced with HTMX via fastapi-htmx (latest).\n5. Email alerts: send summary notifications asynchronously using FastAPI-Mail (latest).\n6. Containerization: provide a multi-stage Dockerfile based on `python:3.11-slim`.\n\nDeliverables:\n- A `requirements.txt` listing the exact versions of all third-party libraries (as of May 6, 2025).\n- A single `main.py` (under 150 lines) containing complete, runnable Python 3.11+ code with clear comments indicating where and how each external library is used.","question_index":23,"modules":["arq","asyncpg","databases","fastapi","fastapi-htmx","fastapi-mail","fastapi-socketio","jinja2","openai","python-dotenv","redis","sqlmodel"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a compact Python microservice that ingests text and images, leverages AI summarization, real-time streams results, and stores everything in a modern database.\n\nFunctional Requirements:\n1. Use the latest Litestar (async web framework released Dec 2023) to define:\n   a. POST \/articles to receive JSON {\u201ctitle\u201d:\u2026, \u201ccontent\u201d:\u2026}.\n   b. WebSocket endpoint \/ws\/summary for streaming summaries.\n2. Connect asynchronously to PostgreSQL via the latest Prisma Python client (released Jul 2023), defining models for Article and Image.\n3. Summarize incoming content using the latest llama-cpp-python (released Sep 2023), utilizing its streaming interface for token-by-token output.\n4. Implement \/upload to accept multipart\/form-data image files, generate HEIF thumbnails with the latest pillow-heif (released Oct 2023), and persist file paths.\n5. Ensure all endpoints use async\/await, proper error handling, and pydantic-based validation.\n\nDeliverables:\n\u2013 A requirements.txt listing the latest library versions as of 2025-05-06.\n\u2013 A single, runnable Python 3.11+ script under 150 lines, with clear comments indicating where each external library is used.","question_index":16,"modules":["Pillow","litestar","llama-cpp-python","pillow-heif","prisma","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"You are tasked with building a prototype web service for authenticated semantic search with real-time visual feedback.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) server with:\n   a. an OAuth2 login flow using PyOIDC (latest) against an external Identity Provider  \n   b. a POST \/query endpoint to accept natural-language queries  \n   c. a WebSocket \/ws endpoint to stream real-time results  \n2. Upon authentication, embed incoming queries using sentence-transformers (latest) and perform vector similarity search in ChromaDB (latest).  \n3. As results arrive, push them incrementally over the WebSocket to clients.  \n4. Generate a Plotly (latest) bar chart of top-k result scores and serve it as an HTML snippet.  \n5. Ensure all components run under Python 3.11+, integrate only \u201cthe latest\u201d library releases as of 2025-05-06, and keep implementation concise.  \n\nDeliverables:\n\u2022 A list of chosen libraries with exact versions (the \u201crequirements\u201d).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":42,"modules":["authlib","chromadb","fastapi","plotly","pydantic","sentence-transformers","starlette","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are building a high-throughput, real-time analytics web application for processing IoT sensor streams using the latest Python async frameworks and libraries.\n\nFunctional Requirements:\n1. Ingest JSON sensor events via a REST API built on FastAPI (use its latest HTTP\/3 support).\n2. Broadcast processed metrics to connected clients over WebSockets using python-socketio (asyncio).\n3. Offload CPU-intensive aggregation to background workers with Arq (latest Redis-backed queue).\n4. Persist time-series summaries in PostgreSQL via SQLModel (Pydantic v2-powered ORM).\n5. Cache hot query results in Redis using aioredis for sub-millisecond lookups.\n6. Expose tracing and Prometheus metrics using OpenTelemetry and prometheus-client.\n\nDeliverables:\n- A bullet list of the latest library dependencies as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments indicating where each external library is used.","question_index":40,"modules":["aioredis","arq","fastapi","opentelemetry-api","opentelemetry-exporter-prometheus","opentelemetry-instrumentation-fastapi","opentelemetry-sdk","prometheus-client","pydantic","python-socketio","sqlmodel"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"gemini-2.5-pro-preview-05-06","question":"You are a senior software engineer tasked with building a Python web application that integrates user authentication, real-time notifications, AI-based recommendations, and payment processing.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (2025-05-06) and Python 3.11 features (e.g. match-case).\n2. Secure all endpoints with OAuth2 JWT authentication via the latest fastapi-users, including email verification flows.\n3. Provide real-time notifications over WebSockets using the latest python-socketio async server.\n4. Handle one-time charges using the latest Stripe Python SDK via the Payment Intents API.\n5. Generate AI-driven product recommendations by invoking gpt-4-turbo through the latest openai library.\n6. Emit structured, context-rich logs for each operation using the latest structlog.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06  \n\u2022 Complete, runnable Python 3.11+ code (under 150 lines) with clear comments marking where each external library is used","question_index":96,"modules":["fastapi","fastapi-users","openai","passlib","pydantic","python-socketio","stripe","structlog"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: As a senior software engineer, design a high-performance microservice for real-time document collaboration with secure access, persistence, and cloud integration.  \n\nFunctional Requirements:  \n1. Implement an asynchronous JSON REST API using the latest Python web framework available as of 2025-05-06, supporting pagination and input validation.  \n2. Secure all endpoints with OAuth2 PKCE and JWT using the latest authentication library (released within the last 12 months).  \n3. Enable bidirectional real-time collaboration events over WebSocket using the latest WebSocket library (released within the last 12 months).  \n4. Persist document data in an async NoSQL database (e.g., MongoDB) using the newest driver supporting transactions and change streams.  \n5. Upload and serve document attachments to an S3-compatible object storage using the latest SDK with multipart upload support.  \n6. Cache frequently accessed documents in Redis with TTL and automatic invalidation using the newest Redis client library.  \n\nDeliverables:  \n- A requirements.txt listing \u201cthe latest\u201d versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":80,"modules":["aiobotocore","fastapi","motor","pydantic","python-jose","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a high-performance Python microservice that combines cutting-edge web, real-time, AI and observability features into a single ASGI application.\n\nFunctional Requirements:\n1. HTTP\/3 ASGI server with automatic TLS and QUIC support using the latest HTTP\/3 library released within the past 12 months.  \n2. Real-time WebSocket endpoint that streams JSON events with zero-copy backpressure control using the newest streaming library created in the last year.  \n3. Semantic search pipeline: accept text uploads, generate embeddings, and perform vector similarity queries via the latest vector-store client released within 12 months.  \n4. On-device LLM inference endpoint that consumes streamed text segments and returns summaries using the most recent quantized model library published this year.  \n5. Distributed tracing and metrics export using the newest OpenTelemetry OTLP exporter released within the past 12 months.\n\nDeliverables:\n- A requirements list (requirements.txt) specifying the exact, latest versions of each third-party library as of 2025-05-06.  \n- A single Python 3.11+ script under 150 lines that implements all features above, with clear comments marking where each external library is used.","question_index":51,"modules":["chromadb","ctransformers","fastapi","hypercorn","opentelemetry-api","opentelemetry-exporter-otlp-proto-http","opentelemetry-instrumentation-fastapi","opentelemetry-sdk","pydantic","python-multipart","sentence-transformers","trustme","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":13}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario  \nDevelop an end-to-end Python web service that provides passwordless WebAuthn authentication, stores and summarizes text inputs via OpenAI, broadcasts real-time summary events, and serves interactive charts.\n\nFunctional Requirements  \n1. Implement passwordless registration and login using the latest \u201cwebauthn\u201d library (FIDO2) with Litestar middleware.  \n2. Create REST endpoints under Litestar (latest version) for submitting text to summarize and retrieving stored summaries.  \n3. Use the latest SQLModel with an async PostgreSQL engine to persist input texts, summaries, timestamps, and user IDs.  \n4. Invoke the latest \u201copenai\u201d Python client to perform GPT-4 Turbo summarization with function-calling.  \n5. Serialize and broadcast new summaries over WebSockets using the latest \u201cmsgspec\u201d for high-performance JSON.  \n6. Serve an HTML page at \u201c\/charts\u201d that embeds an interactive Plotly chart of summary lengths over time.\n\nDeliverables  \n\u2022 A numbered list of all external libraries (with exact latest versions as of 2025-05-06) and their roles.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":13,"modules":["asyncpg","litestar","msgspec","openai","sqlmodel","uvicorn","webauthn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: As a senior software engineer, you must develop a real-time analytics dashboard backend that requires user authentication, data ingestion, live updates, background enrichment via an external API, and a web-based dashboard.\n\nFunctional Requirements:\n1. Implement user registration and login endpoints using JWT authentication with asymmetric keys via FastAPI-JWT-Auth (latest).\n2. Create a POST \/data endpoint to ingest JSON payloads and store them asynchronously in SQLite using SQLModel (latest).\n3. Broadcast each new record to connected WebSocket clients at \/ws using the latest websockets library, leveraging async broadcast.\n4. Schedule a background job every minute with arq (latest) to enrich stored records by streaming JSON from a third-party API via httpx (latest).\n5. Serve a simple HTML dashboard at GET \/dashboard that opens a WebSocket to \/ws and displays incoming records in real time.\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":75,"modules":["aiosqlite","arq","bcrypt","cryptography","fastapi","fastapi-jwt-auth","httpx","passlib","pydantic","sqlmodel","uvicorn","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Design and implement a Python microservice that exposes HTTP endpoints, real-time WebSocket notifications, persistent storage, caching, scheduled tasks, and email alerts using the latest third-party libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use FastAPI (latest) with HTTP\/2 support to expose async REST endpoints for user registration and notification retrieval.\n2. Use SQLModel (latest) as an async ORM for PostgreSQL, defining typed models for users and notifications.\n3. Implement WebSocket broadcasting via FastAPI\u2019s WebSocket support to push new notifications to connected clients.\n4. Leverage Aioredis (latest) Redis Streams to cache session state and stream notification events.\n5. Schedule periodic database cleanup and cache eviction using APScheduler (latest) with its asyncio integration.\n6. Send email alerts for critical notifications using Aiosmtplib (latest) with STARTTLS.\n7. Perform external health-check API calls using HTTPX (latest) with async retries and timeouts.\n\nDeliverables:\n- A requirements list naming each library and its latest version as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, including clear comments that indicate where each external library is used.","question_index":44,"modules":["aioredis","aiosmtplib","apscheduler","asyncpg","fastapi","httpx","pydantic","sqlmodel","sqlalchemy","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are tasked as a senior software engineer to build an end-to-end collaborative document editing service with real-time updates, async persistence, GraphQL subscriptions, and cloud file storage.\n\nFunctional Requirements:\n1. Use the latest Litestar (first released within the last 12 months) to create an HTTP\/2 web server with WebSocket endpoints for real-time edit streams.\n2. Implement a GraphQL API with Strawberry GraphQL (latest) that supports queries, mutations, and live subscriptions for document change events.\n3. Persist documents in a PostgreSQL database via a fully async ORM of your choice that was first released in the last 12 months and supports automated migrations.\n4. Generate AWS S3 presigned URLs for resumable file attachments using aiobotocore (latest) with async upload\/download.\n5. Schedule periodic cleanup of stale sessions using an async task scheduler library first released within the last 12 months.\n\nDeliverables:\n- A brief list of the chosen third-party libraries (with exact versions) that meet the \u201cfirst released within the last 12 months\u201d requirement.\n- Complete, runnable Python 3.11+ code (under 150 lines) fulfilling the above requirements, with clear comments indicating where and how each external library is used.","question_index":78,"modules":["aiobotocore","asyncpg","litestar","sqlalchemy","strawberry-graphql","timely"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario:\nYou are a senior software engineer tasked with building a real-time AI summarization microservice.\n\nFunctional Requirements:\n1. Implement an async RESTful POST endpoint `\/summarize` that accepts JSON `{ \"text\": \"<string>\" }` and returns a summary generated via `llama-cpp-python`.\n2. Implement a WebSocket endpoint `\/stream` that streams incremental summary tokens to the client as they are produced.\n3. Persist each original text and its summary in an async SQLite database using `SQLModel`.\n4. Cache recent summaries in Redis for 10 minutes with `aioredis` to avoid redundant computation.\n5. Expose Prometheus-compatible metrics on `\/metrics` (request counts, latency histograms, cache hit\/miss) via `prometheus-client`.\n6. Use a background task to warm up the Llama model at startup and to periodically clean up expired cache entries.\n\nDeliverables:\n- A restatement of the functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Code must use the latest versions of FastAPI, SQLModel, aioredis, llama-cpp-python, and prometheus-client as of 2025-05-06.\n- Include clear comments indicating where each external library is used.","question_index":54,"modules":["aioredis","aiosqlite","fastapi","llama-cpp-python","prometheus-client","pydantic","sqlmodel"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-pro-preview-05-06","question":"You are a senior software engineer tasked with building a high-throughput real-time sentiment analysis microservice.\n\nFunctional Requirements:\n1. Use FastAPI (latest) to expose a POST \/analyze endpoint that accepts JSON text payloads and stores request metadata in PostgreSQL via SQLModel (latest).\n2. Secure all endpoints with OAuth2 JWT authentication using Authlib (latest), leveraging its PKCE support.\n3. Perform real-time sentiment analysis on incoming text using the latest Hugging Face Transformers pipeline with GPU acceleration via PyTorch (latest) and cache results in Redis using aiocache (latest).\n4. Broadcast live sentiment scores to connected clients over WebSockets using python-socketio (latest) and run the server on Uvicorn with HTTP\/3 support.\n5. Asynchronously upload any user-submitted audio snippets to S3-compatible storage via aiobotocore (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all external libraries as of 2025-05-06  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":62,"modules":["aiocache","aiobotocore","aiosqlite","authlib","fastapi","pydantic","python-socketio","redis","sqlalchemy","sqlmodel","torch","transformers","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":13}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a high-performance Python web application combining HTTP\/3 REST, GraphQL, real-time WebSockets, async database operations, data analytics, AI summarization, and distributed tracing.\n\nFunctional Requirements:\n1. HTTP\/3 REST API: use FastAPI (latest as of 2025-05-06) with Uvicorn HTTP\/3 support to serve \/items endpoints.\n2. Async GraphQL API: use Strawberry (latest as of 2025-05-06) to expose a GraphQL schema with DataLoader for batch fetching.\n3. Async DB access: use SQLModel (latest as of 2025-05-06) with an async SQLAlchemy engine to perform PostgreSQL CRUD.\n4. Real-time WebSockets: use websockets library (latest as of 2025-05-06) to broadcast item updates at \/ws.\n5. Data analytics: use Polars (latest as of 2025-05-06) lazy API to compute summary statistics on items.\n6. AI summarization: use OpenAI Python SDK (latest as of 2025-05-06) to generate summaries of item descriptions.\n7. Observability: use OpenTelemetry Python SDK (latest as of 2025-05-06) to instrument all endpoints for distributed tracing.\n\nDeliverables:\n- requirements.txt listing exact latest versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments marking where each external library is used.","question_index":53,"modules":["fastapi","openai","opentelemetry-api","opentelemetry-exporter-otlp-proto-http","opentelemetry-instrumentation-asgi","opentelemetry-sdk","polars","psycopg","python-dotenv","sqlmodel","strawberry-graphql"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are a senior software engineer building a real-time sentiment and OCR-enhanced geospatial chat dashboard for remote field teams.\n\nFunctional Requirements:\n1. Use FastAPI (latest version as of 2025-05-06) to implement an async REST API that serves a minimal HTML client.\n2. Implement bi-directional real-time chat via Socket.IO using python-socketio (latest) on the FastAPI server.\n3. Perform on-the-fly sentiment analysis of text messages using Hugging Face\u2019s transformers pipeline (latest) and broadcast sentiment scores.\n4. Accept image uploads from clients, extract embedded text with EasyOCR (latest), and inject the OCR result into the chat stream.\n5. Plot message geolocation metadata on a Folium (latest) map and serve it as an embeddable HTML snippet.\n\nDeliverables:\n- A requirements list enumerating the exact \u201clatest\u201d third-party libraries and versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":2,"modules":["fastapi","python-socketio","transformers","easyocr","folium"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Develop a Python microservice that supports AI-driven product imagery, real-time stock updates, secure user authentication, and checkout processing.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) app with JWT-based auth using Authlib (latest) and secure password hashing.  \n2. Define async ORM models for Users and Products with SQLModel (latest) on PostgreSQL, including migration setup.  \n3. On product creation, generate an AI image via Diffusers (latest) or Stability-SDK (latest) and expose it via a REST endpoint.  \n4. Provide a WebSocket endpoint for real-time inventory updates using Starlette\u2019s WebSocket support (latest).  \n5. Create a \/checkout POST endpoint integrating Stripe\u2019s Python SDK (latest) with webhook handling to confirm payments.\n\nDeliverables:\n\u2022 A bullet list of the \u201clatest\u201d libraries (with pip install specifiers as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":85,"modules":["authlib","bcrypt","fastapi","passlib","psycopg","pydantic","sqlmodel","stripe"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are developing a high-performance, real-time AI-powered web service for personalized content recommendations.\n\nFunctional Requirements:\n1. Implement a RESTful POST \/recommend endpoint using the latest Python web framework released within the last 12 months, accepting JSON input and returning JSON responses.\n2. Add a WebSocket \/ws endpoint for streaming live recommendation updates using the newest asyncio-based WebSocket library from the past year.\n3. Integrate semantic search over user profiles by connecting to a vector database via its newest Python client library released within the last 12 months.\n4. Schedule periodic model retraining in a background worker using the most recent Python task scheduler released in the last 12 months.\n5. Store and serve user-uploaded media files on cloud object storage using the latest cloud storage client library published within the past 12 months.\n\nDeliverables:\n\u2022 A requirements list (requirements.txt) specifying the exact library names and versions (all released within the last 12 months).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total, with clear comments marking where and why each external library is used.","question_index":71,"modules":["APScheduler","litestar","minio","qdrant-client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Develop a high-performance Python microservice that ingests real-time social media posts via WebSockets, generates AI summaries, stores them in MongoDB Atlas, serves secured HTTP\/3 endpoints, and provides an interactive Streamlit dashboard for sentiment trends.\n\nFunctional Requirements:\n1. Initialize an asynchronous FastAPI app with HTTP\/3 (h2\/h3) and ASGI support.  \n2. Protect all HTTP endpoints with OAuth2 JWT using Authlib\u2019s latest OAuth library.  \n3. Consume a WebSocket stream of JSON posts from wss:\/\/stream.example.com asynchronously.  \n4. Summarize each post using OpenAI\u2019s latest Python SDK leveraging function-calling or embeddings.  \n5. Persist summaries and metadata in MongoDB Atlas via Motor (async driver).  \n6. Expose a secured REST endpoint `\/summaries` supporting pagination and filtering by sentiment.  \n7. Build a Streamlit dashboard that fetches `\/summaries` and visualizes sentiment trends over time with Plotly.\n\nDeliverables:\n- A requirements list naming the latest versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":28,"modules":["fastapi","httpx","motor","openai","pandas","plotly","pydantic","streamlit","uvicorn","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a real-time HTTP\/3-powered AI chat service that streams user messages, generates LLM responses, persists embeddings in a vector DB, and exposes chat history via GraphQL.\n\nFunctional Requirements:\n1. HTTP\/3 streaming: use the latest aioquic (as of 2025-05-06) to handle bidirectional HTTP\/3 connections for chat.\n2. On-device LLM inference: integrate the latest llama-cpp-python for quantized model loading and response generation.\n3. Vector storage: use the latest chromadb Python client to embed each message (via llama-cpp-python\u2019s embed API) and store\/retrieve vectors.\n4. GraphQL API: implement an async GraphQL schema with the latest strawberry to query chat history from ChromaDB.\n5. Async orchestration: ensure end-to-end async I\/O under Python 3.11+, coordinating the QUIC server, LLM, and DB.\n6. Code constraints: keep the complete solution under 150 lines, include clear comments marking where each external library is used.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of aioquic, llama-cpp-python, chromadb, and strawberry (latest as of 2025-05-06)  \n\u2022 A single Python 3.11+ script (<150 lines) with runnable code and comments identifying each external library usage.","question_index":79,"modules":["aioquic","chromadb","llama-cpp-python","strawberry-graphql"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Design and implement a high-performance real-time chat backend with semantic search and background processing.\n\nFunctional Requirements:\n1. Asynchronous REST API: use FastAPI (latest as of 2025-05-06) to expose user and message endpoints.\n2. HTTP\/3 WebSockets: implement real-time chat over WebSockets with Hypercorn\u2019s HTTP\/3 support.\n3. Async ORM persistence: store users and messages in PostgreSQL via SQLModel (async).\n4. Vector semantic search: index and query messages in Qdrant using the latest qdrant-client.\n5. Background sentiment analysis: enqueue and run analysis jobs with ARQ (async Redis-backed queue).\n6. JWT authentication: secure all endpoints with python-jose for JWT issuance and validation.\n7. Redis caching: cache active WebSocket connections or session data via redis.asyncio.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06  \n\u2022 A single Python 3.11+ file under 150 lines that:\n  \u2013 Is fully runnable  \n  \u2013 Shows clear comments where each external library is used  \n  \u2013 Integrates all above requirements into a cohesive application","question_index":11,"modules":["aiosqlite","arq","fastapi","passlib","pydantic","python-jose","qdrant-client","redis","sqlmodel","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: A fintech startup requires a live transaction monitoring service that ingests, processes, stores, and notifies users of suspicious activity in real time.\n\nFunctional Requirements:\n1. Build an HTTP endpoint to receive and stream-respond to JSON transaction payloads using the latest async web framework released in the last 12 months.\n2. Validate and serialize incoming data with the latest data validation library, enforcing strict type constraints and custom validators.\n3. Persist transactions asynchronously in a relational database using the latest async ORM library compatible with Python 3.11+.\n4. Offload CPU-intensive anomaly detection to a background worker queue using the latest task orchestration library.\n5. Broadcast real-time alerts to connected clients via WebSockets using the latest real-time communication library.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of the latest libraries (as of May 6, 2025)  \n\u2022 A single Python 3.11+ file under 150 lines, complete and runnable, with clear comments indicating where each external library is used.","question_index":39,"modules":["aiosqlite","arq","fastapi","pydantic","sqlmodel","uvicorn","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a real-time collaborative document editor service with AI-powered summarization, OAuth2 access control, WebSocket syncing, Redis caching, and NoSQL persistence.\n\nFunctional Requirements:\n1. Use the latest async Python web framework (as of 2025-05-06) to define RESTful CRUD endpoints for documents.\n2. Implement OAuth2 authorization with the latest Python OAuth library released within the last 12 months for secure user login and token issuance.\n3. Establish bi-directional real-time document synchronization over WebSockets using the newest real-time communication library (with built-in pub\/sub).\n4. Integrate the latest AI inference library published in the past year to generate live summaries of incremental document edits.\n5. Employ the latest Redis client library (released within the last year) to cache active sessions and coordinate pub\/sub channels for syncing.\n6. Persist document state and version history in a NoSQL database using the newest async database client released in the last 12 months.\n\nDeliverables:\n1. A numbered list of the chosen \u201clatest\u201d libraries (with a one-line description of their role).\n2. Complete, runnable Python 3.11+ code under 150 lines, including clear comments indicating where and why each external library is used.","question_index":86,"modules":["fastapi","pydantic","redis","beanie","motor"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Develop a user registration microservice over HTTP\/3 that validates input, persists user data, and integrates with an external email verification API.\n\nFunctional Requirements:\n1. Expose a POST \/users endpoint using Starlite (latest) with HTTP\/3 support.  \n2. Define request and response schemas with Pydantic v2 (latest) for data validation.  \n3. Persist user records in PostgreSQL via the latest Prisma Python client, including model definitions and migrations.  \n4. Perform an asynchronous HTTP\/3 request to an external email verification service using httpx (latest), and incorporate its response.  \n5. Return a JSON payload containing the stored user data and email verification status.\n\nDeliverables:\n- A list of required third-party libraries with exact versions (as of today)  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":100,"modules":["httpx","pydantic","litestar","prisma"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: As a senior software engineer, build a modular web application that authenticates users with WebAuthn passkeys, streams real-time stock data, generates AI trading signals, processes payments, and delivers interactive dashboards.\n\nFunctional Requirements:\n1. Implement user authentication via WebAuthn using the latest pywebauthn library (as of 2025-05-06) with hardware security key support.\n2. Ingest and stream real-time stock price data over WebSockets using FastAPI and the latest Starlette HTTP\/2 push features.\n3. Generate AI-driven trading signals leveraging the latest OpenAI Python SDK\u2019s new function-calling interface.\n4. Integrate secure payment processing with the latest Stripe Python SDK supporting Payment Element and webhook handling.\n5. Serve interactive dashboards using the latest Plotly Dash or Reflex library for live data visualization and client-side callbacks.\n6. Add structured logging and Prometheus metrics using structlog and prometheus-client.\n\nDeliverables:\n- A concise mapping of each functional requirement to its implementation approach.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used and using the latest libraries available as of 2025-05-06.","question_index":56,"modules":["fastapi","starlette","py_webauthn","openai","stripe","dash","structlog","prometheus-client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-pro-preview-05-06","question":"You are a senior software engineer tasked with architecting a small, high-performance async web application using only the latest Python libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Litestar framework (with native HTTP\/3 support) to define async REST endpoints for user signup and profile retrieval.  \n2. Integrate the latest Piccolo-ORM to define a PostgreSQL \u201cusers\u201d table and perform async CRUD operations.  \n3. Implement real-time notifications via WebSocket using the latest python-socketio in asyncio mode.  \n4. Schedule a daily summary email job with APScheduler\u2019s new AsyncIOScheduler and send mail via an async SMTP library.  \n5. Fetch external data during signup from a third-party API using HTTPX\u2019s HTTP\/2 client.\n\nDeliverables:\n\u2022 A requirements.txt listing exact package names and latest versions.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":29,"modules":["litestar","piccolo-orm","python-socketio","httpx","apscheduler","aiosmtplib"],"modulenotfound":["piccolo-orm"],"modulenotfound_count":1,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are a senior software engineer building a high-performance real-time IoT analytics backend.\n\nFunctional Requirements:\n1. Expose an asynchronous HTTP API using FastAPI with WebSocket support to ingest and broadcast live sensor readings.\n2. Persist incoming data in PostgreSQL via SQLModel\u2019s async ORM and validate payloads with Pydantic v2 models.\n3. Enforce per-device rate limiting using Redis Streams and aioredis, leveraging RedisJSON for overflow handling.\n4. Secure all endpoints with OAuth2 passwordless login via FastAPI-Users, sending one-time codes.\n5. Schedule background aggregation jobs with Celery (beat scheduler) to compute rolling metrics and push updates over WebSockets.\n6. Provide a paginated historical metrics REST endpoint, caching hot queries in Redis for low-latency responses.\n\nDeliverables:\n- A list of required external libraries pinned to the latest versions available as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is utilized.","question_index":27,"modules":["aioredis","celery","fastapi","fastapi-users","fastapi-users-db-sqlalchemy","passlib","psycopg","pydantic","python-jose","redis","sqlmodel","sqlalchemy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a real-time collaborative document review service with semantic search and AI summarization.\n\nFunctional Requirements:\n1. HTTP API (HTTP\/3) using the latest starlite (v1.x) to manage documents and user sessions.  \n2. Real-time updates over WebSockets for live editing and presence using starlite\u2019s WebSocket support.  \n3. Semantic indexing and search: on each document save, compute embeddings and store\/retrieve via the latest qdrant-client.  \n4. AI summarization endpoint: call an LLM using the latest llm-tools library to generate concise summaries on demand.  \n5. Background task scheduling with dramatiq (latest) to clean up stale sessions and regenerate indexes.\n\nDeliverables:\n\u2022 Reiterate the numbered requirements.  \n\u2022 A complete, runnable Python 3.11+ code file under 150 lines.  \n\u2022 Use the latest starlite, qdrant-client, llm-tools, and dramatiq libraries available as of 2025-05-06.  \n\u2022 Include clear comments indicating where and why each external library is used.","question_index":55,"modules":["dramatiq","pydantic","qdrant-client","starlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are building a high-performance, AI-driven text-generation microservice for a real-time web application.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/generate endpoint using FastAPI (latest 0.100+) for incoming requests.\n2. Validate and parse the request payload with Pydantic v2 (latest) leveraging its new ArgModel feature.\n3. Enqueue each generation request as an asynchronous job using ARQ (latest 0.7+) backed by Redis.\n4. In the ARQ worker, invoke vLLM (latest 0.7+) AsyncLLMClient to perform streaming text inference.\n5. Persist each request and its full response to PostgreSQL using SQLModel (latest 0.0.x) with SQLAlchemy 2.0 async engine.\n6. Expose a WebSocket \/ws\/stream endpoint in FastAPI to broadcast partial LLM outputs to clients in real time.\n\nDeliverables:\n\u2022 The above numbered requirements list.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":30,"modules":["arq","asyncpg","fastapi","pydantic","sqlmodel","sqlalchemy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You must build a Python microservice that fetches articles by URL, summarizes them with AI, stores results in Postgres, and provides REST, GraphQL, and WebSocket interfaces with background orchestration using only the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a POST \/submit REST endpoint using the latest Starlite framework (released within the last 12 months) over HTTP\/3 to accept JSON { \"url\": string }.  \n2. Fetch article content asynchronously using the latest httpx library and handle timeouts and redirects.  \n3. Summarize text with the latest HuggingFace Transformers v5 pipeline, leveraging GPU if available.  \n4. Persist the original URL and its summary in Postgres via Tortoise ORM v0.25 (async ORM released in the last 12 months).  \n5. Expose a \/graphql endpoint using Strawberry GraphQL (latest release) to query summaries by URL.  \n6. Provide a \/ws\/notifications WebSocket in Starlite to notify clients in real time when a summary is ready.  \n7. Orchestrate fetch-and-summarize tasks as background jobs using Prefect 2 (newly released workflow orchestrator).\n\nDeliverables:\n- A copy of the numbered functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Use only the latest versions of Starlite, httpx, Transformers v5, Tortoise ORM v0.25, Strawberry GraphQL, and Prefect 2 as of 2025-05-06.\n- Include clear comments indicating where and how each external library is used.","question_index":70,"modules":["httpx","pydantic","prefect","starlite","strawberry-graphql","torch","tortoise-orm","transformers"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are tasked with developing a next-generation chat analytics platform that seamlessly integrates HTTP\/3, JWT auth, real-time messaging, vector search, and on-device LLM inference.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) with HTTP\/3 support via Hypercorn to serve all endpoints.\n2. Implement JWT authentication with rotational refresh tokens using fastapi-users (latest).\n3. Provide a real-time WebSocket chat endpoint using fastapi-websocket-pubsub (latest) supporting pub\/sub channels.\n4. On each incoming chat message, generate vector embeddings using llama-cpp-python (latest) with an on-device streaming pipeline, and store them in ChromaDB (latest) with HNSW indexing.\n5. Expose a REST endpoint to query semantic similarity: accept a text query, compute embeddings, perform a ChromaDB search, and return the top-3 similar messages.\n6. Expose a streaming summarization endpoint that returns an on-the-fly summary of recent messages using llama-cpp-python\u2019s streaming completion interface.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements you\u2019ve implemented.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":93,"modules":["chromadb","fastapi","fastapi-users","fastapi-websocket-pubsub","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: As a senior software engineer, develop a Python 3.11+ microservice that manages real-time collaborative whiteboard sessions, persists data, exposes REST and GraphQL APIs, generates session snapshots, integrates external metrics, and secures endpoints with OAuth2.\n\nFunctional Requirements:\n1. Implement session and user management REST endpoints using FastAPI (latest) with async event hooks and Pydantic V2 models.  \n2. Persist all data in PostgreSQL via Tortoise ORM (latest) using model lifecycle events.  \n3. Expose a GraphQL API with Strawberry GraphQL (latest) dataclass resolvers for session and user queries\/mutations.  \n4. Broadcast real-time drawing events using python-socketio (latest) over WebSockets.  \n5. Generate on-demand PNG snapshots of whiteboard state with Pillow-SIMD (latest) leveraging multi-threaded rendering.  \n6. Send session metrics asynchronously to an external analytics service using httpx (latest) with HTTP\/2 support.  \n7. Secure all endpoints with OAuth2 (authorization code + PKCE) using Authlib (latest).  \n\nDeliverables:\n- A numbered list of the functional requirements above.  \n- Complete, runnable Python 3.11+ code (\u2264 150 lines) integrating all requirements.  \n- Inline comments indicating where each external library is used.  \n- Use the latest stable releases of all libraries as of today\u2019s date.","question_index":50,"modules":["Pillow-SIMD","authlib","fastapi","httpx","pydantic","python-socketio","strawberry-graphql","tortoise-orm","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are a senior software engineer tasked with building a real-time analytics microservice that combines REST, GraphQL, persistent storage, and background processing using the latest Python ecosystem tools as of 2025-05-06.\n\nFunctional Requirements:\n1. Expose HTTP and WebSocket endpoints using the latest Litestar ASGI framework with annotated routing and streaming support.\n2. Implement a GraphQL API using the latest Strawberry GraphQL code-first schema builder with asyncio support.\n3. Persist and query analytics data in PostgreSQL via the latest Piccolo-ORM\u2019s async-first models and automatic migrations.\n4. Offload long-running analytics jobs to a Redis-backed async queue using the latest Arq library.\n5. Send WebSocket notifications to clients when background jobs complete.\n\nDeliverables:\n\u2022 A requirements list (pip install commands) specifying the latest versions of Litestar, Strawberry, Piccolo-ORM, and Arq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":65,"modules":["arq","litestar","piccolo","strawberry-graphql"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are a senior software engineer tasked with developing an end-to-end Python web application that leverages the latest libraries to showcase advanced API development, ORM integration, security, background processing, and real-time features.\n\nFunctional Requirements:\n1. Asynchronous REST API: implement CRUD endpoints for a `Task` model using FastAPI (latest version as of 2025-05-06).\n2. Data Modeling and Persistence: define `Task` ORM models with SQLModel (latest version) and connect to a SQLite or PostgreSQL database asynchronously.\n3. Security: secure all API endpoints with OAuth2 password flow using Authlib (latest version) to obtain and validate JWT tokens.\n4. Background Processing: create an asynchronous task queue with Arq (latest version) to process tasks in the background.\n5. Real-Time Notifications: establish a WebSocket endpoint using the websockets library (latest version) to push real-time task status updates.\n\nDeliverables:\n- The numbered list of functional requirements (as above).\n- Complete, runnable Python 3.11+ code under 150 lines. Use the latest versions of FastAPI, SQLModel, Authlib, Arq, and websockets as of 2025-05-06. Include clear comments indicating where and how each external library is used.","question_index":24,"modules":["arq","authlib","aiosqlite","fastapi","python-multipart","sqlmodel","sqlalchemy","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a real-time collaborative messaging microservice integrating graph storage, vector search, JWT authentication, and WebSockets using the latest Python libraries (as of May 6, 2025).\n\nFunctional Requirements:\n1. Authentication: secure all endpoints with JWTs using the latest TokenGuard library.\n2. Message Ingestion: implement POST \/messages to accept user messages; decode JWT, extract user ID.\n3. Graph Storage: store each message and its \u201cauthored_by\u201d relationship in a graph database via the latest GraphitORM.\n4. Vector Indexing & Search: generate text embeddings with the latest Morpher library, store them in its vector store, and expose GET \/search?q={text} to return top-5 similar messages.\n5. Real-Time Updates: broadcast new messages to connected clients over WebSockets using the latest Voila framework.\n\nDeliverables:\n1. A requirements list with each third-party library pinned to its latest version as of May 6, 2025.\n2. A single Python 3.11+ script (\u2264150 lines) implementing all endpoints, with clear comments marking where each external library is used.","question_index":35,"modules":["fastapi","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":2}
{"model":"gemini-2.5-pro-preview-05-06","question":"You are tasked with building a high-performance text summarization service with real-time updates, persistent storage, and caching for internal research teams.\n\nFunctional Requirements:\n1. Implement OAuth2 password flow with JWT tokens for user authentication using the latest python-jose.\n2. Expose a POST \/summarize endpoint that uses the latest transformers library to perform text summarization.\n3. Provide a WebSocket at \/ws\/progress to stream summarization progress in real time using FastAPI\u2019s latest WebSocket support.\n4. Persist user requests and summaries in a SQLite database via the latest SQLModel ORM.\n5. Cache summary results in Redis for 5 minutes using the latest aioredis library to accelerate repeat requests.\n\nDeliverables:\n\u2022 A requirements list specifying the latest versions of FastAPI, transformers, python-jose, SQLModel, aioredis (as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":10,"modules":["aioredis","fastapi","python-jose","pydantic","sqlmodel","transformers","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-pro-preview-05-06","question":"You are a senior software engineer.\n\nScenario: Build an end-to-end Python service for user profile CRUD with high-performance caching and real-time notifications.\n\nFunctional Requirements:\n1. Implement RESTful CRUD endpoints for user profiles using the latest FastAPI (as of 2025-05-06), leveraging its async lifespan context.\n2. Define an async PostgreSQL-backed User model with SQLModel (latest) using Relations and the new async engine.\n3. Cache GET \/users\/{id} responses in Redis with redis.asyncio (latest), utilizing cluster mode and automatic TTL invalidation on updates.\n4. Broadcast profile change events in real time over WebSocket using the websockets library (latest) with permessage-deflate compression.\n\nDeliverables:\n- A requirements.txt listing the latest versions of FastAPI, SQLModel, redis.asyncio and websockets as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":17,"modules":["fastapi","sqlmodel","redis","asyncpg","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are a senior software engineer building a real-time sentiment-analysis web service for social media feeds.\n\nFunctional Requirements:\n1. Use the latest Python web framework (released within the last 12 months) to expose REST endpoints with dependency injection and OpenAPI support.\n2. Implement real-time websocket notifications for incoming sentiment updates using a recently released WebSocket library (last 12 months).\n3. Persist incoming posts and sentiment scores in a NoSQL database via its newest async client library (released within the last 12 months).\n4. Offload sentiment inference to a CPU-optimized ML library (first released within the last 12 months) with zero-copy or memory-efficient batch inference.\n5. Schedule periodic cleanup and summary-generation tasks using a lightweight scheduler library introduced within the last 12 months.\n6. Serve a minimal interactive dashboard at \u201c\/dashboard\u201d using a modern, Python-native plotting\/UI library created in the last 12 months.\n\nDeliverables:\n- A bullet list of all third-party libraries chosen, pinned to \u201clatest\u201d versions as of today\u2019s date.\n- A single, complete Python 3.11+ file (under 150 lines) that implements all requirements, with clear comments marking where each external library is used.","question_index":94,"modules":["litestar","pydantic","solara","surrealdb","timely-scheduler"],"modulenotfound":["timely-scheduler"],"modulenotfound_count":1,"module_count":5}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: As a senior software engineer, you must build a real-time analytics backend that ingests user events, persists them, and serves both REST and GraphQL clients with low latency and live updates.\n\nFunctional Requirements:\n1. Implement a POST \/events endpoint using the latest FastAPI (as of 2025-05-06), leveraging its new lifespan context and async router features.\n2. Validate incoming JSON payloads with Pydantic v2\u2019s new @model_validator decorator for strict schema enforcement.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise ORM, utilizing its recent bulk_create optimization.\n4. Expose a GraphQL query and mutation schema via the latest Strawberry GraphQL with code-first federation support.\n5. Broadcast new events to connected clients over WebSockets using the latest websockets library with per-message deflate compression.\n\nDeliverables:\n\u2022 A requirements.txt listing each third-party library pinned to its latest version as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments indicating where and how each external library is used.","question_index":19,"modules":["fastapi","pydantic","tortoise-orm","strawberry-graphql","asyncpg","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are a senior software engineer building a modern microblogging backend that supports authenticated posts, AI-generated images, real-time feed updates, and cloud storage integration.\n\nFunctional Requirements:\n1. Implement a REST API using a Python web framework released in the last 12 months (use the latest available as of 2025-05-06).\n2. Persist posts in a SQL database via an async ORM library created in the last 12 months, leveraging advanced async model features.\n3. Broadcast new posts to connected clients over WebSockets using a recently released high-performance async messaging library.\n4. Authenticate users with OAuth2 (Google) using the latest OAuth client library.\n5. Generate post images by calling Stable Diffusion through the most recent Python API client for AI image generation.\n6. Store and serve AI-generated images in AWS S3 using the newest AWS SDK for Python.\n7. Ensure all components run under Python 3.11+ and fit within 150 lines of code.\n\nDeliverables:\n\u2022 A numbered list of the specific libraries (with versions) you chose to satisfy each requirement.  \n\u2022 Complete, runnable Python 3.11+ code (<150 lines) combining all features.  \n\u2022 Clear inline comments marking where and why each external library is used.","question_index":7,"modules":["fastapi","sqlmodel","authlib","starlette","aiosqlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are a senior software engineer tasked with building a serverless real-time IoT analytics service that ingests sensor data, performs streaming aggregation, applies online anomaly detection, and streams results to clients.\n\nFunctional Requirements:\n1. Ingest JSON messages from an MQTT broker (\u201csensors\/+\/data\u201d) using the latest asyncio-based gmqtt library (as of 2025-05-06).  \n2. Stream-process incoming messages with streamz (latest version) to compute 1-minute tumbling-window averages for temperature and humidity.  \n3. Apply online anomaly detection on each windowed aggregation using River\u2019s latest HDBSCAN-based drift detector.  \n4. Persist raw messages and aggregated results into TimescaleDB via the asyncpg driver (latest version).  \n5. Expose a Python 3.11+ FastAPI application with a WebSocket endpoint for live metric and anomaly alert streaming.  \n6. Wrap the FastAPI app for AWS Lambda deployment using the latest mangum adapter.\n\nDeliverables:\n\u2022 A list of third-party dependencies pinned to \u201clatest\u201d versions as of 2025-05-06.  \n\u2022 A single, complete, runnable Python 3.11+ source file under 150 lines.  \n\u2022 Clear inline comments showing where each external library is used.  \n\u2022 The code must integrate all functional requirements and be deployable serverlessly on AWS Lambda.","question_index":38,"modules":["gmqtt","streamz","river","asyncpg","fastapi","mangum"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario:\nYou are tasked with developing a lightweight AI-powered semantic search microservice in Python 3.11+.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI framework with HTTP\/3 support) to expose a POST \/search endpoint.  \n2. Validate and parse incoming JSON with the latest Pydantic v2\u2019s strict model parsing and Python pattern-matching features.  \n3. Asynchronously fetch text embeddings from OpenAI\u2019s embedding API using the latest HTTPX client with HTTP\/3.  \n4. Store and query vectors in a GPU-accelerated ChromaDB instance via the latest ChromaDB Python client.  \n5. Return the top 5 semantically related document IDs and similarity scores as a JSON response.\n\nDeliverables:\n\u2022 A numbered list confirming the above functional requirements.  \n\u2022 A single, complete, runnable Python 3.11+ source file (\u2264150 lines) that implements the microservice, with clear comments indicating where each external library is used.","question_index":3,"modules":["litestar","pydantic","httpx","chromadb","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a real-time chat analysis service that ingests user messages, performs on-device LLM summarization and sentiment analysis, caches results, persists chat history, and broadcasts updates via WebSockets.\n\nFunctional Requirements:\n1. Expose a POST \/messages endpoint using FastAPI (latest) to receive JSON payloads with \u201cuser_id\u201d and \u201ctext\u201d.\n2. Perform sentiment analysis and summarization on each message using llama-cpp-python (latest) in an async background task.\n3. Cache sentiment results for identical texts in Redis via redis-py (latest) to avoid recomputation.\n4. Persist messages, sentiments, and summaries in PostgreSQL using SQLModel (latest version).\n5. Provide a WebSocket \/ws\/{user_id} endpoint (FastAPI) that broadcasts new summaries and sentiments to connected clients in real time.\n6. Secure all endpoints with OAuth2 password flow via Authlib (latest).\n\nDeliverables:\n\u2022 Restate the numbered requirements above.  \n\u2022 Provide complete, runnable Python 3.11+ code integrating the latest versions of FastAPI, llama-cpp-python, redis-py, SQLModel, and Authlib, under 150 lines total.  \n\u2022 Include clear comments indicating where and how each external library is used.","question_index":32,"modules":["authlib","fastapi","llama-cpp-python","pydantic","redis","sqlmodel"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python microservice for real-time IoT sensor analytics and visualization.\n\nFunctional Requirements:\n1. REST API: Use the latest asynchronous Python web framework available as of 2025-05-06 to expose CRUD endpoints with advanced dependency injection and OpenAPI schema generation.\n2. WebSockets: Integrate the latest real-time streaming library as of 2025-05-06 supporting protocol-level backpressure to push live sensor updates to clients.\n3. ML Inference: Embed an on-the-fly analytics model using the latest Python JIT-accelerated inference library as of 2025-05-06, auto-batching requests on CPU\/GPU.\n4. Graph Storage: Persist sensor relationships in a distributed graph database via the latest async driver as of 2025-05-06 with reactive query pipelining.\n5. Dashboard UI: Serve an interactive web dashboard using the latest server-driven UI component library as of 2025-05-06 for real-time charting and controls.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":14,"modules":["fastapi","nicegui","pydantic","python-multipart","uvicorn","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"You are a Senior Software Engineer tasked with building a high-performance, real-time analytics microservice using the latest asynchronous Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled ASGI endpoint using the latest Litestar and aioquic (as of 2025-05-06) for bi-directional streaming.\n2. Implement real-time GraphQL subscriptions with the latest Strawberry GraphQL to push analytics updates.\n3. Define domain models and perform asynchronous CRUD operations on Postgres using the latest SQLModel.\n4. Integrate a Redis cache layer with the latest aioredis for hot-path data retrieval.\n5. Instrument request handling and background tasks with OpenTelemetry SDK for distributed tracing.\n\nDeliverables:\n- requirements.txt listing the latest library versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments indicating where each external library is used.","question_index":90,"modules":["SQLAlchemy","aioredis","asyncpg","litestar","opentelemetry-exporter-logging","opentelemetry-instrumentation-asgi","opentelemetry-sdk","sqlmodel","strawberry-graphql","uvicorn"],"modulenotfound":["opentelemetry-exporter-logging"],"modulenotfound_count":1,"module_count":10}
{"model":"gemini-2.5-pro-preview-05-06","question":"You are building a real-time collaborative code-review web application that leverages the latest AI, async, storage and security frameworks.\n\nFunctional Requirements:\n1. Implement a FastAPI 0.95+ server with async endpoints and Jinja2 templates for the main UI.\n2. Enable real-time code editing and review comments via WebSockets using the latest websockets library as of 2025-05-06.\n3. On each code submission, stream analysis from OpenAI\u2019s GPT-4o API (or equivalent newest model) and display suggestions in real time.\n4. Persist code snapshots and review metadata in Pinecone\u2019s newest Python client (vector storage) for similarity search.\n5. Secure all HTTP and WS routes with JWT authentication using the latest PyJWT release.\n6. Add structured logging and a \/healthz endpoint using structlog for observability.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total.  \n\u2022 Clear comments indicating where and why each external library is used.","question_index":37,"modules":["fastapi","jinja2","pydantic","pyjwt","python-dotenv","structlog","uvicorn","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a real-time Q&A web application that ingests documents, generates embeddings, serves semantic search, supports live WebSocket notifications, and exposes observability metrics.\n\nFunctional Requirements:\n1. Use the latest FastAPI (>=0.100) to implement asynchronous REST endpoints for document ingestion and search, plus a WebSocket endpoint leveraging its new lifecycle event hooks.\n2. Upon document ingestion, asynchronously generate embeddings with the latest llama-cpp-python (v0.2+) using 4-bit quantization and store vectors in Pinecone (latest) using hybrid search.\n3. Implement a `\/search` endpoint that queries Pinecone for nearest neighbors based on user input and returns ranked results.\n4. Schedule periodic re-indexing tasks with the latest arq (v1.10+), utilizing its async Redis job scheduler.\n5. Integrate OpenTelemetry Python SDK (v1.21+) for auto-instrumentation, exporting traces and metrics to Prometheus.\n\nDeliverables:\n- A requirements.txt listing the latest libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":33,"modules":["arq","fastapi","llama-cpp-python","opentelemetry-api","opentelemetry-exporter-prometheus","opentelemetry-instrumentation-fastapi","opentelemetry-sdk","pinecone-client","prometheus-client","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: As a senior software engineer, you must build a modular Python web application that serves real-time, ML-enhanced classification results with persistent storage and observability.\n\nFunctional Requirements:\n1. API Layer: Use the latest ASGI web framework available as of 2025-05-06 that supports HTTP\/3 and automatic OpenAPI generation to expose a POST \/classify endpoint.  \n2. Database Layer: Connect asynchronously to PostgreSQL using the latest async driver with statement pooling to record incoming requests and classification results.  \n3. Real-time Pub\/Sub: Implement server-side WebSocket broadcasts of classification results using the latest message queue library offering at-least-once delivery semantics.  \n4. ML Inference: Integrate on-demand image classification via the latest lightweight, GPU-accelerated inference engine released in the past 12 months.  \n5. Observability: Instrument distributed tracing and metrics collection using the latest OpenTelemetry-compatible tracing library.\n\nDeliverables:\n- A list of the latest library dependencies as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":18,"modules":["asyncpg","fastapi","opentelemetry-exporter-otlp-proto-http","opentelemetry-instrumentation-fastapi","opentelemetry-sdk","pydantic","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: As a senior software engineer, build a high-performance microblogging API using the latest Python libraries for REST, validation, async database operations, caching, background tasks, and real-time streaming.\n\nFunctional Requirements:\n1. Implement a user signup endpoint in FastAPI (>=0.100.0) using Pydantic v2 root_validator for advanced schema validation.  \n2. Create and list posts asynchronously in PostgreSQL via SQLModel\u2019s latest async engine.  \n3. Maintain a cache of the 10 most recent posts in Redis using redis-py (>=5.0) asyncio client, updating on post creation.  \n4. Offload image thumbnail generation to an async Dramatiq (>=2.0) background task triggered on new post with image.  \n5. Provide a WebSocket endpoint in FastAPI to broadcast newly created posts to connected clients in real time.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library (FastAPI, Pydantic v2, SQLModel, redis-py asyncio, Dramatiq) is used. Use the latest available versions of all libraries as of 2025-05-06.","question_index":87,"modules":["aiosqlite","dramatiq","fastapi","pydantic","redis","sqlmodel","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are building a real-time news aggregation microservice with live client updates.\n\nFunctional Requirements:\n1. Develop a REST API using Litestar (the latest version as of 2025-05-06) leveraging its OpenAPI v3.1 auto-generation feature.\n2. Asynchronously fetch and parse external RSS feeds over HTTP\/3 using HTTPX AsyncClient (the latest), then cache results in aiocache (the latest) with a TTL.\n3. Validate and serialize each article using Pydantic v2 (the latest) functional-style models to ensure data integrity.\n4. Expose a WebSocket endpoint using the websockets library (the latest) with permessage-deflate compression for real-time article broadcasting.\n5. Schedule periodic background tasks with Arq (the latest) to re-poll feeds every 5 minutes.\n\nDeliverables:\n- A list of the chosen libraries and their latest versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":31,"modules":["aiocache","arq","feedparser","httpx","litestar","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: Build a real-time collaborative note-taking microservice integrating REST, database persistence, caching, GraphQL, WebSockets, and background tasks.\n\nFunctional Requirements:\n1. Expose REST endpoints using FastAPI (latest) for creating, updating and retrieving notes; leverage Pydantic v2\u2019s new validation decorators.\n2. Persist notes asynchronously in SQLite via SQLModel (latest) with SQLAlchemy 2.0-style typing and async engine.\n3. Broadcast note creation and updates over WebSockets with FastAPI\u2019s WebSocket support to subscribed clients in real time.\n4. Cache note retrieval results in Redis using aioredis (latest), implement TTL and cache invalidation on updates.\n5. Provide a GraphQL schema and endpoint using Strawberry (latest) to query and filter notes with federation-style resolvers.\n6. Schedule a daily summary email using FastAPI BackgroundTasks and aiosmtplib (latest) for asynchronous SMTP delivery.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- A single Python 3.11+ source file under 150 lines that implements all requirements, with clear comments marking where each external library is used.","question_index":73,"modules":["aioredis","aiosmtplib","fastapi","pydantic","sqlmodel","sqlalchemy","strawberry-graphql"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"gemini-2.5-pro-preview-05-06","question":"Scenario: You are a senior software engineer building a proof-of-concept Python service that ingests user text, generates embeddings and classifications via an LLM, stores them, and streams nearest-neighbor results in real time.\n\nFunctional Requirements:\n1. Implement a JWT-secured POST \/ingest endpoint using the latest async Python web framework as of 2025-05-06 (e.g., Litestar).  \n2. In \/ingest, accept JSON `{ \"text\": \"...\" }`, call a state-of-the-art LLM (via the latest Transformers or LangChain client) to produce both an embedding vector and a classification label.  \n3. Persist the embedding vector in a vector database using the newest ChromaDB client.  \n4. Log the original text and its classification into a relational database through the latest async ORM (e.g., Prisma Python).  \n5. Expose a GET \/stream endpoint that streams nearest-neighbor matches for incoming embeddings over server-sent events using the newest SSE library (e.g., httpx-sse or framework-native support).  \n6. Target Python 3.11+, include type hints, and keep all code under 150 lines.\n\nDeliverables:\n\u2022 A `requirements.txt` (or equivalent) listing each library with exact versions (latest as of 2025-05-06).  \n\u2022 A single Python module (\u2264150 lines) that meets all requirements, with clear comments marking where and why each external library is used.","question_index":81,"modules":["chromadb","litestar","prisma","pydantic","sentence-transformers"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"o4-mini","question":"Scenario: As a senior software engineer, build a high-performance Python web application integrating real-time streaming, GraphQL, async ORM, JWT security, and caching using the latest third-party libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. HTTP & WebSocket server with the latest Litestar, utilizing its new streaming response API for real-time event delivery.\n2. GraphQL API with the latest Strawberry-GraphQL, defining queries, mutations, and subscription streams for live data updates.\n3. Async database models and CRUD operations using the latest Piccolo ORM\u2019s Python 3.11+ async support and built-in migration tooling.\n4. JWT authentication via the latest python-jose, supporting secure login, token rotation, and injection of user context into requests.\n5. Redis-backed caching with the latest aioredis to cache GraphQL query results and automatically invalidate on data changes.\n6. Trigger real-time client notifications over WebSockets when database events occur, leveraging Litestar\u2019s WebSocket routing.\n\nDeliverables:\n\u2022 A numbered list of the final functional requirements.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":57,"modules":["litestar","strawberry","piccolo","jose","aioredis","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario: You are tasked with building a modular Python 3.11+ microservice that delivers real-time chat with semantic search and scheduled maintenance, leveraging only the latest libraries released within the past 12 months.\n\nFunctional Requirements:\n1. Implement HTTP REST endpoints for posting and retrieving chat messages using Litestar (latest release).  \n2. Establish a WebSocket chat channel that broadcasts new messages to all connected clients via the official Litestar WebSocket plugin (latest).  \n3. Persist each message to MongoDB with Beanie ODM (latest) and simultaneously index its vector embedding into Qdrant using qdrant-client (latest).  \n4. Expose a GraphQL query endpoint that performs semantic search over stored messages via strawberry-graphql (latest).  \n5. Schedule a daily background job to purge messages older than 30 days using Dramatiq (latest).\n\nDeliverables:\n- A requirements.txt (or pyproject.toml) listing all dependencies with exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments marking where and how each external library is used.","question_index":76,"modules":["litestar","strawberry","beanie","motor","qdrant_client","sentence_transformers","dramatiq","dramatiq_scheduler"],"modulenotfound":["dramatiq_scheduler"],"modulenotfound_count":1,"module_count":8}
{"model":"o4-mini","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python web service that combines asynchronous REST & GraphQL APIs, real-time notifications, persistent storage, caching, and AI-driven text summarization.\n\nFunctional Requirements:\n1. Implement async HTTP endpoints with FastAPI (latest as of 2025-05-06) using Python 3.11 structural pattern matching and Pydantic v2 dataclass models.\n2. Provide a GraphQL endpoint powered by Strawberry GraphQL v1.0+ with schema federation support.\n3. Persist and query data in PostgreSQL via the latest Tortoise-ORM with asyncio-optimized connection pooling.\n4. Push real-time notifications over WebSockets using python-socketio latest async server mode.\n5. Summarize user-submitted text on demand using the latest llama-cpp-python library with optional GPU offload.\n6. Cache frequent database queries in Redis using aioredis latest cluster-mode support.\n\nDeliverables:\n- A requirements list specifying exact versions of all third-party libraries.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":74,"modules":["fastapi","pydantic","strawberry-graphql","tortoise-orm","python-socketio","aioredis","llama-cpp-python","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"o4-mini","question":"Scenario: Build a real-time collaborative note-taking service that supports user auth, live updates, AI summaries, PDF export, and caching using the latest Python web stack.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST API for user signup\/login with FastAPI (latest) using OAuth2 password flow.\n2. Persist users and notes in PostgreSQL via SQLModel (latest async features).\n3. Create a WebSocket endpoint broadcasting live note edits using the newest websockets library.\n4. Add an endpoint `\/summarize` that calls an external AI summarization API via httpx\u2019s HTTP\/3 client (latest).\n5. Provide a `\/export\/{note_id}` endpoint that generates a PDF of the note using Playwright Python (latest).\n6. Cache active JWT tokens in Redis Streams using aioredis v2.x.\n7. Emit structured JSON logs via loguru (latest advanced configuration).\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- A complete, runnable Python 3.11+ codebase under 150 lines, with clear comments indicating where each external library is used.","question_index":9,"modules":["fastapi","sqlmodel","sqlalchemy","jwt","aioredis","httpx","loguru","playwright","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"o4-mini","question":"You are a senior software engineer tasked with building a minimal Python web service for text ingestion, semantic search, and AI-powered summarization using the latest libraries.\n\nFunctional Requirements:\n1. HTTP API: use the latest Starlite framework (as of 2025-05-06) to expose async endpoints.\n2. Text Storage: on POST \/submit, accept JSON {\u201ctext\u201d: \u2026}, store original text and metadata in Redis via the latest redis-om Python client with async HashModel.\n3. Embedding: upon submission, generate a vector embedding using the latest ChromaDB Python client\u2019s async API and upsert into its vector store.\n4. Semantic Search: implement GET \/search?query=\u2026 that computes a query embedding via ChromaDB and returns top 3 matching texts from Redis.\n5. Summarization: implement GET \/summarize\/{id} to retrieve stored text by ID and produce a concise summary using the latest llama-cpp-python streaming API.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries (as of 2025-05-06).  \n\u2022 A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments indicating where and how each external library is used.","question_index":45,"modules":["starlite","redis-om","chromadb","llama-cpp-python","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"o4-mini","question":"Scenario: Senior software engineer tasked with developing a real-time GraphQL web service that validates incoming event data, persists it to PostgreSQL, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI web framework) as of 2025-05-06 to implement an async HTTP POST endpoint at `\/events`.\n2. Validate incoming JSON payloads with Pydantic v2 (latest) models, including at least one custom field validator.\n3. Persist validated events to PostgreSQL using the latest Prisma Python client (type-safe generated models and migrations).\n4. Expose a GraphQL API with the latest Strawberry GraphQL library, supporting queries for event history and subscriptions for real-time updates.\n5. Configure WebSocket support in Starlite to broadcast new events to all GraphQL subscription clients.\n\nDeliverables:\n- A brief recap of the functional requirements.\n- Complete, runnable Python 3.11+ code under 150 lines that uses the latest versions of each library as of 2025-05-06, with clear comments marking where each external library is imported and utilized.","question_index":22,"modules":["anyio","prisma","pydantic","strawberry","starlite"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"o4-mini","question":"Scenario: Design a minimal real-time chat service combining HTTP endpoints, data validation, async ORM persistence, WebSocket broadcasting, and JWT security using the latest Python libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Starlite to expose POST \/register and GET \/messages endpoints, leveraging its advanced dependency injection system.\n2. Define and serialize request\/response models with the latest Pydantic v2, employing its new `model_serializer` API.\n3. Persist User and Message models to SQLite asynchronously with the latest Ormar ORM, utilizing its built-in async migration feature.\n4. Implement a `\/ws` WebSocket route using the latest websockets library for real-time message broadcasting to connected clients.\n5. Secure both HTTP routes and WebSocket connections with JWT authentication via the latest PyJWT release.\n\nDeliverables:\n- A concise mapping of each numbered requirement to the specific library and feature used.\n- Complete, runnable Python 3.11+ code under 150 lines total, with clear comments indicating where each external library is imported and applied.","question_index":21,"modules":["jwt","sqlalchemy","databases","ormar","pydantic","starlite","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"Scenario: You are building a Python microservice that accepts user-uploaded text files, asynchronously generates summaries with a local LLM, and streams live progress to clients via WebSocket.\n\nFunctional Requirements:\n1. Implement a FastAPI app (use the latest FastAPI as of 2025-05-06) with an `\/upload` POST endpoint that accepts plain-text file uploads.  \n2. Upon upload, enqueue a background job using arq (latest version) to process the file asynchronously.  \n3. In the arq worker, load a local LLM via llama-cpp-python (latest version) using its new GPU offloading API to generate a summary.  \n4. Configure python-socketio (latest version) as an ASGI middleware in FastAPI to broadcast progress and final summary events to connected WebSocket clients.\n\nDeliverables:\n- A bullet list of the chosen third-party libraries with their exact versions.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":46,"modules":["fastapi","arq","python-socketio","llama-cpp-python","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"o4-mini","question":"Scenario: Build an end-to-end real-time customer feedback summarization service with live dashboard updates.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled JSON ingest endpoint using Litestar (latest as of 2025-05-06) with async support.\n2. Define a feedback model with Pydantic v2 and persist submissions into SQLite asynchronously via SQLModel (latest).\n3. On each new submission, call the OpenAI Python SDK (latest) asynchronously to generate a brief summary.\n4. Cache each summary in Redis via redis-om (latest) with a 1-hour TTL.\n5. Serve a live dashboard using Reflex (latest) that connects over WebSockets to display incoming summaries in real time.\n6. Use HTTPX (latest) with HTTP\/3 support for any external HTTP calls.\n\nDeliverables:\n\u2022 requirements.txt listing all dependencies pinned to the latest versions as of 2025-05-06  \n\u2022 A single Python 3.11+ script (under 150 lines) that implements the above, with clear comments indicating where and why each external library is used","question_index":61,"modules":["litestar","pydantic","sqlmodel","sqlalchemy","openai","httpx","redis_om","reflex","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"o4-mini","question":"Scenario: Develop a Python 3.11+ web application that provides secure collaborative document management with GraphQL, real-time WebSocket updates, AI-powered summarization, and on-demand PDF export.\n\nFunctional Requirements:\n1. Implement a GraphQL API for document queries and mutations using the latest \u201cariadne\u201d library with subscription support.\n2. Enable OAuth2 login with passwordless WebAuthn using the latest \u201cauthlib\u201d and \u201cfido2\u201d libraries.\n3. Provide real-time document update notifications over WebSocket leveraging FastAPI\u2019s built-in WebSocket support.\n4. Schedule asynchronous AI summarization tasks using the latest \u201cllama-cpp-python\u201d library and \u201cAPScheduler\u201d for background job management.\n5. Generate PDF exports of documents on demand using the latest \u201cplaywright\u201d headless browser library in Python.\n6. Persist documents and user metadata in PostgreSQL via the latest \u201ctortoise-orm\u201d with native async and Pydantic v2 model support.\n\nDeliverables:\n- A brief requirements list naming each third-party library and its version.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used. Use the latest library versions available as of 2025-05-06.","question_index":88,"modules":["fastapi","ariadne","tortoise-orm","pydantic","APScheduler","llama-cpp-python","playwright","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"o4-mini","question":"Scenario: You are designing a Python microservice for ingesting user text, storing embeddings, and providing real-time, rate-limited on-device summaries via WebSockets.\n\nFunctional Requirements:\n1. HTTP POST \/ingest  \n   - Validate payload (id: str, text: str) using Pydantic v2 root validators  \n   - Compute embeddings on-device with llama-cpp-python and store (id, embedding, text) in ChromaDB  \n2. HTTP GET \/query  \n   - Accept query text, validate with Pydantic v2  \n   - Compute embedding via llama-cpp-python, retrieve top-5 nearest documents from ChromaDB, return metadata  \n3. WebSocket \/summarize  \n   - Client sends document id  \n   - Fetch text from ChromaDB metadata  \n   - Stream back a summary using llama-cpp-python streaming API  \n   - Enforce token-per-second rate limiting via aiolimiter  \n4. Lifespan events  \n   - On startup, initialize ChromaDB client, Llama model, and aiolimiter  \n   - On shutdown, cleanly close any resources  \n\nDeliverables:\n- requirements.txt listing the latest available versions as of today of:\n  \u2022 starlite  \n  \u2022 pydantic v2  \n  \u2022 chromadb  \n  \u2022 llama-cpp-python  \n  \u2022 aiolimiter  \n- main.py: a single, runnable Python 3.11+ script (under 150 lines) implementing all above, with clear comments marking where each external library is used.","question_index":8,"modules":["starlite","pydantic","chromadb","llama-cpp-python","aiolimiter"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"o4-mini","question":"Scenario: Build a serverless Python application enabling real-time geolocated chat with sentiment analysis, dynamic map visualization, and PDF session reporting.\n\nFunctional Requirements:\n1. Real\u2010time WebSocket chat using the latest Litestar (released in the last 12 months) with asynchronous message persistence via a modern async ORM.\n2. Interactive geospatial mapping of incoming messages on the client side using the latest Python geospatial visualization library released in the last 12 months.\n3. On-the-fly sentiment analysis of each message using the latest Python sentiment analysis library released in the last 12 months.\n4. PDF generation of the entire chat session\u2014including message timeline, geolocation markers, and sentiment scores\u2014using the latest Python PDF library released in the last 12 months.\n5. Deployment configuration for AWS Lambda using the latest Serverless Framework plugin and AWS Lambda Powertools (all released in the last 12 months).\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":97,"modules":["litestar","tortoise-orm","transformers","folium","borb","aws-lambda-powertools","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"Scenario: You are a senior software engineer tasked with building a real-time event analytics dashboard that ingests events, stores them, generates AI-driven summaries, caches counts, and streams updates to authenticated clients with full observability.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to implement a POST \/events REST endpoint for event ingestion.\n2. Validate incoming JSON payloads with the latest Pydantic v2 models.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise-ORM.\n4. Cache aggregated event counts in Redis via the latest aioredis.\n5. Generate streaming summaries of event batches with the latest OpenAI Python client.\n6. Stream real-time updates to clients over WebSocket using FastAPI.\n7. Secure all HTTP and WebSocket endpoints with JWT authentication via the latest fastapi-jwt-auth.\n8. Instrument HTTP, database, cache, and AI client calls with the latest OpenTelemetry for distributed tracing.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest library names and versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines integrating all subdomains, with clear comments indicating where each external library is used.","question_index":4,"modules":["fastapi","pydantic","tortoise-orm","aioredis","openai","fastapi-jwt-auth","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-redis","opentelemetry-instrumentation-tortoise-orm"],"modulenotfound":["opentelemetry-instrumentation-tortoise-orm"],"modulenotfound_count":1,"module_count":11}
{"model":"o4-mini","question":"Scenario: You\u2019re building a high-performance async task management API using the latest libraries in web framework, async ORM, caching, background processing, and observability as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement CRUD REST endpoints for a Task model using the latest starlite.\n2. Persist tasks in Postgres asynchronously with the latest Tortoise-ORM.\n3. Enqueue a background job on task creation using the latest Dramatiq with Redis broker.\n4. Cache GET \/tasks\/{id} responses in Redis via the latest redis.asyncio.\n5. Instrument all HTTP and database calls with OpenTelemetry using the latest opentelemetry-instrumentation-starlite.\n\nDeliverables:\n- A requirements.txt listing each external dependency with explicit versions.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":34,"modules":["starlite","opentelemetry","tortoise","dramatiq","redis","uvicorn"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":6}
{"model":"o4-mini","question":"Scenario  \nYou are building an AI-powered document search microservice that ingests user uploads, indexes them with a GPU-accelerated vector store, caches recent queries, and exposes HTTP\/3 endpoints for real-time search.  \n\nFunctional Requirements  \n1. HTTP API (Starlite, latest)  \n   1.1 POST \/upload: receive plain-text documents, validate with Pydantic.  \n   1.2 GET \/query?q=\u2026: return top-k results for query.  \n2. Document Ingestion (llama-index, latest)  \n   2.1 Stream tokenized embeddings upon upload.  \n3. Vector Store (ChromaDB, latest)  \n   3.1 Use GPU-backed indexing for fast similarity search.  \n4. Caching Layer (redis-om, latest)  \n   4.1 Cache each query + results with TTL and Pydantic models.  \n\nDeliverables  \n\u2022 A requirements.txt listing the latest versions (as of 2025-05-06) of all third-party libraries, each released within the past 12 months.  \n\u2022 Complete, runnable Python 3.11+ code (\u2264150 lines) implementing all requirements.  \n\u2022 Clear inline comments indicating where and how each external library is used.","question_index":47,"modules":["starlite","pydantic","chromadb","llama-index","redis-om"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"o4-mini","question":"Scenario: You are a senior engineer building a high-throughput job processing microservice with a modern async Python stack.  \n\nFunctional Requirements:  \n1. Expose a POST \/jobs endpoint using the latest Starlite (as of 2025-05-06) to accept JSON job submissions.  \n2. Validate incoming JSON payloads with the latest Pydantic v2 models.  \n3. Persist job records to PostgreSQL via the latest prisma-client-py async ORM.  \n4. Enqueue and process jobs in the background using the latest Taskiq queue.  \n5. Expose a GET \/jobs\/{id}\/status endpoint to retrieve current job status.  \n6. Ensure all I\/O is non-blocking and use Python 3.11+ async\/await throughout.  \n\nDeliverables:  \n\u2022 A requirements list specifying the exact latest versions of Starlite, Pydantic v2, prisma-client-py, and Taskiq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ source code under 150 lines.  \n\u2022 Clear inline comments marking where each external library is used.","question_index":99,"modules":["starlite","pydantic","prisma-client-py","taskiq"],"modulenotfound":["prisma-client-py"],"modulenotfound_count":1,"module_count":4}
{"model":"o4-mini","question":"Scenario: Build a secure, real-time AI-powered chat web service.\n\nFunctional Requirements:\n1. Expose an async REST API using the latest FastAPI for user registration and login endpoints.  \n2. Implement JWT authentication with rotating refresh tokens via the latest Authlib.  \n3. Persist users and chat messages in SQLite using the latest SQLModel, leveraging its new async support.  \n4. Enable real-time chat notifications over WebSockets using the latest python-socketio and ASGI.  \n5. Integrate AI chat completions using the latest OpenAI Python client in an async background task.\n\nDeliverables:\n1. A requirements.txt listing exact, up-to-date package versions as of today\u2019s date.  \n2. A single Python 3.11+ file (<150 lines) containing complete, runnable code with clear comments marking where each external library is used.","question_index":91,"modules":["fastapi","sqlmodel","sqlalchemy","authlib","socketio","openai"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario: You are a senior software engineer tasked with building an AI-powered collaborative document annotation API.\n\nFunctional Requirements:\n1. Secure user signup and login with JWT-based authentication using Starlite (latest) and Pydantic v2.\n2. REST endpoint to upload documents and persist metadata in Redis OM Python (latest).\n3. WebSocket route for real-time annotation broadcasting using Starlite\u2019s built-in WebSocket support.\n4. Background summarization of uploaded documents using llama-cpp-python (latest).\n\nDeliverables:\n- A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":1,"modules":["PyJWT","pydantic","starlite","starlette","redis-om","llama-cpp-python"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario: You are a senior engineer tasked with building a secure, high-throughput microservice for customer order ingestion, persistence, background processing, and caching.\n\nFunctional Requirements:\n1. Use FastAPI (latest as of 2025-05-06) with HTTP\/3 support to implement POST \/orders accepting JSON order data.\n2. Define an asynchronous Order model with SQLModel (latest) and connect to an async database engine (SQLite or PostgreSQL).\n3. Enqueue and process new orders via Arq (latest) background worker.\n4. Implement OAuth2 with JWT issuance and protected endpoints using Authlib (latest).\n5. Cache GET \/orders\/{id} responses in Redis using aiocache (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, complete and runnable, with clear comments marking where each external library is used.","question_index":68,"modules":["fastapi","sqlmodel","sqlalchemy","authlib","arq","aiocache"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario: Build an end-to-end Python web service that accepts data submissions, runs ML inference asynchronously, and pushes results back in real time.\n\nFunctional Requirements:\n1. Implement a POST \/submit endpoint using the latest FastAPI (ASGI, HTTP\/2) to accept JSON payloads.\n2. Validate and persist submissions into PostgreSQL via the latest SQLModel with asynchronous sessions.\n3. Dispatch an asynchronous classification task via the latest Celery (Redis broker) when new data arrives.\n4. In the Celery task, perform inference using the latest Hugging Face Transformers pipeline (e.g., text-classification).\n5. Cache inference results in Redis using the latest aioredis with a 5-minute TTL.\n6. Expose a WebSocket \/ws endpoint (FastAPI) to stream cached or newly computed results to subscribed clients.\n7. Instrument all HTTP requests and Celery tasks with the latest OpenTelemetry SDK for tracing.\n\nDeliverables:\n\u2022 requirements.txt listing \u201cthe latest\u201d versions of FastAPI, SQLModel, Celery, Redis, aioredis, Transformers, and OpenTelemetry as of 2025-05-06.  \n\u2022 A single Python 3.11+ file under 150 lines of runnable code. Include clear comments where each external library is used.","question_index":59,"modules":["fastapi","sqlmodel","sqlalchemy","celery","transformers","aioredis","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-celery"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"o4-mini","question":"Scenario: You are tasked with building a real-time IoT analytics dashboard featuring anomaly detection, edge caching, vector search, and HTTP\/3 streaming.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of May 6, 2025) with HTTP\/3 and ASGI concurrency for API endpoints.\n2. Ingest IoT telemetry via real-time WebSocket streams using the latest websockets library.\n3. Perform transformer-based anomaly detection using the latest HuggingFace transformers and PyTorch 2.x.\n4. Store and query time-series embeddings in a vector database using the latest Pinecone Python client with HNSW indexing.\n5. Cache recent query results at the edge using the latest aioredis client with TTL invalidation.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the \u201clatest\u201d versions of all third-party libraries as of today.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":67,"modules":["fastapi","transformers","torch","pinecone","aioredis","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario: Build a high-performance, AI-powered web dashboard for real-time analytics and user collaboration.\n\nFunctional Requirements:\n1. Web API: Use the latest FastAPI (as of 2025-05-06) with async endpoints and the new dependency-injection system.\n2. Real-Time Collaboration: Implement bidirectional WebSocket chat using python-socketio v5.x+ for live user updates.\n3. AI Integration: Leverage the latest llama-cpp-python (or equivalent) to compute embeddings on incoming messages.\n4. Database Persistence: Store users and message history asynchronously with SQLModel v0.7+ (Pydantic v2) on SQLite.\n5. Caching Layer: Cache AI embedding results in Redis via aioredis v3.x+ to minimize redundant computation.\n6. Security: Protect all endpoints with OAuth2 JWT authentication using Authlib v1.2+.\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all third-party libraries as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, with clear inline comments indicating where each external library is used.","question_index":58,"modules":["fastapi","authlib","sqlmodel","sqlalchemy","aioredis","python-socketio","llama-cpp-python"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"You are a senior software engineer tasked with building a modern image-processing web service end-to-end.\n\nFunctional Requirements:\n1. Implement a REST endpoint in Litestar for image uploads that converts incoming images to AVIF using the latest pillow-avif-plugin.\n2. Asynchronously persist image metadata (filename, upload timestamp, AVIF URL, dimensions) in MongoDB via the latest Beanie ODM.\n3. Index and cache metadata in Redis for fast lookup using the latest Redis-OM Python library.\n4. Expose a GraphQL API via the latest Litestar-GraphQL plugin to query images with filtering, pagination, and type validation.\n5. Broadcast real-time upload notifications over WebSocket connections using Litestar\u2019s built-in WebSocket support.\n\nDeliverables:\n- A concise list of the five functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines, using the latest Litestar, Litestar-GraphQL, Beanie, Redis-OM, and pillow-avif-plugin (as of May 6, 2025), with clear comments indicating where each external library is used.","question_index":89,"modules":["pillow","pillow-avif-plugin","litestar","motor","beanie","redis-om","strawberry-graphql","litestar-graphql","uvicorn"],"modulenotfound":["litestar-graphql"],"modulenotfound_count":1,"module_count":9}
{"model":"o4-mini","question":"You are a Senior Software Engineer building a high-throughput, real-time web application that ingests user events, enriches them with AI, persists outcomes, and serves analytics via a modern dashboard.\n\nFunctional Requirements:\n1. Implement JWT-based user authentication using the latest async web framework with schema-first validation (e.g., HTTP\/3 support).\n2. Expose a WebSocket endpoint for real-time event ingestion using the latest WebSocket library with per-message compression.\n3. Perform fully async CRUD on PostgreSQL using the latest ORM with native asyncio support and advanced connection pooling.\n4. Offload heavy computations to background workers via the latest task queue library supporting asyncio and result backends.\n5. Integrate an AI text-classification pipeline using the latest AI inference library with streaming outputs.\n6. Instrument the service with distributed tracing using the latest observability library supporting OpenTelemetry protocols.\n\nDeliverables:\n\u2022 requirements.txt listing the latest available versions of all dependencies (as of May 6, 2025).  \n\u2022 A single Python 3.11+ file under 150 lines that is complete and runnable, with clear comments marking where each external library is used.","question_index":12,"modules":["arq","fastapi","jose","opentelemetry","sqlmodel","transformers","uvicorn"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":7}
{"model":"o4-mini","question":"Scenario: Develop a high-performance asynchronous web service for ingesting user datasets, persisting metadata, launching background processing jobs, and tracing all operations end-to-end.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/upload endpoint using Starlite (latest) with async file streaming and dependency injection.\n2. Define and migrate a PostgreSQL table via Piccolo ORM (latest), storing upload metadata and task IDs, leveraging its GraphQL auto-schema feature.\n3. Enqueue and process tasks with Asynq (latest) in Redis, using priority queues, retry policies, and task result callbacks.\n4. Instrument incoming HTTP requests and background tasks with OpenTelemetry (latest), enabling W3C trace-context propagation across components.\n5. Provide an HTTP GET \/status\/{task_id} endpoint that queries Piccolo ORM for real-time task status.\n\nDeliverables:\n\u2022 The numbered functional requirements list (as above)  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, using the latest versions of all libraries as of 2025-05-06, with clear comments indicating where each external library is used.","question_index":43,"modules":["opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-starlite","starlite","piccolo","asynq","uvicorn"],"modulenotfound":["opentelemetry-instrumentation-starlite"],"modulenotfound_count":1,"module_count":7}
{"model":"o4-mini","question":"As a senior software engineer, you are tasked with building a secure, real-time collaborative whiteboard microservice using cutting-edge Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3 API using the latest FastAPI (as of 2025-05-06) and uvicorn[standard] for low-latency streaming.\n2. Implement OAuth2 authorization with JWTs and refresh-token rotation using the latest Authlib.\n3. Persist whiteboard sessions and user metadata in PostgreSQL via the latest SQLModel with an async engine.\n4. Broadcast drawing events in real time over WebSockets using the latest websockets library with room-based pub\/sub.\n5. Vectorize stroke data into SVG on the fly using the latest Shapely or pyvips for advanced geometry operations.\n6. Upload periodic session snapshots to AWS S3 using the latest aioboto3 with asynchronous multipart uploads.\n\nDeliverables:\n- A requirements.txt listing the latest available versions of all external libraries as of 2025-05-06.\n- A single Python 3.11+ script under 150 lines, fully runnable, with clear comments indicating where each third-party library is used.","question_index":25,"modules":["fastapi","sqlmodel","authlib","aioboto3","websockets","shapely","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"Scenario: Build a real-time sentiment analysis dashboard that fetches trending Twitter topics, analyzes sentiment, stores results, and serves updates via WebSockets.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to create both REST and WebSocket endpoints, leveraging its built-in asyncio and WebSocket broadcast features.\n2. Use the latest HTTPX to asynchronously fetch trending topics from the Twitter API v2 over HTTP\/3.\n3. Use the latest Hugging Face transformers to perform sentiment analysis via a pretrained pipeline.\n4. Use the latest SQLModel to define async SQLite models and persist topics with sentiment scores.\n5. Use the latest Jinja2 to render a minimal HTML dashboard for initial page load before WebSocket updates.\n\nDeliverables:\n- A requirements list (pip install commands) specifying the latest versions of FastAPI, HTTPX, transformers, SQLModel, and Jinja2 as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":36,"modules":["httpx","transformers","sqlmodel","fastapi","jinja2"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"o4-mini","question":"Senior Software Engineer: You are developing an intelligent IoT monitoring dashboard that integrates real-time ingestion, streaming anomaly detection, WebSocket updates, dynamic UI rendering, and cloud infrastructure provisioning.\n\nFunctional Requirements:\n1. Build an ASGI web server using the latest ASGI framework (as of 2025-05-06) with HTTP\/2 and WebSocket endpoints for real-time data streaming.\n2. Connect asynchronously to an MQTT broker using the latest Python MQTT client library (as of 2025-05-06), subscribe to sensor topics, and relay messages to the server.\n3. Implement real-time anomaly detection on incoming sensor data using the latest Python anomaly detection library supporting incremental or deep learning.\n4. Serve a live, interactive front-end auto-generated from Python to JavaScript using the latest Python-to-JS UI framework.\n5. Provision and deploy all components via infrastructure-as-code using the latest Python IaC SDK on a major cloud provider.\n\nDeliverables:\n\u2022 Restate the numbered functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code integrating all five requirements, under 150 lines.  \n\u2022 Clear inline comments indicating where and how each external \u201clatest\u201d library (as of 2025-05-06) is imported and used.","question_index":49,"modules":["fastapi","uvicorn","asyncio-mqtt","river","reflex","aws-cdk.core","aws-cdk.aws-ec2"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"Scenario: Build a real-time collaborative note-taking web application with authentication, persistence, caching, live updates, and scheduled cleanup.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (as of today) for note creation, retrieval, update, and deletion.\n2. Secure endpoints with JWT authentication using the latest PyJWT.\n3. Persist notes in a relational database via the latest SQLModel ORM, defining Note and User models.\n4. Enable real-time note synchronization between clients using WebSockets with the latest python-socketio.\n5. Cache frequently accessed note metadata in Redis using the latest redis-om to reduce DB load.\n6. Schedule automatic archiving of notes older than 30 days using the latest APScheduler.\n7. Document all endpoints with OpenAPI and include example request\/response schemas.\n\nDeliverables:\n- A numbered list of all third-party libraries (with versions) you\u2019ve chosen.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear comments indicating where each external library is used.","question_index":69,"modules":["fastapi","sqlmodel","pyjwt","python-socketio","redis-om","apscheduler"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario: Build a high-performance asynchronous message processing service for real-time sentiment analysis, storage, notification, and search.\n\nFunctional Requirements:\n1. Implement an async HTTP API using the latest FastAPI (as of May 6, 2025) to accept user messages.\n2. Define and persist message models in PostgreSQL via the latest SQLModel ORM.\n3. Schedule and execute sentiment analysis in the background using the latest Dramatiq with RabbitMQ.\n4. After analysis, update the database and push real-time notifications to connected clients over WebSockets.\n5. Index messages and sentiment scores into Elasticsearch using the latest official Python client.\n6. Provide a search endpoint to query stored messages by keyword and sentiment.\n\nDeliverables:\n\u2022 A numbered list of the \u201clatest\u201d third-party libraries (with versions) used for each sub-domain.  \n\u2022 Complete, runnable Python 3.11+ application code under 150 lines, with clear comments indicating where each external library is used.","question_index":48,"modules":["fastapi","sqlmodel","dramatiq","elasticsearch"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"o4-mini","question":"Scenario: Build a microservice that accepts text input, asynchronously generates summaries, persists data, and provides real-time metrics.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI 3, HTTP\/3 support) to implement POST \/summaries accepting JSON { \"text\": \"\u2026\" }, enqueue a background job via Arq, and return a job ID.\n2. Persist requests and summaries in SQLite asynchronously with the latest SQLModel (Pydantic V2 model support).\n3. Offload summarization jobs to Redis using the latest Arq, invoking the latest LangChain to call OpenAI for a concise summary.\n4. Expose GET \/summaries\/{id} to retrieve stored summaries from the database.\n5. Instrument request counts and job durations with the latest prometheus_client, exposing metrics on GET \/metrics.\n\nDeliverables:\n- A bulleted list of chosen \u201clatest\u201d libraries with exact version numbers.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Clear inline comments indicating where each external library is used.\n- Self-contained implementation: after installing dependencies, code runs without further setup.","question_index":95,"modules":["starlite","sqlmodel","sqlalchemy","arq","langchain","prometheus_client","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"Scenario: Create a serverless real-time analytics API that ingests WebSocket events, performs on-the-fly NLP summarization, indexes embeddings, exposes a GraphQL endpoint, and deploys to AWS Lambda.\n\nFunctional Requirements:\n1. Use the latest Litestar (as of 2025-05-06) to build an HTTP\/2 GraphQL endpoint with schema-based typing.\n2. Implement a real-time WebSocket consumer using wsproto or litestar-websockets for event ingestion.\n3. Offload text summarization to an asynchronous background task leveraging the latest Transformers library.\n4. Generate embeddings with the newest OpenAI Embeddings API or sentence-transformers and store them in the latest ChromaDB vector store.\n5. Secure all endpoints with passwordless authentication via the latest fastapi-users or equivalent.\n6. Package and deploy the entire application to AWS Lambda using the latest Mangum adapter.\n\nDeliverables:\n- A numbered list of chosen \u201clatest\u201d library versions (as of 2025-05-06) mapping to each requirement.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":77,"modules":["litestar","starlette","strawberry","transformers","sentence-transformers","chromadb","mangum"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"Scenario: Develop a modern, high-performance AI-powered web service for real-time text processing.\n\nFunctional Requirements:\n1. Build a JSON REST API with Litestar using its latest dependency-injection and ASGI lifespan hooks.\n2. Integrate Tortoise ORM (async) to manage a PostgreSQL \u201cjobs\u201d table with migrations.\n3. Use llama-cpp-python to generate text embeddings or responses via a local LLaMA model.\n4. Expose a WebSocket endpoint with python-socketio (asyncio mode) for live client notifications.\n5. Cache recent embeddings in Redis using redis-py\u2019s asyncio client.\n6. Schedule background tasks (e.g., stale job cleanup) leveraging asyncio\u2019s latest task groups.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":60,"modules":["litestar","tortoise-orm","llama-cpp-python","python-socketio","redis","pydantic","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"Scenario: You are a senior software engineer building a real-time social media monitoring application that ingests tweets, analyzes sentiment, classifies images, displays live charts, and deploys as a serverless microservice using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Ingest tweets in real time from Twitter API v2 using the latest HTTPX async streaming client.\n2. Perform NLP sentiment analysis on tweet text with the latest Hugging Face Transformers pipeline.\n3. Classify embedded images using the latest Ultralytics YOLOv8 model.\n4. Render an interactive, live-updating dashboard using the latest Plotly Dash framework.\n5. Scaffold serverless deployment with the latest AWS Lambda Powertools for Python.\n\nDeliverables:\n- A requirements list of all external dependencies.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments marking where each external library is used.","question_index":98,"modules":["httpx","transformers","ultralytics","dash","aws_lambda_powertools","plotly"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario: A knowledge management platform must allow users to upload PDFs, index their content, and perform real-time similarity searches via a high-performance API.\n\nFunctional Requirements:\n1. Implement a PDF upload REST endpoint using the latest unstructured-sdk to extract text.\n2. Use the latest ChromaDB Python client to generate and store embeddings for each document.\n3. Provide a search REST endpoint that queries ChromaDB for top-k similar documents given a text query.\n4. Create a real-time WebSocket over HTTP\/3 endpoint using the latest aioquic to stream incremental search results.\n5. Assemble the application as an asynchronous service using the latest Starlite web framework.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":6,"modules":["unstructured_sdk","chromadb","starlite","aioquic","uvicorn"],"modulenotfound":["unstructured_sdk"],"modulenotfound_count":1,"module_count":5}
{"model":"o4-mini","question":"Scenario: As a senior software engineer, build an asyncio-based Python microservice showcasing HTTP\/3 REST, GraphQL, async ORM, real-time WebSockets, and scheduled tasks using the latest libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement an HTTP\/3 REST endpoint at `\/api\/items` using FastAPI (latest) and Hypercorn (latest) with HTTP\/3 support.\n2. Expose a GraphQL API at `\/graphql` using Strawberry (latest) with asynchronous resolvers fetching from the database.\n3. Integrate PostgreSQL via Tortoise ORM (latest), define an `Item` model, and apply programmatic migrations on startup.\n4. Provide WebSocket notifications at `\/ws\/updates` using python-socketio (latest), broadcasting item creation and deletion events.\n5. Schedule an hourly cleanup job with APScheduler (latest) in asyncio mode to remove `Item` records older than 7 days.\n\nDeliverables:\n- A `requirements.txt` listing the latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is imported and utilized.","question_index":66,"modules":["fastapi","pydantic","strawberry","tortoise","socketio","apscheduler"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario: Develop a real-time AI-driven sentiment analysis web service that ingests text, processes it via an on-device LLM, persists results, and streams live analytics to clients.\n\nFunctional Requirements:\n1. Expose RESTful and WebSocket endpoints using the latest Litestar Python framework (released within the last 12 months).\n2. Perform local LLM inference for sentiment analysis with the latest llama-cpp-python library (released within the last 12 months).\n3. Asynchronously persist raw text and sentiment scores in MongoDB using the latest async MongoDB driver (released within the last 12 months) with code-first schema.\n4. Offload sentiment analysis to a distributed background worker using the latest Dramatiq (or equivalent) async task queue released in the last 12 months.\n5. Broadcast real-time sentiment updates to connected clients over WebSockets.\n6. Render an interactive dashboard in server-side templates with the latest Plotly Python library (released within the last 12 months).\n7. Instrument the entire application with distributed tracing via the latest OpenTelemetry Python SDK (released within the last 12 months).\n\nDeliverables:\n- A requirements list specifying each third-party library and its exact latest version as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines total.\n- Clear inline comments indicating where and how each external library is used.","question_index":5,"modules":["litestar","llama-cpp-python","odmantic","dramatiq","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-dramatiq","plotly"],"modulenotfound":["opentelemetry-instrumentation-dramatiq"],"modulenotfound_count":1,"module_count":8}
{"model":"o4-mini","question":"You are a senior software engineer tasked with building a real-time analytics dashboard with AI-driven anomaly alerts using the latest Python libraries as of 2025-05-06.  \n\nFunctional Requirements:  \n1. Implement asynchronous REST API endpoints using the latest FastAPI.  \n2. Add WebSocket-based real-time alert streaming using the latest websockets.  \n3. Persist incoming metrics in PostgreSQL via async ORM operations using the latest SQLModel.  \n4. Cache expensive queries in Redis for high throughput using the latest aioredis.  \n5. Perform streaming anomaly detection on incoming metrics using the latest river library.  \n6. Enable file uploads of detailed reports to S3-compatible storage via the latest minio SDK.  \n\nDeliverables:  \n\u2022 A numbered list of required Python libraries (with versions as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":20,"modules":["fastapi","sqlalchemy","sqlmodel","aioredis","websockets","river","minio","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"o4-mini","question":"Build a real-time analytics microservice that lets authenticated users ingest sales data, stores it in PostgreSQL, streams live updates to connected clients, and generates PDF reports with embedded charts.\n\n1. Implement OAuth2 JWT authentication using the latest fastapi-jwt-auth as of May 6, 2025.  \n2. Create async CRUD endpoints for sales records with SQLModel and asyncpg.  \n3. Broadcast new sales events over WebSocket using python-socketio (latest).  \n4. Generate interactive charts in HTML via Plotly (latest) and convert them to PDF using WeasyPrint (latest).  \n5. Expose generated PDFs via an endpoint, serving files asynchronously with aiofiles.  \n\nDeliverables:  \n- A numbered confirmation of the functional requirements above.  \n- A single Python 3.11+ file under 150 lines\u2014complete, runnable code\u2014with clear comments indicating where each external library is used. Use the latest versions of all third-party libraries as of today\u2019s date.","question_index":26,"modules":["fastapi","fastapi-jwt-auth","pydantic","sqlmodel","sqlalchemy","python-socketio","plotly","weasyprint","aiofiles"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"o4-mini","question":"Scenario: You are tasked with building a real-time collaborative task management web service for enterprise teams.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to expose HTTP endpoints and WebSocket routes, leveraging its new dependency injection and WebSocket lifetime management.\n2. Configure an async PostgreSQL database using the latest Tortoise-ORM with its auto-migration CLI and Pydantic model integration.\n3. Broadcast real-time task updates to connected clients via the latest python-socketio in async mode.\n4. Implement OAuth2 authentication with JWT access and refresh tokens using the latest Authlib, including token rotation.\n5. Cache sessions and publish update events using the latest aioredis async Redis client.\n6. Schedule and execute background jobs (e.g., daily summaries) with the latest APScheduler async support.\n7. Instrument request tracing and logging using the latest OpenTelemetry Python SDK, exporting to console.\n8. Auto-generate interactive API docs using FastAPI\u2019s built-in Swagger UI and Redoc.\n\nDeliverables:\n\u2022 A bullet list of selected libraries (with version numbers).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":15,"modules":["fastapi","tortoise-orm","python-socketio","authlib","aioredis","apscheduler","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"o4-mini","question":"Scenario: You are building a high-performance Python backend that ingests user articles, generates AI-powered summaries, stores and caches results, and exposes them via REST, GraphQL, and real-time channels with background processing.\n\nFunctional Requirements:\n1. Accept article submissions over a REST API using FastAPI and validate payloads with Pydantic v2.  \n2. Offload summarization to a background worker with Dramatiq using Redis as a broker.  \n3. Summarize articles via the latest OpenAI Python SDK and cache summaries with aiocache.  \n4. Persist articles and summaries in SQLite using SQLModel\u2019s async ORM features.  \n5. Expose stored summaries through a GraphQL endpoint implemented with Strawberry GraphQL.  \n6. Broadcast new summary notifications to connected clients over WebSockets with python-socketio.  \n\nDeliverables:\n\u2022 List of the latest third-party libraries (as of today) and their install commands.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":63,"modules":["fastapi","pydantic","sqlmodel","dramatiq","openai","aiocache","strawberry","socketio"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"o4-mini","question":"Scenario: Develop an internal Python microservice where authenticated users can submit text to be summarized by an LLM and receive real-time notifications via WebSockets.\n\nFunctional Requirements:\n1. Implement an asynchronous REST API using the latest Starlite framework (2025-05-06) leveraging its dependency injection and lifespan event system.\n2. Integrate OAuth2 Authorization Code Flow for user authentication with Authlib\u2019s async client libraries released in the last 12 months.\n3. Persist users, submissions, and summaries in PostgreSQL using the latest Piccolo ORM with async schema migrations and connection pooling.\n4. Summarize submitted text by calling OpenAI\u2019s GPT-4 Turbo via the newest openai Python SDK and its function-calling feature.\n5. Broadcast summary completion events over WebSockets using a modern Python websockets library (with HTTP\/2 support) released in the past year.\n\nDeliverables:\n- requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":82,"modules":["starlite","authlib","piccolo","openai","websockets","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"You are tasked with building a real-time analytics web dashboard that ingests streaming data, processes it, summarizes it with AI, secures user access, and visualizes results dynamically using the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Implement a REST API in Python 3.11+ with FastAPI (HTTP\/3 support via Hypercorn) to accept JSON event streams.\n2. Broadcast processed insights in real time over WebSocket using python-socketio.\n3. Buffer incoming events and generate streaming AI summaries using the latest LangChain and OpenAI Python SDK.\n4. Perform real-time aggregation and lazy analytics on events with Polars.\n5. Produce dynamic Plotly charts of key metrics and serve them as JSON for front-end rendering.\n6. Secure all endpoints and WebSocket connections with JWT-based auth via fastapi-users.\n\nDeliverables:\n\u2022 A numbered list of the above functional requirements.  \n\u2022 A single Python 3.11+ file (under 150 lines) that fulfills all requirements, complete and runnable. Include clear comments indicating where and how each external library is used.","question_index":84,"modules":["fastapi","fastapi-users","pydantic","python-socketio","polars","plotly","langchain"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"Scenario: You are a senior software engineer tasked with building a next-generation real-time collaborative data analytics dashboard. Use the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a FastAPI server with a WebSocket endpoint for live data updates using the latest fastapi[full].\n2. Expose a type-safe GraphQL API at `\/graphql` using Strawberry with async schema stitching.\n3. Integrate AI-driven insights by generating embeddings on incoming data with llama-cpp-python.\n4. Perform vector similarity search over embeddings via the Weaviate Python client\u2019s async API.\n5. Secure all endpoints with passwordless authentication using py_webauthn for WebAuthn flows.\n6. Containerize the application using docker-py to build images and dynamically generate a docker-compose config.\n7. Instrument the app with async Prometheus metrics using prometheus-client.\n\nDeliverables:\n- A list of required libraries (with versions) using the latest releases as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":72,"modules":["yaml","fastapi","prometheus_client","strawberry","uvicorn","llama_cpp","weaviate","py_webauthn","docker"],"modulenotfound":["yaml","llama_cpp"],"modulenotfound_count":2,"module_count":9}
{"model":"o4-mini","question":"Scenario: Develop a real-time collaborative note-taking web application with AI-powered summarization.\n\nFunctional Requirements:\n1. Implement user authentication via OAuth2 password flow using the latest FastAPI features for secure login.  \n2. Expose a WebSocket endpoint for real-time CRDT-based document state synchronization using the newest FastAPI WebSocket enhancements.  \n3. Persist users and documents in PostgreSQL via the latest Piccolo ORM async API with JSONB indexing for rich querying.  \n4. Cache live presence and session metadata in Redis using aioredis with RedisJSON support for low-latency reads\/writes.  \n5. Summarize document edits on demand using the latest LangChain library and ChatOpenAI (GPT-4) integration.  \n6. Serve a minimal HTML frontend via Jinja2 templates that connects to the WebSocket and displays live updates.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of each dependency as of 2025-05-06.  \n\u2022 main.py: Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":52,"modules":["fastapi","starlette","python-jose","passlib","piccolo-orm","aioredis","y_py","langchain","uvicorn"],"modulenotfound":["piccolo-orm"],"modulenotfound_count":1,"module_count":9}
{"model":"o4-mini","question":"Scenario: Senior Software Engineer, you are tasked with building a prototype real-time collaborative note-taking service using only the latest Python libraries released in the last 12 months.\n\nFunctional Requirements:\n1. Develop an asynchronous REST API for note CRUD operations using the latest Litestar framework.\n2. Persist notes in a SQL database via a newly released async ORM library.\n3. Cache frequent note queries in Redis with a recently published async Redis client.\n4. Enable real-time collaboration via WebSockets using a modern Python WebSocket library launched within the last year.\n5. Expose an AI-powered summarization endpoint using the latest OpenAI Python client.\n6. Render an interactive HTML dashboard showing active collaborators with a new Python-to-JS visualization tool.\n7. Instrument distributed tracing and structured logging using a cutting-edge observability library released in the past 12 months.\n\nDeliverables:\n- requirements.txt listing only libraries created in the last 12 months with exact versions.\n- A single Python 3.11+ file under 150 lines containing the complete, runnable application, with clear comments indicating where each external library is used.","question_index":41,"modules":["litestar","tortoise-orm","redis","websockets","openai","shiny","opentelemetry-sdk","opentelemetry-instrumentation","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"o4-mini","question":"Scenario:\nCreate an AI-powered article summarization service that accepts user-submitted text, stores it, generates summaries asynchronously via an AI API, caches results, and streams them back to clients in real time.\n\nFunctional Requirements:\n1. Implement a REST API using FastAPI (latest) with endpoints POST \/articles to submit text and GET \/stream\/{article_id} for server-sent events.\n2. Persist submissions in a database using SQLModel (latest) with an async SQLite or PostgreSQL engine.\n3. Orchestrate a background job queue using Celery (latest) with Redis as broker to process newly submitted articles.\n4. Perform summarization using the OpenAI Python SDK (latest) and its chat completion endpoint.\n5. Cache generated summaries in Redis via aioredis (latest) with a configurable TTL.\n6. Stream summaries to connected clients in real time using server-sent events via sse-starlette (latest).\n\nDeliverables:\n\u2022 A list of all third-party libraries (with versions pinned to the latest as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":64,"modules":["fastapi","sqlmodel","sqlalchemy","celery","openai","aioredis","sse-starlette"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"Scenario: Build a lightweight feedback API that records user comments, analyzes sentiment in real time, stores results, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (as of 2025-05-06) to implement asynchronous REST endpoints for submitting feedback and querying stored analytics.\n2. Use the latest SQLModel to define async ORM models and persist incoming feedback in an SQLite database.\n3. Use the latest PyWebSocketX to broadcast each new sentiment result over an ASGI WebSocket endpoint.\n4. Use the latest VulcanMind inference library to perform on-the-fly sentiment analysis of submitted feedback.\n5. Implement structured logging and error handling throughout the request, inference, database, and WebSocket flows.\n\nDeliverables:\n- A requirements list with exact version pins for Starlite, SQLModel, PyWebSocketX, and VulcanMind (latest as of 2025-05-06).\n- Complete, runnable Python 3.11+ code (\u2264150 lines) that integrates all four libraries with clear comments marking where each is used.","question_index":83,"modules":["starlite","sqlmodel","sqlalchemy","pywebsocketx","vulcanmind","uvicorn"],"modulenotfound":["pywebsocketx","vulcanmind"],"modulenotfound_count":2,"module_count":6}
{"model":"o4-mini","question":"You are a senior software engineer building a high-performance, real-time analytics web service for IoT sensor data.  \n\nFunctional Requirements:  \n1. Expose HTTP REST ingest endpoints using the latest Nebula framework (released 2024-06) with its async dependency-injection.  \n2. Persist incoming readings to PostgreSQL via StarDB ORM (v1.0+, released 2024-08), leveraging async transactions and auto-migration.  \n3. Secure all endpoints with OAuth2 PKCE flows using AuthFusion (released 2024-12).  \n4. Broadcast processed data updates to dashboard clients via WaveSocket (released 2024-10) server-clustering.  \n5. Schedule hourly aggregation jobs using ChronoTask (released 2024-11) cron-style scheduler.  \n\nDeliverables:  \n\u2022 A requirements.txt listing the latest versions of all libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":92,"modules":["nebula","stardb","authfusion","wavesocket","chronotask"],"modulenotfound":["nebula","stardb","authfusion","wavesocket"],"modulenotfound_count":4,"module_count":5}
{"model":"o4-mini","question":"Scenario: You are a senior software engineer building a high-throughput chat summarization microservice in Python 3.11+ using only the latest libraries available as of May 6, 2025.\n\nFunctional Requirements:\n1. Real-time WebSocket ingestion: implement a `\/ws` endpoint using fastapi-socketio (latest) with session storage in Redis via `redis.asyncio`.\n2. AI summarization: enqueue incoming messages in Arq (latest) and process them with the OpenAI Python SDK (latest) calling GPT-4 Turbo for summaries.\n3. Async data persistence: store raw messages and summaries in PostgreSQL using SQLModel + encode\/databases with the asyncpg driver.\n4. Dynamic admin dashboard: expose `\/admin` rendering Jinja2 templates enhanced with HTMX via fastapi-htmx (latest).\n5. Email alerts: send summary notifications asynchronously using FastAPI-Mail (latest).\n6. Containerization: provide a multi-stage Dockerfile based on `python:3.11-slim`.\n\nDeliverables:\n- A `requirements.txt` listing the exact versions of all third-party libraries (as of May 6, 2025).\n- A single `main.py` (under 150 lines) containing complete, runnable Python 3.11+ code with clear comments indicating where and how each external library is used.","question_index":23,"modules":["fastapi","fastapi-socketio","redis","arq","openai","sqlmodel","sqlalchemy","databases","fastapi-htmx","fastapi-mail"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"o4-mini","question":"Scenario: Build a compact Python microservice that ingests text and images, leverages AI summarization, real-time streams results, and stores everything in a modern database.\n\nFunctional Requirements:\n1. Use the latest Litestar (async web framework released Dec 2023) to define:\n   a. POST \/articles to receive JSON {\u201ctitle\u201d:\u2026, \u201ccontent\u201d:\u2026}.\n   b. WebSocket endpoint \/ws\/summary for streaming summaries.\n2. Connect asynchronously to PostgreSQL via the latest Prisma Python client (released Jul 2023), defining models for Article and Image.\n3. Summarize incoming content using the latest llama-cpp-python (released Sep 2023), utilizing its streaming interface for token-by-token output.\n4. Implement \/upload to accept multipart\/form-data image files, generate HEIF thumbnails with the latest pillow-heif (released Oct 2023), and persist file paths.\n5. Ensure all endpoints use async\/await, proper error handling, and pydantic-based validation.\n\nDeliverables:\n\u2013 A requirements.txt listing the latest library versions as of 2025-05-06.\n\u2013 A single, runnable Python 3.11+ script under 150 lines, with clear comments indicating where each external library is used.","question_index":16,"modules":["pydantic","prisma","llama-cpp-python","pillow","pillow-heif","litestar","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"You are tasked with building a prototype web service for authenticated semantic search with real-time visual feedback.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) server with:\n   a. an OAuth2 login flow using PyOIDC (latest) against an external Identity Provider  \n   b. a POST \/query endpoint to accept natural-language queries  \n   c. a WebSocket \/ws endpoint to stream real-time results  \n2. Upon authentication, embed incoming queries using sentence-transformers (latest) and perform vector similarity search in ChromaDB (latest).  \n3. As results arrive, push them incrementally over the WebSocket to clients.  \n4. Generate a Plotly (latest) bar chart of top-k result scores and serve it as an HTML snippet.  \n5. Ensure all components run under Python 3.11+, integrate only \u201cthe latest\u201d library releases as of 2025-05-06, and keep implementation concise.  \n\nDeliverables:\n\u2022 A list of chosen libraries with exact versions (the \u201crequirements\u201d).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":42,"modules":["chromadb","fastapi","plotly","pyoidc","sentence_transformers","starlette"],"modulenotfound":["pyoidc"],"modulenotfound_count":1,"module_count":6}
{"model":"o4-mini","question":"Scenario: You are building a high-throughput, real-time analytics web application for processing IoT sensor streams using the latest Python async frameworks and libraries.\n\nFunctional Requirements:\n1. Ingest JSON sensor events via a REST API built on FastAPI (use its latest HTTP\/3 support).\n2. Broadcast processed metrics to connected clients over WebSockets using python-socketio (asyncio).\n3. Offload CPU-intensive aggregation to background workers with Arq (latest Redis-backed queue).\n4. Persist time-series summaries in PostgreSQL via SQLModel (Pydantic v2-powered ORM).\n5. Cache hot query results in Redis using aioredis for sub-millisecond lookups.\n6. Expose tracing and Prometheus metrics using OpenTelemetry and prometheus-client.\n\nDeliverables:\n- A bullet list of the latest library dependencies as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines with clear comments indicating where each external library is used.","question_index":40,"modules":["fastapi","sqlmodel","sqlalchemy","socketio","arq","aioredis","prometheus_client","opentelemetry","uvicorn"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":9}
{"model":"o4-mini","question":"You are a senior software engineer tasked with building a Python web application that integrates user authentication, real-time notifications, AI-based recommendations, and payment processing.\n\nFunctional Requirements:\n1. Implement a RESTful API using the latest FastAPI (2025-05-06) and Python 3.11 features (e.g. match-case).\n2. Secure all endpoints with OAuth2 JWT authentication via the latest fastapi-users, including email verification flows.\n3. Provide real-time notifications over WebSockets using the latest python-socketio async server.\n4. Handle one-time charges using the latest Stripe Python SDK via the Payment Intents API.\n5. Generate AI-driven product recommendations by invoking gpt-4-turbo through the latest openai library.\n6. Emit structured, context-rich logs for each operation using the latest structlog.\n\nDeliverables:\n\u2022 requirements.txt listing the latest versions of all third-party libraries as of 2025-05-06  \n\u2022 Complete, runnable Python 3.11+ code (under 150 lines) with clear comments marking where each external library is used","question_index":96,"modules":["structlog","fastapi","fastapi-users","sqlalchemy","python-socketio","stripe","openai"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"Scenario: As a senior software engineer, design a high-performance microservice for real-time document collaboration with secure access, persistence, and cloud integration.  \n\nFunctional Requirements:  \n1. Implement an asynchronous JSON REST API using the latest Python web framework available as of 2025-05-06, supporting pagination and input validation.  \n2. Secure all endpoints with OAuth2 PKCE and JWT using the latest authentication library (released within the last 12 months).  \n3. Enable bidirectional real-time collaboration events over WebSocket using the latest WebSocket library (released within the last 12 months).  \n4. Persist document data in an async NoSQL database (e.g., MongoDB) using the newest driver supporting transactions and change streams.  \n5. Upload and serve document attachments to an S3-compatible object storage using the latest SDK with multipart upload support.  \n6. Cache frequently accessed documents in Redis with TTL and automatic invalidation using the newest Redis client library.  \n\nDeliverables:  \n- A requirements.txt listing \u201cthe latest\u201d versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":80,"modules":["fastapi","jose","pydantic","motor","bson","aioboto3","redis","socketio","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"o4-mini","question":"Scenario: Build a high-performance Python microservice that combines cutting-edge web, real-time, AI and observability features into a single ASGI application.\n\nFunctional Requirements:\n1. HTTP\/3 ASGI server with automatic TLS and QUIC support using the latest HTTP\/3 library released within the past 12 months.  \n2. Real-time WebSocket endpoint that streams JSON events with zero-copy backpressure control using the newest streaming library created in the last year.  \n3. Semantic search pipeline: accept text uploads, generate embeddings, and perform vector similarity queries via the latest vector-store client released within 12 months.  \n4. On-device LLM inference endpoint that consumes streamed text segments and returns summaries using the most recent quantized model library published this year.  \n5. Distributed tracing and metrics export using the newest OpenTelemetry OTLP exporter released within the past 12 months.\n\nDeliverables:\n- A requirements list (requirements.txt) specifying the exact, latest versions of each third-party library as of 2025-05-06.  \n- A single Python 3.11+ script under 150 lines that implements all features above, with clear comments marking where each external library is used.","question_index":51,"modules":["fastapi","hypercorn","cryptography","opentelemetry","pinecone","sentence_transformers","llama_cpp"],"modulenotfound":["opentelemetry","llama_cpp"],"modulenotfound_count":2,"module_count":7}
{"model":"o4-mini","question":"Scenario  \nDevelop an end-to-end Python web service that provides passwordless WebAuthn authentication, stores and summarizes text inputs via OpenAI, broadcasts real-time summary events, and serves interactive charts.\n\nFunctional Requirements  \n1. Implement passwordless registration and login using the latest \u201cwebauthn\u201d library (FIDO2) with Litestar middleware.  \n2. Create REST endpoints under Litestar (latest version) for submitting text to summarize and retrieving stored summaries.  \n3. Use the latest SQLModel with an async PostgreSQL engine to persist input texts, summaries, timestamps, and user IDs.  \n4. Invoke the latest \u201copenai\u201d Python client to perform GPT-4 Turbo summarization with function-calling.  \n5. Serialize and broadcast new summaries over WebSockets using the latest \u201cmsgspec\u201d for high-performance JSON.  \n6. Serve an HTML page at \u201c\/charts\u201d that embeds an interactive Plotly chart of summary lengths over time.\n\nDeliverables  \n\u2022 A numbered list of all external libraries (with exact latest versions as of 2025-05-06) and their roles.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":13,"modules":["litestar","webauthn","sqlmodel","sqlalchemy","openai","msgspec","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"Scenario: As a senior software engineer, you must develop a real-time analytics dashboard backend that requires user authentication, data ingestion, live updates, background enrichment via an external API, and a web-based dashboard.\n\nFunctional Requirements:\n1. Implement user registration and login endpoints using JWT authentication with asymmetric keys via FastAPI-JWT-Auth (latest).\n2. Create a POST \/data endpoint to ingest JSON payloads and store them asynchronously in SQLite using SQLModel (latest).\n3. Broadcast each new record to connected WebSocket clients at \/ws using the latest websockets library, leveraging async broadcast.\n4. Schedule a background job every minute with arq (latest) to enrich stored records by streaming JSON from a third-party API via httpx (latest).\n5. Serve a simple HTML dashboard at GET \/dashboard that opens a WebSocket to \/ws and displays incoming records in real time.\n\nDeliverables:\n- requirements.txt listing the latest versions of all libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":75,"modules":["fastapi","fastapi-jwt-auth","pydantic","sqlmodel","websockets","httpx","arq"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"Scenario: Design and implement a Python microservice that exposes HTTP endpoints, real-time WebSocket notifications, persistent storage, caching, scheduled tasks, and email alerts using the latest third-party libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use FastAPI (latest) with HTTP\/2 support to expose async REST endpoints for user registration and notification retrieval.\n2. Use SQLModel (latest) as an async ORM for PostgreSQL, defining typed models for users and notifications.\n3. Implement WebSocket broadcasting via FastAPI\u2019s WebSocket support to push new notifications to connected clients.\n4. Leverage Aioredis (latest) Redis Streams to cache session state and stream notification events.\n5. Schedule periodic database cleanup and cache eviction using APScheduler (latest) with its asyncio integration.\n6. Send email alerts for critical notifications using Aiosmtplib (latest) with STARTTLS.\n7. Perform external health-check API calls using HTTPX (latest) with async retries and timeouts.\n\nDeliverables:\n- A requirements list naming each library and its latest version as of 2025-05-06.\n- A single, runnable Python 3.11+ script under 150 lines, including clear comments that indicate where each external library is used.","question_index":44,"modules":["fastapi","sqlmodel","sqlalchemy","aioredis","apscheduler","aiosmtplib","httpx","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"o4-mini","question":"Scenario: You are tasked as a senior software engineer to build an end-to-end collaborative document editing service with real-time updates, async persistence, GraphQL subscriptions, and cloud file storage.\n\nFunctional Requirements:\n1. Use the latest Litestar (first released within the last 12 months) to create an HTTP\/2 web server with WebSocket endpoints for real-time edit streams.\n2. Implement a GraphQL API with Strawberry GraphQL (latest) that supports queries, mutations, and live subscriptions for document change events.\n3. Persist documents in a PostgreSQL database via a fully async ORM of your choice that was first released in the last 12 months and supports automated migrations.\n4. Generate AWS S3 presigned URLs for resumable file attachments using aiobotocore (latest) with async upload\/download.\n5. Schedule periodic cleanup of stale sessions using an async task scheduler library first released within the last 12 months.\n\nDeliverables:\n- A brief list of the chosen third-party libraries (with exact versions) that meet the \u201cfirst released within the last 12 months\u201d requirement.\n- Complete, runnable Python 3.11+ code (under 150 lines) fulfilling the above requirements, with clear comments indicating where and how each external library is used.","question_index":78,"modules":["litestar","strawberry","asyncorm","aiobotocore","async_scheduler","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario:\nYou are a senior software engineer tasked with building a real-time AI summarization microservice.\n\nFunctional Requirements:\n1. Implement an async RESTful POST endpoint `\/summarize` that accepts JSON `{ \"text\": \"<string>\" }` and returns a summary generated via `llama-cpp-python`.\n2. Implement a WebSocket endpoint `\/stream` that streams incremental summary tokens to the client as they are produced.\n3. Persist each original text and its summary in an async SQLite database using `SQLModel`.\n4. Cache recent summaries in Redis for 10 minutes with `aioredis` to avoid redundant computation.\n5. Expose Prometheus-compatible metrics on `\/metrics` (request counts, latency histograms, cache hit\/miss) via `prometheus-client`.\n6. Use a background task to warm up the Llama model at startup and to periodically clean up expired cache entries.\n\nDeliverables:\n- A restatement of the functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Code must use the latest versions of FastAPI, SQLModel, aioredis, llama-cpp-python, and prometheus-client as of 2025-05-06.\n- Include clear comments indicating where each external library is used.","question_index":54,"modules":["fastapi","sqlmodel","aioredis","llama-cpp-python","prometheus-client"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"o4-mini","question":"You are a senior software engineer tasked with building a high-throughput real-time sentiment analysis microservice.\n\nFunctional Requirements:\n1. Use FastAPI (latest) to expose a POST \/analyze endpoint that accepts JSON text payloads and stores request metadata in PostgreSQL via SQLModel (latest).\n2. Secure all endpoints with OAuth2 JWT authentication using Authlib (latest), leveraging its PKCE support.\n3. Perform real-time sentiment analysis on incoming text using the latest Hugging Face Transformers pipeline with GPU acceleration via PyTorch (latest) and cache results in Redis using aiocache (latest).\n4. Broadcast live sentiment scores to connected clients over WebSockets using python-socketio (latest) and run the server on Uvicorn with HTTP\/3 support.\n5. Asynchronously upload any user-submitted audio snippets to S3-compatible storage via aiobotocore (latest).\n\nDeliverables:\n- requirements.txt listing the latest versions of all external libraries as of 2025-05-06  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":62,"modules":["fastapi","sqlmodel","authlib","transformers","aiocache","socketio","aiobotocore","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"o4-mini","question":"Scenario: Build a high-performance Python web application combining HTTP\/3 REST, GraphQL, real-time WebSockets, async database operations, data analytics, AI summarization, and distributed tracing.\n\nFunctional Requirements:\n1. HTTP\/3 REST API: use FastAPI (latest as of 2025-05-06) with Uvicorn HTTP\/3 support to serve \/items endpoints.\n2. Async GraphQL API: use Strawberry (latest as of 2025-05-06) to expose a GraphQL schema with DataLoader for batch fetching.\n3. Async DB access: use SQLModel (latest as of 2025-05-06) with an async SQLAlchemy engine to perform PostgreSQL CRUD.\n4. Real-time WebSockets: use websockets library (latest as of 2025-05-06) to broadcast item updates at \/ws.\n5. Data analytics: use Polars (latest as of 2025-05-06) lazy API to compute summary statistics on items.\n6. AI summarization: use OpenAI Python SDK (latest as of 2025-05-06) to generate summaries of item descriptions.\n7. Observability: use OpenTelemetry Python SDK (latest as of 2025-05-06) to instrument all endpoints for distributed tracing.\n\nDeliverables:\n- requirements.txt listing exact latest versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments marking where each external library is used.","question_index":53,"modules":["fastapi","opentelemetry","sqlmodel","sqlalchemy","strawberry","polars","openai","websockets"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":8}
{"model":"o4-mini","question":"Scenario: You are a senior software engineer building a real-time sentiment and OCR-enhanced geospatial chat dashboard for remote field teams.\n\nFunctional Requirements:\n1. Use FastAPI (latest version as of 2025-05-06) to implement an async REST API that serves a minimal HTML client.\n2. Implement bi-directional real-time chat via Socket.IO using python-socketio (latest) on the FastAPI server.\n3. Perform on-the-fly sentiment analysis of text messages using Hugging Face\u2019s transformers pipeline (latest) and broadcast sentiment scores.\n4. Accept image uploads from clients, extract embedded text with EasyOCR (latest), and inject the OCR result into the chat stream.\n5. Plot message geolocation metadata on a Folium (latest) map and serve it as an embeddable HTML snippet.\n\nDeliverables:\n- A requirements list enumerating the exact \u201clatest\u201d third-party libraries and versions as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":2,"modules":["numpy","PIL","easyocr","folium","socketio","fastapi","transformers","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"o4-mini","question":"Scenario: Develop a Python microservice that supports AI-driven product imagery, real-time stock updates, secure user authentication, and checkout processing.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) app with JWT-based auth using Authlib (latest) and secure password hashing.  \n2. Define async ORM models for Users and Products with SQLModel (latest) on PostgreSQL, including migration setup.  \n3. On product creation, generate an AI image via Diffusers (latest) or Stability-SDK (latest) and expose it via a REST endpoint.  \n4. Provide a WebSocket endpoint for real-time inventory updates using Starlette\u2019s WebSocket support (latest).  \n5. Create a \/checkout POST endpoint integrating Stripe\u2019s Python SDK (latest) with webhook handling to confirm payments.\n\nDeliverables:\n\u2022 A bullet list of the \u201clatest\u201d libraries (with pip install specifiers as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":85,"modules":["stripe","fastapi","authlib","passlib","sqlmodel","diffusers"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario: You are developing a high-performance, real-time AI-powered web service for personalized content recommendations.\n\nFunctional Requirements:\n1. Implement a RESTful POST \/recommend endpoint using the latest Python web framework released within the last 12 months, accepting JSON input and returning JSON responses.\n2. Add a WebSocket \/ws endpoint for streaming live recommendation updates using the newest asyncio-based WebSocket library from the past year.\n3. Integrate semantic search over user profiles by connecting to a vector database via its newest Python client library released within the last 12 months.\n4. Schedule periodic model retraining in a background worker using the most recent Python task scheduler released in the last 12 months.\n5. Store and serve user-uploaded media files on cloud object storage using the latest cloud storage client library published within the past 12 months.\n\nDeliverables:\n\u2022 A requirements list (requirements.txt) specifying the exact library names and versions (all released within the last 12 months).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total, with clear comments marking where and why each external library is used.","question_index":71,"modules":["litestar","websockets","pinecone","apscheduler","boto3","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario: Develop a high-performance Python microservice that ingests real-time social media posts via WebSockets, generates AI summaries, stores them in MongoDB Atlas, serves secured HTTP\/3 endpoints, and provides an interactive Streamlit dashboard for sentiment trends.\n\nFunctional Requirements:\n1. Initialize an asynchronous FastAPI app with HTTP\/3 (h2\/h3) and ASGI support.  \n2. Protect all HTTP endpoints with OAuth2 JWT using Authlib\u2019s latest OAuth library.  \n3. Consume a WebSocket stream of JSON posts from wss:\/\/stream.example.com asynchronously.  \n4. Summarize each post using OpenAI\u2019s latest Python SDK leveraging function-calling or embeddings.  \n5. Persist summaries and metadata in MongoDB Atlas via Motor (async driver).  \n6. Expose a secured REST endpoint `\/summaries` supporting pagination and filtering by sentiment.  \n7. Build a Streamlit dashboard that fetches `\/summaries` and visualizes sentiment trends over time with Plotly.\n\nDeliverables:\n- A requirements list naming the latest versions of all third-party libraries as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":28,"modules":["fastapi","authlib","pydantic","motor","openai","websockets","uvicorn","streamlit","httpx","pandas","plotly"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"o4-mini","question":"Scenario: Build a real-time HTTP\/3-powered AI chat service that streams user messages, generates LLM responses, persists embeddings in a vector DB, and exposes chat history via GraphQL.\n\nFunctional Requirements:\n1. HTTP\/3 streaming: use the latest aioquic (as of 2025-05-06) to handle bidirectional HTTP\/3 connections for chat.\n2. On-device LLM inference: integrate the latest llama-cpp-python for quantized model loading and response generation.\n3. Vector storage: use the latest chromadb Python client to embed each message (via llama-cpp-python\u2019s embed API) and store\/retrieve vectors.\n4. GraphQL API: implement an async GraphQL schema with the latest strawberry to query chat history from ChromaDB.\n5. Async orchestration: ensure end-to-end async I\/O under Python 3.11+, coordinating the QUIC server, LLM, and DB.\n6. Code constraints: keep the complete solution under 150 lines, include clear comments marking where each external library is used.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of aioquic, llama-cpp-python, chromadb, and strawberry (latest as of 2025-05-06)  \n\u2022 A single Python 3.11+ script (<150 lines) with runnable code and comments identifying each external library usage.","question_index":79,"modules":["aioquic","llama-cpp-python","chromadb","strawberry-graphql"],"modulenotfound":[],"modulenotfound_count":0,"module_count":4}
{"model":"o4-mini","question":"Scenario: Design and implement a high-performance real-time chat backend with semantic search and background processing.\n\nFunctional Requirements:\n1. Asynchronous REST API: use FastAPI (latest as of 2025-05-06) to expose user and message endpoints.\n2. HTTP\/3 WebSockets: implement real-time chat over WebSockets with Hypercorn\u2019s HTTP\/3 support.\n3. Async ORM persistence: store users and messages in PostgreSQL via SQLModel (async).\n4. Vector semantic search: index and query messages in Qdrant using the latest qdrant-client.\n5. Background sentiment analysis: enqueue and run analysis jobs with ARQ (async Redis-backed queue).\n6. JWT authentication: secure all endpoints with python-jose for JWT issuance and validation.\n7. Redis caching: cache active WebSocket connections or session data via redis.asyncio.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06  \n\u2022 A single Python 3.11+ file under 150 lines that:\n  \u2013 Is fully runnable  \n  \u2013 Shows clear comments where each external library is used  \n  \u2013 Integrates all above requirements into a cohesive application","question_index":11,"modules":["uvloop","fastapi","jose","sqlmodel","qdrant_client","arq","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"Scenario: A fintech startup requires a live transaction monitoring service that ingests, processes, stores, and notifies users of suspicious activity in real time.\n\nFunctional Requirements:\n1. Build an HTTP endpoint to receive and stream-respond to JSON transaction payloads using the latest async web framework released in the last 12 months.\n2. Validate and serialize incoming data with the latest data validation library, enforcing strict type constraints and custom validators.\n3. Persist transactions asynchronously in a relational database using the latest async ORM library compatible with Python 3.11+.\n4. Offload CPU-intensive anomaly detection to a background worker queue using the latest task orchestration library.\n5. Broadcast real-time alerts to connected clients via WebSockets using the latest real-time communication library.\n\nDeliverables:\n\u2022 requirements.txt listing exact versions of the latest libraries (as of May 6, 2025)  \n\u2022 A single Python 3.11+ file under 150 lines, complete and runnable, with clear comments indicating where each external library is used.","question_index":39,"modules":["fastapi","pydantic","tortoise","arq","socketio","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario: Build a real-time collaborative document editor service with AI-powered summarization, OAuth2 access control, WebSocket syncing, Redis caching, and NoSQL persistence.\n\nFunctional Requirements:\n1. Use the latest async Python web framework (as of 2025-05-06) to define RESTful CRUD endpoints for documents.\n2. Implement OAuth2 authorization with the latest Python OAuth library released within the last 12 months for secure user login and token issuance.\n3. Establish bi-directional real-time document synchronization over WebSockets using the newest real-time communication library (with built-in pub\/sub).\n4. Integrate the latest AI inference library published in the past year to generate live summaries of incremental document edits.\n5. Employ the latest Redis client library (released within the last year) to cache active sessions and coordinate pub\/sub channels for syncing.\n6. Persist document state and version history in a NoSQL database using the newest async database client released in the last 12 months.\n\nDeliverables:\n1. A numbered list of the chosen \u201clatest\u201d libraries (with a one-line description of their role).\n2. Complete, runnable Python 3.11+ code under 150 lines, including clear comments indicating where and why each external library is used.","question_index":86,"modules":["fastapi","authlib","motor","redis","python-socketio","openai","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"Scenario: Develop a user registration microservice over HTTP\/3 that validates input, persists user data, and integrates with an external email verification API.\n\nFunctional Requirements:\n1. Expose a POST \/users endpoint using Starlite (latest) with HTTP\/3 support.  \n2. Define request and response schemas with Pydantic v2 (latest) for data validation.  \n3. Persist user records in PostgreSQL via the latest Prisma Python client, including model definitions and migrations.  \n4. Perform an asynchronous HTTP\/3 request to an external email verification service using httpx (latest), and incorporate its response.  \n5. Return a JSON payload containing the stored user data and email verification status.\n\nDeliverables:\n- A list of required third-party libraries with exact versions (as of today)  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used","question_index":100,"modules":["starlite","pydantic","prisma","httpx","hypercorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"o4-mini","question":"Scenario: As a senior software engineer, build a modular web application that authenticates users with WebAuthn passkeys, streams real-time stock data, generates AI trading signals, processes payments, and delivers interactive dashboards.\n\nFunctional Requirements:\n1. Implement user authentication via WebAuthn using the latest pywebauthn library (as of 2025-05-06) with hardware security key support.\n2. Ingest and stream real-time stock price data over WebSockets using FastAPI and the latest Starlette HTTP\/2 push features.\n3. Generate AI-driven trading signals leveraging the latest OpenAI Python SDK\u2019s new function-calling interface.\n4. Integrate secure payment processing with the latest Stripe Python SDK supporting Payment Element and webhook handling.\n5. Serve interactive dashboards using the latest Plotly Dash or Reflex library for live data visualization and client-side callbacks.\n6. Add structured logging and Prometheus metrics using structlog and prometheus-client.\n\nDeliverables:\n- A concise mapping of each functional requirement to its implementation approach.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used and using the latest libraries available as of 2025-05-06.","question_index":56,"modules":["structlog","fastapi","starlette","prometheus_client","openai","stripe","dash","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"o4-mini","question":"You are a senior software engineer tasked with architecting a small, high-performance async web application using only the latest Python libraries as of 2025-05-06.\n\nFunctional Requirements:\n1. Use the latest Litestar framework (with native HTTP\/3 support) to define async REST endpoints for user signup and profile retrieval.  \n2. Integrate the latest Piccolo-ORM to define a PostgreSQL \u201cusers\u201d table and perform async CRUD operations.  \n3. Implement real-time notifications via WebSocket using the latest python-socketio in asyncio mode.  \n4. Schedule a daily summary email job with APScheduler\u2019s new AsyncIOScheduler and send mail via an async SMTP library.  \n5. Fetch external data during signup from a third-party API using HTTPX\u2019s HTTP\/2 client.\n\nDeliverables:\n\u2022 A requirements.txt listing exact package names and latest versions.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":29,"modules":["litestar","piccolo","httpx","python-socketio","APScheduler","aiosmtplib","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"Scenario: You are a senior software engineer building a high-performance real-time IoT analytics backend.\n\nFunctional Requirements:\n1. Expose an asynchronous HTTP API using FastAPI with WebSocket support to ingest and broadcast live sensor readings.\n2. Persist incoming data in PostgreSQL via SQLModel\u2019s async ORM and validate payloads with Pydantic v2 models.\n3. Enforce per-device rate limiting using Redis Streams and aioredis, leveraging RedisJSON for overflow handling.\n4. Secure all endpoints with OAuth2 passwordless login via FastAPI-Users, sending one-time codes.\n5. Schedule background aggregation jobs with Celery (beat scheduler) to compute rolling metrics and push updates over WebSockets.\n6. Provide a paginated historical metrics REST endpoint, caching hot queries in Redis for low-latency responses.\n\nDeliverables:\n- A list of required external libraries pinned to the latest versions available as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is utilized.","question_index":27,"modules":["aioredis","celery","fastapi","fastapi-users","pydantic","redis","sqlalchemy","sqlmodel"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"o4-mini","question":"Scenario: Build a real-time collaborative document review service with semantic search and AI summarization.\n\nFunctional Requirements:\n1. HTTP API (HTTP\/3) using the latest starlite (v1.x) to manage documents and user sessions.  \n2. Real-time updates over WebSockets for live editing and presence using starlite\u2019s WebSocket support.  \n3. Semantic indexing and search: on each document save, compute embeddings and store\/retrieve via the latest qdrant-client.  \n4. AI summarization endpoint: call an LLM using the latest llm-tools library to generate concise summaries on demand.  \n5. Background task scheduling with dramatiq (latest) to clean up stale sessions and regenerate indexes.\n\nDeliverables:\n\u2022 Reiterate the numbered requirements.  \n\u2022 A complete, runnable Python 3.11+ code file under 150 lines.  \n\u2022 Use the latest starlite, qdrant-client, llm-tools, and dramatiq libraries available as of 2025-05-06.  \n\u2022 Include clear comments indicating where and why each external library is used.","question_index":55,"modules":["starlite","qdrant_client","llm_tools","dramatiq","uvicorn"],"modulenotfound":["llm_tools"],"modulenotfound_count":1,"module_count":5}
{"model":"o4-mini","question":"Scenario: You are building a high-performance, AI-driven text-generation microservice for a real-time web application.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/generate endpoint using FastAPI (latest 0.100+) for incoming requests.\n2. Validate and parse the request payload with Pydantic v2 (latest) leveraging its new ArgModel feature.\n3. Enqueue each generation request as an asynchronous job using ARQ (latest 0.7+) backed by Redis.\n4. In the ARQ worker, invoke vLLM (latest 0.7+) AsyncLLMClient to perform streaming text inference.\n5. Persist each request and its full response to PostgreSQL using SQLModel (latest 0.0.x) with SQLAlchemy 2.0 async engine.\n6. Expose a WebSocket \/ws\/stream endpoint in FastAPI to broadcast partial LLM outputs to clients in real time.\n\nDeliverables:\n\u2022 The above numbered requirements list.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":30,"modules":["fastapi","pydantic","sqlmodel","sqlalchemy","arq","vllm","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"Scenario: You must build a Python microservice that fetches articles by URL, summarizes them with AI, stores results in Postgres, and provides REST, GraphQL, and WebSocket interfaces with background orchestration using only the latest libraries available as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement a POST \/submit REST endpoint using the latest Starlite framework (released within the last 12 months) over HTTP\/3 to accept JSON { \"url\": string }.  \n2. Fetch article content asynchronously using the latest httpx library and handle timeouts and redirects.  \n3. Summarize text with the latest HuggingFace Transformers v5 pipeline, leveraging GPU if available.  \n4. Persist the original URL and its summary in Postgres via Tortoise ORM v0.25 (async ORM released in the last 12 months).  \n5. Expose a \/graphql endpoint using Strawberry GraphQL (latest release) to query summaries by URL.  \n6. Provide a \/ws\/notifications WebSocket in Starlite to notify clients in real time when a summary is ready.  \n7. Orchestrate fetch-and-summarize tasks as background jobs using Prefect 2 (newly released workflow orchestrator).\n\nDeliverables:\n- A copy of the numbered functional requirements above.\n- Complete, runnable Python 3.11+ code under 150 lines.\n- Use only the latest versions of Starlite, httpx, Transformers v5, Tortoise ORM v0.25, Strawberry GraphQL, and Prefect 2 as of 2025-05-06.\n- Include clear comments indicating where and how each external library is used.","question_index":70,"modules":["starlite","httpx","transformers","tortoise","strawberry","prefect","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"o4-mini","question":"Scenario: You are tasked with developing a next-generation chat analytics platform that seamlessly integrates HTTP\/3, JWT auth, real-time messaging, vector search, and on-device LLM inference.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) with HTTP\/3 support via Hypercorn to serve all endpoints.\n2. Implement JWT authentication with rotational refresh tokens using fastapi-users (latest).\n3. Provide a real-time WebSocket chat endpoint using fastapi-websocket-pubsub (latest) supporting pub\/sub channels.\n4. On each incoming chat message, generate vector embeddings using llama-cpp-python (latest) with an on-device streaming pipeline, and store them in ChromaDB (latest) with HNSW indexing.\n5. Expose a REST endpoint to query semantic similarity: accept a text query, compute embeddings, perform a ChromaDB search, and return the top-3 similar messages.\n6. Expose a streaming summarization endpoint that returns an on-the-fly summary of recent messages using llama-cpp-python\u2019s streaming completion interface.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements you\u2019ve implemented.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":93,"modules":["fastapi","fastapi-users","fastapi-websocket-pubsub","llama-cpp-python","chromadb"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"o4-mini","question":"Scenario: As a senior software engineer, develop a Python 3.11+ microservice that manages real-time collaborative whiteboard sessions, persists data, exposes REST and GraphQL APIs, generates session snapshots, integrates external metrics, and secures endpoints with OAuth2.\n\nFunctional Requirements:\n1. Implement session and user management REST endpoints using FastAPI (latest) with async event hooks and Pydantic V2 models.  \n2. Persist all data in PostgreSQL via Tortoise ORM (latest) using model lifecycle events.  \n3. Expose a GraphQL API with Strawberry GraphQL (latest) dataclass resolvers for session and user queries\/mutations.  \n4. Broadcast real-time drawing events using python-socketio (latest) over WebSockets.  \n5. Generate on-demand PNG snapshots of whiteboard state with Pillow-SIMD (latest) leveraging multi-threaded rendering.  \n6. Send session metrics asynchronously to an external analytics service using httpx (latest) with HTTP\/2 support.  \n7. Secure all endpoints with OAuth2 (authorization code + PKCE) using Authlib (latest).  \n\nDeliverables:\n- A numbered list of the functional requirements above.  \n- Complete, runnable Python 3.11+ code (\u2264 150 lines) integrating all requirements.  \n- Inline comments indicating where each external library is used.  \n- Use the latest stable releases of all libraries as of today\u2019s date.","question_index":50,"modules":["authlib","fastapi","httpx","pillow-simd","pydantic","python-socketio","starlette","strawberry-graphql","tortoise-orm"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"o4-mini","question":"Scenario: You are a senior software engineer tasked with building a real-time analytics microservice that combines REST, GraphQL, persistent storage, and background processing using the latest Python ecosystem tools as of 2025-05-06.\n\nFunctional Requirements:\n1. Expose HTTP and WebSocket endpoints using the latest Litestar ASGI framework with annotated routing and streaming support.\n2. Implement a GraphQL API using the latest Strawberry GraphQL code-first schema builder with asyncio support.\n3. Persist and query analytics data in PostgreSQL via the latest Piccolo-ORM\u2019s async-first models and automatic migrations.\n4. Offload long-running analytics jobs to a Redis-backed async queue using the latest Arq library.\n5. Send WebSocket notifications to clients when background jobs complete.\n\nDeliverables:\n\u2022 A requirements list (pip install commands) specifying the latest versions of Litestar, Strawberry, Piccolo-ORM, and Arq as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":65,"modules":["litestar","strawberry","piccolo","arq","redis"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"o4-mini","question":"Scenario: You are a senior software engineer tasked with developing an end-to-end Python web application that leverages the latest libraries to showcase advanced API development, ORM integration, security, background processing, and real-time features.\n\nFunctional Requirements:\n1. Asynchronous REST API: implement CRUD endpoints for a `Task` model using FastAPI (latest version as of 2025-05-06).\n2. Data Modeling and Persistence: define `Task` ORM models with SQLModel (latest version) and connect to a SQLite or PostgreSQL database asynchronously.\n3. Security: secure all API endpoints with OAuth2 password flow using Authlib (latest version) to obtain and validate JWT tokens.\n4. Background Processing: create an asynchronous task queue with Arq (latest version) to process tasks in the background.\n5. Real-Time Notifications: establish a WebSocket endpoint using the websockets library (latest version) to push real-time task status updates.\n\nDeliverables:\n- The numbered list of functional requirements (as above).\n- Complete, runnable Python 3.11+ code under 150 lines. Use the latest versions of FastAPI, SQLModel, Authlib, Arq, and websockets as of 2025-05-06. Include clear comments indicating where and how each external library is used.","question_index":24,"modules":["fastapi","sqlmodel","sqlalchemy","authlib","arq","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario: Build a real-time collaborative messaging microservice integrating graph storage, vector search, JWT authentication, and WebSockets using the latest Python libraries (as of May 6, 2025).\n\nFunctional Requirements:\n1. Authentication: secure all endpoints with JWTs using the latest TokenGuard library.\n2. Message Ingestion: implement POST \/messages to accept user messages; decode JWT, extract user ID.\n3. Graph Storage: store each message and its \u201cauthored_by\u201d relationship in a graph database via the latest GraphitORM.\n4. Vector Indexing & Search: generate text embeddings with the latest Morpher library, store them in its vector store, and expose GET \/search?q={text} to return top-5 similar messages.\n5. Real-Time Updates: broadcast new messages to connected clients over WebSockets using the latest Voila framework.\n\nDeliverables:\n1. A requirements list with each third-party library pinned to its latest version as of May 6, 2025.\n2. A single Python 3.11+ script (\u2264150 lines) implementing all endpoints, with clear comments marking where each external library is used.","question_index":35,"modules":["voila","tokenguard","graphitorm","morpher"],"modulenotfound":["tokenguard","graphitorm","morpher"],"modulenotfound_count":3,"module_count":4}
{"model":"o4-mini","question":"You are tasked with building a high-performance text summarization service with real-time updates, persistent storage, and caching for internal research teams.\n\nFunctional Requirements:\n1. Implement OAuth2 password flow with JWT tokens for user authentication using the latest python-jose.\n2. Expose a POST \/summarize endpoint that uses the latest transformers library to perform text summarization.\n3. Provide a WebSocket at \/ws\/progress to stream summarization progress in real time using FastAPI\u2019s latest WebSocket support.\n4. Persist user requests and summaries in a SQLite database via the latest SQLModel ORM.\n5. Cache summary results in Redis for 5 minutes using the latest aioredis library to accelerate repeat requests.\n\nDeliverables:\n\u2022 A requirements list specifying the latest versions of FastAPI, transformers, python-jose, SQLModel, aioredis (as of 2025-05-06).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":10,"modules":["fastapi","python-jose","sqlmodel","aioredis","transformers"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"o4-mini","question":"You are a senior software engineer.\n\nScenario: Build an end-to-end Python service for user profile CRUD with high-performance caching and real-time notifications.\n\nFunctional Requirements:\n1. Implement RESTful CRUD endpoints for user profiles using the latest FastAPI (as of 2025-05-06), leveraging its async lifespan context.\n2. Define an async PostgreSQL-backed User model with SQLModel (latest) using Relations and the new async engine.\n3. Cache GET \/users\/{id} responses in Redis with redis.asyncio (latest), utilizing cluster mode and automatic TTL invalidation on updates.\n4. Broadcast profile change events in real time over WebSocket using the websockets library (latest) with permessage-deflate compression.\n\nDeliverables:\n- A requirements.txt listing the latest versions of FastAPI, SQLModel, redis.asyncio and websockets as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":17,"modules":["fastapi","sqlmodel","sqlalchemy","redis","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"o4-mini","question":"Scenario: You are a senior software engineer building a real-time sentiment-analysis web service for social media feeds.\n\nFunctional Requirements:\n1. Use the latest Python web framework (released within the last 12 months) to expose REST endpoints with dependency injection and OpenAPI support.\n2. Implement real-time websocket notifications for incoming sentiment updates using a recently released WebSocket library (last 12 months).\n3. Persist incoming posts and sentiment scores in a NoSQL database via its newest async client library (released within the last 12 months).\n4. Offload sentiment inference to a CPU-optimized ML library (first released within the last 12 months) with zero-copy or memory-efficient batch inference.\n5. Schedule periodic cleanup and summary-generation tasks using a lightweight scheduler library introduced within the last 12 months.\n6. Serve a minimal interactive dashboard at \u201c\/dashboard\u201d using a modern, Python-native plotting\/UI library created in the last 12 months.\n\nDeliverables:\n- A bullet list of all third-party libraries chosen, pinned to \u201clatest\u201d versions as of today\u2019s date.\n- A single, complete Python 3.11+ file (under 150 lines) that implements all requirements, with clear comments marking where each external library is used.","question_index":94,"modules":["litestar","motor","llama_cpp","websockets","cronite","nicegui"],"modulenotfound":["llama_cpp","cronite"],"modulenotfound_count":2,"module_count":6}
{"model":"o4-mini","question":"Scenario: As a senior software engineer, you must build a real-time analytics backend that ingests user events, persists them, and serves both REST and GraphQL clients with low latency and live updates.\n\nFunctional Requirements:\n1. Implement a POST \/events endpoint using the latest FastAPI (as of 2025-05-06), leveraging its new lifespan context and async router features.\n2. Validate incoming JSON payloads with Pydantic v2\u2019s new @model_validator decorator for strict schema enforcement.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise ORM, utilizing its recent bulk_create optimization.\n4. Expose a GraphQL query and mutation schema via the latest Strawberry GraphQL with code-first federation support.\n5. Broadcast new events to connected clients over WebSockets using the latest websockets library with per-message deflate compression.\n\nDeliverables:\n\u2022 A requirements.txt listing each third-party library pinned to its latest version as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear inline comments indicating where and how each external library is used.","question_index":19,"modules":["fastapi","pydantic","tortoise","strawberry","websockets","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario: You are a senior software engineer building a modern microblogging backend that supports authenticated posts, AI-generated images, real-time feed updates, and cloud storage integration.\n\nFunctional Requirements:\n1. Implement a REST API using a Python web framework released in the last 12 months (use the latest available as of 2025-05-06).\n2. Persist posts in a SQL database via an async ORM library created in the last 12 months, leveraging advanced async model features.\n3. Broadcast new posts to connected clients over WebSockets using a recently released high-performance async messaging library.\n4. Authenticate users with OAuth2 (Google) using the latest OAuth client library.\n5. Generate post images by calling Stable Diffusion through the most recent Python API client for AI image generation.\n6. Store and serve AI-generated images in AWS S3 using the newest AWS SDK for Python.\n7. Ensure all components run under Python 3.11+ and fit within 150 lines of code.\n\nDeliverables:\n\u2022 A numbered list of the specific libraries (with versions) you chose to satisfy each requirement.  \n\u2022 Complete, runnable Python 3.11+ code (<150 lines) combining all features.  \n\u2022 Clear inline comments marking where and why each external library is used.","question_index":7,"modules":["litestar","tortoise-orm","nats-py","authlib","diffusers","boto3"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario: You are a senior software engineer tasked with building a serverless real-time IoT analytics service that ingests sensor data, performs streaming aggregation, applies online anomaly detection, and streams results to clients.\n\nFunctional Requirements:\n1. Ingest JSON messages from an MQTT broker (\u201csensors\/+\/data\u201d) using the latest asyncio-based gmqtt library (as of 2025-05-06).  \n2. Stream-process incoming messages with streamz (latest version) to compute 1-minute tumbling-window averages for temperature and humidity.  \n3. Apply online anomaly detection on each windowed aggregation using River\u2019s latest HDBSCAN-based drift detector.  \n4. Persist raw messages and aggregated results into TimescaleDB via the asyncpg driver (latest version).  \n5. Expose a Python 3.11+ FastAPI application with a WebSocket endpoint for live metric and anomaly alert streaming.  \n6. Wrap the FastAPI app for AWS Lambda deployment using the latest mangum adapter.\n\nDeliverables:\n\u2022 A list of third-party dependencies pinned to \u201clatest\u201d versions as of 2025-05-06.  \n\u2022 A single, complete, runnable Python 3.11+ source file under 150 lines.  \n\u2022 Clear inline comments showing where each external library is used.  \n\u2022 The code must integrate all functional requirements and be deployable serverlessly on AWS Lambda.","question_index":38,"modules":["gmqtt","streamz","river","asyncpg","fastapi","mangum"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario:\nYou are tasked with developing a lightweight AI-powered semantic search microservice in Python 3.11+.\n\nFunctional Requirements:\n1. Use the latest Starlite (ASGI framework with HTTP\/3 support) to expose a POST \/search endpoint.  \n2. Validate and parse incoming JSON with the latest Pydantic v2\u2019s strict model parsing and Python pattern-matching features.  \n3. Asynchronously fetch text embeddings from OpenAI\u2019s embedding API using the latest HTTPX client with HTTP\/3.  \n4. Store and query vectors in a GPU-accelerated ChromaDB instance via the latest ChromaDB Python client.  \n5. Return the top 5 semantically related document IDs and similarity scores as a JSON response.\n\nDeliverables:\n\u2022 A numbered list confirming the above functional requirements.  \n\u2022 A single, complete, runnable Python 3.11+ source file (\u2264150 lines) that implements the microservice, with clear comments indicating where each external library is used.","question_index":3,"modules":["starlite","starlette","pydantic","httpx","chromadb","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario: Build a real-time chat analysis service that ingests user messages, performs on-device LLM summarization and sentiment analysis, caches results, persists chat history, and broadcasts updates via WebSockets.\n\nFunctional Requirements:\n1. Expose a POST \/messages endpoint using FastAPI (latest) to receive JSON payloads with \u201cuser_id\u201d and \u201ctext\u201d.\n2. Perform sentiment analysis and summarization on each message using llama-cpp-python (latest) in an async background task.\n3. Cache sentiment results for identical texts in Redis via redis-py (latest) to avoid recomputation.\n4. Persist messages, sentiments, and summaries in PostgreSQL using SQLModel (latest version).\n5. Provide a WebSocket \/ws\/{user_id} endpoint (FastAPI) that broadcasts new summaries and sentiments to connected clients in real time.\n6. Secure all endpoints with OAuth2 password flow via Authlib (latest).\n\nDeliverables:\n\u2022 Restate the numbered requirements above.  \n\u2022 Provide complete, runnable Python 3.11+ code integrating the latest versions of FastAPI, llama-cpp-python, redis-py, SQLModel, and Authlib, under 150 lines total.  \n\u2022 Include clear comments indicating where and how each external library is used.","question_index":32,"modules":["fastapi","llama-cpp-python","redis","sqlmodel","authlib"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"o4-mini","question":"Scenario: You are a senior software engineer tasked with building a high-performance Python microservice for real-time IoT sensor analytics and visualization.\n\nFunctional Requirements:\n1. REST API: Use the latest asynchronous Python web framework available as of 2025-05-06 to expose CRUD endpoints with advanced dependency injection and OpenAPI schema generation.\n2. WebSockets: Integrate the latest real-time streaming library as of 2025-05-06 supporting protocol-level backpressure to push live sensor updates to clients.\n3. ML Inference: Embed an on-the-fly analytics model using the latest Python JIT-accelerated inference library as of 2025-05-06, auto-batching requests on CPU\/GPU.\n4. Graph Storage: Persist sensor relationships in a distributed graph database via the latest async driver as of 2025-05-06 with reactive query pipelining.\n5. Dashboard UI: Serve an interactive web dashboard using the latest server-driven UI component library as of 2025-05-06 for real-time charting and controls.\n\nDeliverables:\n\u2022 requirements.txt listing the latest library versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":14,"modules":["fastapi","pydantic","neo4j","jax","nicegui"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"o4-mini","question":"You are a Senior Software Engineer tasked with building a high-performance, real-time analytics microservice using the latest asynchronous Python libraries.\n\nFunctional Requirements:\n1. Expose an HTTP\/3-enabled ASGI endpoint using the latest Litestar and aioquic (as of 2025-05-06) for bi-directional streaming.\n2. Implement real-time GraphQL subscriptions with the latest Strawberry GraphQL to push analytics updates.\n3. Define domain models and perform asynchronous CRUD operations on Postgres using the latest SQLModel.\n4. Integrate a Redis cache layer with the latest aioredis for hot-path data retrieval.\n5. Instrument request handling and background tasks with OpenTelemetry SDK for distributed tracing.\n\nDeliverables:\n- requirements.txt listing the latest library versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with inline comments indicating where each external library is used.","question_index":90,"modules":["litestar","sqlmodel","sqlalchemy","aioredis","strawberry","opentelemetry","aioquic","uvicorn"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":8}
{"model":"o4-mini","question":"You are building a real-time collaborative code-review web application that leverages the latest AI, async, storage and security frameworks.\n\nFunctional Requirements:\n1. Implement a FastAPI 0.95+ server with async endpoints and Jinja2 templates for the main UI.\n2. Enable real-time code editing and review comments via WebSockets using the latest websockets library as of 2025-05-06.\n3. On each code submission, stream analysis from OpenAI\u2019s GPT-4o API (or equivalent newest model) and display suggestions in real time.\n4. Persist code snapshots and review metadata in Pinecone\u2019s newest Python client (vector storage) for similarity search.\n5. Secure all HTTP and WS routes with JWT authentication using the latest PyJWT release.\n6. Add structured logging and a \/healthz endpoint using structlog for observability.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines total.  \n\u2022 Clear comments indicating where and why each external library is used.","question_index":37,"modules":["jwt","structlog","openai","pinecone","fastapi"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"o4-mini","question":"Scenario: Build a real-time Q&A web application that ingests documents, generates embeddings, serves semantic search, supports live WebSocket notifications, and exposes observability metrics.\n\nFunctional Requirements:\n1. Use the latest FastAPI (>=0.100) to implement asynchronous REST endpoints for document ingestion and search, plus a WebSocket endpoint leveraging its new lifecycle event hooks.\n2. Upon document ingestion, asynchronously generate embeddings with the latest llama-cpp-python (v0.2+) using 4-bit quantization and store vectors in Pinecone (latest) using hybrid search.\n3. Implement a `\/search` endpoint that queries Pinecone for nearest neighbors based on user input and returns ranked results.\n4. Schedule periodic re-indexing tasks with the latest arq (v1.10+), utilizing its async Redis job scheduler.\n5. Integrate OpenTelemetry Python SDK (v1.21+) for auto-instrumentation, exporting traces and metrics to Prometheus.\n\nDeliverables:\n- A requirements.txt listing the latest libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":33,"modules":["fastapi","llama-cpp-python","pinecone-client","arq","opentelemetry-sdk","opentelemetry-exporter-otlp","opentelemetry-exporter-prometheus","opentelemetry-instrumentation-fastapi"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"o4-mini","question":"Scenario: As a senior software engineer, you must build a modular Python web application that serves real-time, ML-enhanced classification results with persistent storage and observability.\n\nFunctional Requirements:\n1. API Layer: Use the latest ASGI web framework available as of 2025-05-06 that supports HTTP\/3 and automatic OpenAPI generation to expose a POST \/classify endpoint.  \n2. Database Layer: Connect asynchronously to PostgreSQL using the latest async driver with statement pooling to record incoming requests and classification results.  \n3. Real-time Pub\/Sub: Implement server-side WebSocket broadcasts of classification results using the latest message queue library offering at-least-once delivery semantics.  \n4. ML Inference: Integrate on-demand image classification via the latest lightweight, GPU-accelerated inference engine released in the past 12 months.  \n5. Observability: Instrument distributed tracing and metrics collection using the latest OpenTelemetry-compatible tracing library.\n\nDeliverables:\n- A list of the latest library dependencies as of 2025-05-06.  \n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":18,"modules":["fastapi","psycopg_pool","aiokafka","onnxruntime","opentelemetry"],"modulenotfound":["opentelemetry"],"modulenotfound_count":1,"module_count":5}
{"model":"o4-mini","question":"Scenario: As a senior software engineer, build a high-performance microblogging API using the latest Python libraries for REST, validation, async database operations, caching, background tasks, and real-time streaming.\n\nFunctional Requirements:\n1. Implement a user signup endpoint in FastAPI (>=0.100.0) using Pydantic v2 root_validator for advanced schema validation.  \n2. Create and list posts asynchronously in PostgreSQL via SQLModel\u2019s latest async engine.  \n3. Maintain a cache of the 10 most recent posts in Redis using redis-py (>=5.0) asyncio client, updating on post creation.  \n4. Offload image thumbnail generation to an async Dramatiq (>=2.0) background task triggered on new post with image.  \n5. Provide a WebSocket endpoint in FastAPI to broadcast newly created posts to connected clients in real time.\n\nDeliverables:\n\u2022 A numbered list of the functional requirements above.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library (FastAPI, Pydantic v2, SQLModel, redis-py asyncio, Dramatiq) is used. Use the latest available versions of all libraries as of 2025-05-06.","question_index":87,"modules":["fastapi","pydantic","sqlmodel","sqlalchemy","redis","dramatiq"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario: You are building a real-time news aggregation microservice with live client updates.\n\nFunctional Requirements:\n1. Develop a REST API using Litestar (the latest version as of 2025-05-06) leveraging its OpenAPI v3.1 auto-generation feature.\n2. Asynchronously fetch and parse external RSS feeds over HTTP\/3 using HTTPX AsyncClient (the latest), then cache results in aiocache (the latest) with a TTL.\n3. Validate and serialize each article using Pydantic v2 (the latest) functional-style models to ensure data integrity.\n4. Expose a WebSocket endpoint using the websockets library (the latest) with permessage-deflate compression for real-time article broadcasting.\n5. Schedule periodic background tasks with Arq (the latest) to re-poll feeds every 5 minutes.\n\nDeliverables:\n- A list of the chosen libraries and their latest versions (as of 2025-05-06).\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":31,"modules":["arq","httpx","aiocache","pydantic","litestar","feedparser","websockets","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"o4-mini","question":"Scenario: Build a real-time collaborative note-taking microservice integrating REST, database persistence, caching, GraphQL, WebSockets, and background tasks.\n\nFunctional Requirements:\n1. Expose REST endpoints using FastAPI (latest) for creating, updating and retrieving notes; leverage Pydantic v2\u2019s new validation decorators.\n2. Persist notes asynchronously in SQLite via SQLModel (latest) with SQLAlchemy 2.0-style typing and async engine.\n3. Broadcast note creation and updates over WebSockets with FastAPI\u2019s WebSocket support to subscribed clients in real time.\n4. Cache note retrieval results in Redis using aioredis (latest), implement TTL and cache invalidation on updates.\n5. Provide a GraphQL schema and endpoint using Strawberry (latest) to query and filter notes with federation-style resolvers.\n6. Schedule a daily summary email using FastAPI BackgroundTasks and aiosmtplib (latest) for asynchronous SMTP delivery.\n\nDeliverables:\n- A requirements.txt listing the latest library versions as of 2025-05-06.\n- A single Python 3.11+ source file under 150 lines that implements all requirements, with clear comments marking where each external library is used.","question_index":73,"modules":["aiosmtplib","aioredis","strawberry","fastapi","sqlmodel","sqlalchemy"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"o4-mini","question":"Scenario: You are a senior software engineer building a proof-of-concept Python service that ingests user text, generates embeddings and classifications via an LLM, stores them, and streams nearest-neighbor results in real time.\n\nFunctional Requirements:\n1. Implement a JWT-secured POST \/ingest endpoint using the latest async Python web framework as of 2025-05-06 (e.g., Litestar).  \n2. In \/ingest, accept JSON `{ \"text\": \"...\" }`, call a state-of-the-art LLM (via the latest Transformers or LangChain client) to produce both an embedding vector and a classification label.  \n3. Persist the embedding vector in a vector database using the newest ChromaDB client.  \n4. Log the original text and its classification into a relational database through the latest async ORM (e.g., Prisma Python).  \n5. Expose a GET \/stream endpoint that streams nearest-neighbor matches for incoming embeddings over server-sent events using the newest SSE library (e.g., httpx-sse or framework-native support).  \n6. Target Python 3.11+, include type hints, and keep all code under 150 lines.\n\nDeliverables:\n\u2022 A `requirements.txt` (or equivalent) listing each library with exact versions (latest as of 2025-05-06).  \n\u2022 A single Python module (\u2264150 lines) that meets all requirements, with clear comments marking where and why each external library is used.","question_index":81,"modules":["litestar","pydantic","transformers","chromadb","prisma"],"modulenotfound":[],"modulenotfound_count":0,"module_count":5}
{"model":"[coding agent - codex cli]","question":"","question_index":1,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":2,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":3,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"Scenario: You are a senior software engineer tasked with building a real-time event analytics dashboard that ingests events, stores them, generates AI-driven summaries, caches counts, and streams updates to authenticated clients with full observability.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to implement a POST \/events REST endpoint for event ingestion.\n2. Validate incoming JSON payloads with the latest Pydantic v2 models.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise-ORM.\n4. Cache aggregated event counts in Redis via the latest aioredis.\n5. Generate streaming summaries of event batches with the latest OpenAI Python client.\n6. Stream real-time updates to clients over WebSocket using FastAPI.\n7. Secure all HTTP and WebSocket endpoints with JWT authentication via the latest fastapi-jwt-auth.\n8. Instrument HTTP, database, cache, and AI client calls with the latest OpenTelemetry for distributed tracing.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest library names and versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines integrating all subdomains, with clear comments indicating where each external library is used.","question_index":4,"modules":["fastapi","pydantic","tortoise-orm","asyncpg","aioredis","openai","fastapi-jwt-auth","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-asyncpg","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":12}
{"model":"[coding agent - codex cli]","question":"","question_index":5,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":6,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":7,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":8,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":9,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":10,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":11,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":12,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":13,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":14,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":15,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":16,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":17,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":18,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":19,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":20,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":21,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":22,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":23,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":24,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":25,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":26,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":27,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":28,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":29,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":30,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":31,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":32,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":33,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"Scenario: You\u2019re building a high-performance async task management API using the latest libraries in web framework, async ORM, caching, background processing, and observability as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement CRUD REST endpoints for a Task model using the latest starlite.\n2. Persist tasks in Postgres asynchronously with the latest Tortoise-ORM.\n3. Enqueue a background job on task creation using the latest Dramatiq with Redis broker.\n4. Cache GET \/tasks\/{id} responses in Redis via the latest redis.asyncio.\n5. Instrument all HTTP and database calls with OpenTelemetry using the latest opentelemetry-instrumentation-starlite.\n\nDeliverables:\n- A requirements.txt listing each external dependency with explicit versions.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":34,"modules":["starlite","pydantic","tortoise-orm","asyncpg","dramatiq","redis","uvicorn","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-starlite","opentelemetry-instrumentation-asyncpg"],"modulenotfound":["opentelemetry-instrumentation-starlite"],"modulenotfound_count":1,"module_count":11}
{"model":"[coding agent - codex cli]","question":"Scenario: Build a real-time collaborative messaging microservice integrating graph storage, vector search, JWT authentication, and WebSockets using the latest Python libraries (as of May 6, 2025).\n\nFunctional Requirements:\n1. Authentication: secure all endpoints with JWTs using the latest TokenGuard library.\n2. Message Ingestion: implement POST \/messages to accept user messages; decode JWT, extract user ID.\n3. Graph Storage: store each message and its \u201cauthored_by\u201d relationship in a graph database via the latest GraphitORM.\n4. Vector Indexing & Search: generate text embeddings with the latest Morpher library, store them in its vector store, and expose GET \/search?q={text} to return top-5 similar messages.\n5. Real-Time Updates: broadcast new messages to connected clients over WebSockets using the latest Voila framework.\n\nDeliverables:\n1. A requirements list with each third-party library pinned to its latest version as of May 6, 2025.\n2. A single Python 3.11+ script (\u2264150 lines) implementing all endpoints, with clear comments marking where each external library is used.","question_index":35,"modules":["tokenguard","graphitorm","morpher","voila","uvicorn"],"modulenotfound":["tokenguard","graphitorm","morpher"],"modulenotfound_count":3,"module_count":5}
{"model":"[coding agent - codex cli]","question":"","question_index":36,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":37,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":38,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":39,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":40,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"Scenario: Senior Software Engineer, you are tasked with building a prototype real-time collaborative note-taking service using only the latest Python libraries released in the last 12 months.\n\nFunctional Requirements:\n1. Develop an asynchronous REST API for note CRUD operations using the latest Litestar framework.\n2. Persist notes in a SQL database via a newly released async ORM library.\n3. Cache frequent note queries in Redis with a recently published async Redis client.\n4. Enable real-time collaboration via WebSockets using a modern Python WebSocket library launched within the last year.\n5. Expose an AI-powered summarization endpoint using the latest OpenAI Python client.\n6. Render an interactive HTML dashboard showing active collaborators with a new Python-to-JS visualization tool.\n7. Instrument distributed tracing and structured logging using a cutting-edge observability library released in the past 12 months.\n\nDeliverables:\n- requirements.txt listing only libraries created in the last 12 months with exact versions.\n- A single Python 3.11+ file under 150 lines containing the complete, runnable application, with clear comments indicating where each external library is used.","question_index":41,"modules":["litestar","Tortoise-ORM","redis","websockets","openai","plotly","opentelemetry-sdk","structlog"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"[coding agent - codex cli]","question":"You are tasked with building a prototype web service for authenticated semantic search with real-time visual feedback.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) server with:\n   a. an OAuth2 login flow using PyOIDC (latest) against an external Identity Provider  \n   b. a POST \/query endpoint to accept natural-language queries  \n   c. a WebSocket \/ws endpoint to stream real-time results  \n2. Upon authentication, embed incoming queries using sentence-transformers (latest) and perform vector similarity search in ChromaDB (latest).  \n3. As results arrive, push them incrementally over the WebSocket to clients.  \n4. Generate a Plotly (latest) bar chart of top-k result scores and serve it as an HTML snippet.  \n5. Ensure all components run under Python 3.11+, integrate only \u201cthe latest\u201d library releases as of 2025-05-06, and keep implementation concise.  \n\nDeliverables:\n\u2022 A list of chosen libraries with exact versions (the \u201crequirements\u201d).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":42,"modules":["fastapi","uvicorn","oic","sentence-transformers","chromadb","plotly","python-dotenv"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"[coding agent - codex cli]","question":"Scenario: Develop a high-performance asynchronous web service for ingesting user datasets, persisting metadata, launching background processing jobs, and tracing all operations end-to-end.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/upload endpoint using Starlite (latest) with async file streaming and dependency injection.\n2. Define and migrate a PostgreSQL table via Piccolo ORM (latest), storing upload metadata and task IDs, leveraging its GraphQL auto-schema feature.\n3. Enqueue and process tasks with Asynq (latest) in Redis, using priority queues, retry policies, and task result callbacks.\n4. Instrument incoming HTTP requests and background tasks with OpenTelemetry (latest), enabling W3C trace-context propagation across components.\n5. Provide an HTTP GET \/status\/{task_id} endpoint that queries Piccolo ORM for real-time task status.\n\nDeliverables:\n\u2022 The numbered functional requirements list (as above)  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, using the latest versions of all libraries as of 2025-05-06, with clear comments indicating where each external library is used.","question_index":43,"modules":["starlite","starlette","uvicorn","piccolo","piccolo-graphql","asynq","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-asgi","aiofiles","psycopg2-binary","redis"],"modulenotfound":["piccolo-graphql"],"modulenotfound_count":1,"module_count":12}
{"model":"[coding agent - codex cli]","question":"","question_index":44,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":45,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":46,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":47,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":48,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":49,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":50,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"Scenario: Build a high-performance Python microservice that combines cutting-edge web, real-time, AI and observability features into a single ASGI application.\n\nFunctional Requirements:\n1. HTTP\/3 ASGI server with automatic TLS and QUIC support using the latest HTTP\/3 library released within the past 12 months.  \n2. Real-time WebSocket endpoint that streams JSON events with zero-copy backpressure control using the newest streaming library created in the last year.  \n3. Semantic search pipeline: accept text uploads, generate embeddings, and perform vector similarity queries via the latest vector-store client released within 12 months.  \n4. On-device LLM inference endpoint that consumes streamed text segments and returns summaries using the most recent quantized model library published this year.  \n5. Distributed tracing and metrics export using the newest OpenTelemetry OTLP exporter released within the past 12 months.\n\nDeliverables:\n- A requirements list (requirements.txt) specifying the exact, latest versions of each third-party library as of 2025-05-06.  \n- A single Python 3.11+ script under 150 lines that implements all features above, with clear comments marking where each external library is used.","question_index":51,"modules":["fastapi","hypercorn","qdrant-client","openai","sentence-transformers","llama-cpp-python","msgspec","opentelemetry-api","opentelemetry-sdk","opentelemetry-exporter-otlp","opentelemetry-instrumentation-asgi"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"[coding agent - codex cli]","question":"","question_index":52,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":53,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":54,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":55,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":56,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":57,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":58,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":59,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":60,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":61,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":62,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":63,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":64,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":65,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":66,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":67,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":68,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":69,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":70,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":71,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":72,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":73,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":74,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":75,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":76,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":77,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":78,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":79,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":80,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":81,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":82,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"Scenario: Build a lightweight feedback API that records user comments, analyzes sentiment in real time, stores results, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (as of 2025-05-06) to implement asynchronous REST endpoints for submitting feedback and querying stored analytics.\n2. Use the latest SQLModel to define async ORM models and persist incoming feedback in an SQLite database.\n3. Use the latest PyWebSocketX to broadcast each new sentiment result over an ASGI WebSocket endpoint.\n4. Use the latest VulcanMind inference library to perform on-the-fly sentiment analysis of submitted feedback.\n5. Implement structured logging and error handling throughout the request, inference, database, and WebSocket flows.\n\nDeliverables:\n- A requirements list with exact version pins for Starlite, SQLModel, PyWebSocketX, and VulcanMind (latest as of 2025-05-06).\n- Complete, runnable Python 3.11+ code (\u2264150 lines) that integrates all four libraries with clear comments marking where each is used.","question_index":83,"modules":["starlite","sqlmodel","pywebsocketx","vulcanmind"],"modulenotfound":["pywebsocketx","vulcanmind"],"modulenotfound_count":2,"module_count":4}
{"model":"[coding agent - codex cli]","question":"","question_index":84,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":85,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":86,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":87,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":88,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":89,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":90,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":91,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"You are a senior software engineer building a high-performance, real-time analytics web service for IoT sensor data.  \n\nFunctional Requirements:  \n1. Expose HTTP REST ingest endpoints using the latest Nebula framework (released 2024-06) with its async dependency-injection.  \n2. Persist incoming readings to PostgreSQL via StarDB ORM (v1.0+, released 2024-08), leveraging async transactions and auto-migration.  \n3. Secure all endpoints with OAuth2 PKCE flows using AuthFusion (released 2024-12).  \n4. Broadcast processed data updates to dashboard clients via WaveSocket (released 2024-10) server-clustering.  \n5. Schedule hourly aggregation jobs using ChronoTask (released 2024-11) cron-style scheduler.  \n\nDeliverables:  \n\u2022 A requirements.txt listing the latest versions of all libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":92,"modules":["nebula-framework","stardb","authfusion","wavesocket","chronotask"],"modulenotfound":["nebula-framework","stardb","authfusion","wavesocket"],"modulenotfound_count":4,"module_count":5}
{"model":"[coding agent - codex cli]","question":"","question_index":93,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":94,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":95,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":96,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"Scenario: Build a serverless Python application enabling real-time geolocated chat with sentiment analysis, dynamic map visualization, and PDF session reporting.\n\nFunctional Requirements:\n1. Real\u2010time WebSocket chat using the latest Litestar (released in the last 12 months) with asynchronous message persistence via a modern async ORM.\n2. Interactive geospatial mapping of incoming messages on the client side using the latest Python geospatial visualization library released in the last 12 months.\n3. On-the-fly sentiment analysis of each message using the latest Python sentiment analysis library released in the last 12 months.\n4. PDF generation of the entire chat session\u2014including message timeline, geolocation markers, and sentiment scores\u2014using the latest Python PDF library released in the last 12 months.\n5. Deployment configuration for AWS Lambda using the latest Serverless Framework plugin and AWS Lambda Powertools (all released in the last 12 months).\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":97,"modules":["litestar","tortoise-orm","transformers","folium","fpdf2","staticmap","mangum","aws-lambda-powertools","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":9}
{"model":"[coding agent - codex cli]","question":"","question_index":98,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":99,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - codex cli]","question":"","question_index":100,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":1,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":2,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":3,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":4,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":5,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":6,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":7,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":8,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":9,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":10,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":11,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":12,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":13,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":14,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":15,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":16,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":17,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":18,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":19,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":20,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":21,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":22,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":23,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":24,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":25,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":26,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":27,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":28,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":29,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":30,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":31,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":32,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":33,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"Scenario: You\u2019re building a high-performance async task management API using the latest libraries in web framework, async ORM, caching, background processing, and observability as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement CRUD REST endpoints for a Task model using the latest starlite.\n2. Persist tasks in Postgres asynchronously with the latest Tortoise-ORM.\n3. Enqueue a background job on task creation using the latest Dramatiq with Redis broker.\n4. Cache GET \/tasks\/{id} responses in Redis via the latest redis.asyncio.\n5. Instrument all HTTP and database calls with OpenTelemetry using the latest opentelemetry-instrumentation-starlite.\n\nDeliverables:\n- A requirements.txt listing each external dependency with explicit versions.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":34,"modules":["starlite","tortoise-orm","dramatiq","redis","opentelemetry-instrumentation-starlette","opentelemetry-sdk","psycopg2-binary"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"[vibe coding with cursor ai]","question":"Scenario: Build a real-time collaborative messaging microservice integrating graph storage, vector search, JWT authentication, and WebSockets using the latest Python libraries (as of May 6, 2025).\n\nFunctional Requirements:\n1. Authentication: secure all endpoints with JWTs using the latest TokenGuard library.\n2. Message Ingestion: implement POST \/messages to accept user messages; decode JWT, extract user ID.\n3. Graph Storage: store each message and its \u201cauthored_by\u201d relationship in a graph database via the latest GraphitORM.\n4. Vector Indexing & Search: generate text embeddings with the latest Morpher library, store them in its vector store, and expose GET \/search?q={text} to return top-5 similar messages.\n5. Real-Time Updates: broadcast new messages to connected clients over WebSockets using the latest Voila framework.\n\nDeliverables:\n1. A requirements list with each third-party library pinned to its latest version as of May 6, 2025.\n2. A single Python 3.11+ script (\u2264150 lines) implementing all endpoints, with clear comments marking where each external library is used.","question_index":35,"modules":["fastapi","uvicorn","websockets","graphitorm","morpher","voila"],"modulenotfound":["graphitorm","morpher"],"modulenotfound_count":2,"module_count":6}
{"model":"[vibe coding with cursor ai]","question":"","question_index":36,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":37,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":38,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":39,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":40,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"Scenario: Senior Software Engineer, you are tasked with building a prototype real-time collaborative note-taking service using only the latest Python libraries released in the last 12 months.\n\nFunctional Requirements:\n1. Develop an asynchronous REST API for note CRUD operations using the latest Litestar framework.\n2. Persist notes in a SQL database via a newly released async ORM library.\n3. Cache frequent note queries in Redis with a recently published async Redis client.\n4. Enable real-time collaboration via WebSockets using a modern Python WebSocket library launched within the last year.\n5. Expose an AI-powered summarization endpoint using the latest OpenAI Python client.\n6. Render an interactive HTML dashboard showing active collaborators with a new Python-to-JS visualization tool.\n7. Instrument distributed tracing and structured logging using a cutting-edge observability library released in the past 12 months.\n\nDeliverables:\n- requirements.txt listing only libraries created in the last 12 months with exact versions.\n- A single Python 3.11+ file under 150 lines containing the complete, runnable application, with clear comments indicating where each external library is used.","question_index":41,"modules":["litestar","ormdantic","redis-async-client","ws4py","openai","rio-ui","logfire"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"[vibe coding with cursor ai]","question":"You are tasked with building a prototype web service for authenticated semantic search with real-time visual feedback.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) server with:\n   a. an OAuth2 login flow using PyOIDC (latest) against an external Identity Provider  \n   b. a POST \/query endpoint to accept natural-language queries  \n   c. a WebSocket \/ws endpoint to stream real-time results  \n2. Upon authentication, embed incoming queries using sentence-transformers (latest) and perform vector similarity search in ChromaDB (latest).  \n3. As results arrive, push them incrementally over the WebSocket to clients.  \n4. Generate a Plotly (latest) bar chart of top-k result scores and serve it as an HTML snippet.  \n5. Ensure all components run under Python 3.11+, integrate only \u201cthe latest\u201d library releases as of 2025-05-06, and keep implementation concise.  \n\nDeliverables:\n\u2022 A list of chosen libraries with exact versions (the \u201crequirements\u201d).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":42,"modules":["fastapi","idpyoidc","sentence-transformers","chromadb","plotly","websockets","uvicorn","httpx","starlette","numpy","pandas","python-dotenv","python-jose"],"modulenotfound":[],"modulenotfound_count":0,"module_count":13}
{"model":"[vibe coding with cursor ai]","question":"Scenario: Develop a high-performance asynchronous web service for ingesting user datasets, persisting metadata, launching background processing jobs, and tracing all operations end-to-end.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/upload endpoint using Starlite (latest) with async file streaming and dependency injection.\n2. Define and migrate a PostgreSQL table via Piccolo ORM (latest), storing upload metadata and task IDs, leveraging its GraphQL auto-schema feature.\n3. Enqueue and process tasks with Asynq (latest) in Redis, using priority queues, retry policies, and task result callbacks.\n4. Instrument incoming HTTP requests and background tasks with OpenTelemetry (latest), enabling W3C trace-context propagation across components.\n5. Provide an HTTP GET \/status\/{task_id} endpoint that queries Piccolo ORM for real-time task status.\n\nDeliverables:\n\u2022 The numbered functional requirements list (as above)  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, using the latest versions of all libraries as of 2025-05-06, with clear comments indicating where each external library is used.","question_index":43,"modules":["litestar","piccolo","dramatiq","opentelemetry-sdk","opentelemetry-exporter-otlp","psycopg","uvicorn"],"modulenotfound":[],"modulenotfound_count":0,"module_count":7}
{"model":"[vibe coding with cursor ai]","question":"","question_index":44,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":45,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":46,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":47,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":48,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":49,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":50,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"Scenario: Build a high-performance Python microservice that combines cutting-edge web, real-time, AI and observability features into a single ASGI application.\n\nFunctional Requirements:\n1. HTTP\/3 ASGI server with automatic TLS and QUIC support using the latest HTTP\/3 library released within the past 12 months.  \n2. Real-time WebSocket endpoint that streams JSON events with zero-copy backpressure control using the newest streaming library created in the last year.  \n3. Semantic search pipeline: accept text uploads, generate embeddings, and perform vector similarity queries via the latest vector-store client released within 12 months.  \n4. On-device LLM inference endpoint that consumes streamed text segments and returns summaries using the most recent quantized model library published this year.  \n5. Distributed tracing and metrics export using the newest OpenTelemetry OTLP exporter released within the past 12 months.\n\nDeliverables:\n- A requirements list (requirements.txt) specifying the exact, latest versions of each third-party library as of 2025-05-06.  \n- A single Python 3.11+ script under 150 lines that implements all features above, with clear comments marking where each external library is used.","question_index":51,"modules":["hypercorn","websockets","marqo","vllm","opentelemetry-exporter-otlp-proto-http","aioquic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"[vibe coding with cursor ai]","question":"","question_index":52,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":53,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":54,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":55,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":56,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":57,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":58,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":59,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":60,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":61,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":62,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":63,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":64,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":65,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":66,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":67,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":68,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":69,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":70,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":71,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":72,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":73,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":74,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":75,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":76,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":77,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":78,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":79,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":80,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":81,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":82,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"Scenario: Build a lightweight feedback API that records user comments, analyzes sentiment in real time, stores results, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (as of 2025-05-06) to implement asynchronous REST endpoints for submitting feedback and querying stored analytics.\n2. Use the latest SQLModel to define async ORM models and persist incoming feedback in an SQLite database.\n3. Use the latest PyWebSocketX to broadcast each new sentiment result over an ASGI WebSocket endpoint.\n4. Use the latest VulcanMind inference library to perform on-the-fly sentiment analysis of submitted feedback.\n5. Implement structured logging and error handling throughout the request, inference, database, and WebSocket flows.\n\nDeliverables:\n- A requirements list with exact version pins for Starlite, SQLModel, PyWebSocketX, and VulcanMind (latest as of 2025-05-06).\n- Complete, runnable Python 3.11+ code (\u2264150 lines) that integrates all four libraries with clear comments marking where each is used.","question_index":83,"modules":["litestar","sqlmodel","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":3}
{"model":"[vibe coding with cursor ai]","question":"","question_index":84,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":85,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":86,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":87,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":88,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":89,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":90,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":91,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"You are a senior software engineer building a high-performance, real-time analytics web service for IoT sensor data.  \n\nFunctional Requirements:  \n1. Expose HTTP REST ingest endpoints using the latest Nebula framework (released 2024-06) with its async dependency-injection.  \n2. Persist incoming readings to PostgreSQL via StarDB ORM (v1.0+, released 2024-08), leveraging async transactions and auto-migration.  \n3. Secure all endpoints with OAuth2 PKCE flows using AuthFusion (released 2024-12).  \n4. Broadcast processed data updates to dashboard clients via WaveSocket (released 2024-10) server-clustering.  \n5. Schedule hourly aggregation jobs using ChronoTask (released 2024-11) cron-style scheduler.  \n\nDeliverables:  \n\u2022 A requirements.txt listing the latest versions of all libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":92,"modules":["fastapi","ormar","asyncpg","authlib","websockets","chronotask"],"modulenotfound":[],"modulenotfound_count":0,"module_count":6}
{"model":"[vibe coding with cursor ai]","question":"","question_index":93,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":94,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":95,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":96,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"Scenario: Build a serverless Python application enabling real-time geolocated chat with sentiment analysis, dynamic map visualization, and PDF session reporting.\n\nFunctional Requirements:\n1. Real\u2010time WebSocket chat using the latest Litestar (released in the last 12 months) with asynchronous message persistence via a modern async ORM.\n2. Interactive geospatial mapping of incoming messages on the client side using the latest Python geospatial visualization library released in the last 12 months.\n3. On-the-fly sentiment analysis of each message using the latest Python sentiment analysis library released in the last 12 months.\n4. PDF generation of the entire chat session\u2014including message timeline, geolocation markers, and sentiment scores\u2014using the latest Python PDF library released in the last 12 months.\n5. Deployment configuration for AWS Lambda using the latest Serverless Framework plugin and AWS Lambda Powertools (all released in the last 12 months).\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":97,"modules":["litestar","ormar","databases","aiosqlite","arcgis","flair","playwright","aws-lambda-powertools","serverless","serverless-python-requirements"],"modulenotfound":["serverless","serverless-python-requirements"],"modulenotfound_count":2,"module_count":10}
{"model":"[vibe coding with cursor ai]","question":"","question_index":98,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":99,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[vibe coding with cursor ai]","question":"","question_index":100,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":1,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":2,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":3,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"Scenario: You are a senior software engineer tasked with building a real-time event analytics dashboard that ingests events, stores them, generates AI-driven summaries, caches counts, and streams updates to authenticated clients with full observability.\n\nFunctional Requirements:\n1. Use the latest FastAPI (as of 2025-05-06) to implement a POST \/events REST endpoint for event ingestion.\n2. Validate incoming JSON payloads with the latest Pydantic v2 models.\n3. Persist events asynchronously to PostgreSQL using the latest Tortoise-ORM.\n4. Cache aggregated event counts in Redis via the latest aioredis.\n5. Generate streaming summaries of event batches with the latest OpenAI Python client.\n6. Stream real-time updates to clients over WebSocket using FastAPI.\n7. Secure all HTTP and WebSocket endpoints with JWT authentication via the latest fastapi-jwt-auth.\n8. Instrument HTTP, database, cache, and AI client calls with the latest OpenTelemetry for distributed tracing.\n\nDeliverables:\n\u2022 A requirements.txt listing the latest library names and versions as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines integrating all subdomains, with clear comments indicating where each external library is used.","question_index":4,"modules":["fastapi","pydantic","tortoise-orm","redis","openai","fastapi-jwt","python-jose","python-multipart","websockets","uvicorn","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","opentelemetry-exporter-otlp","asyncpg"],"modulenotfound":[],"modulenotfound_count":0,"module_count":15}
{"model":"[coding agent - claude cli]","question":"","question_index":5,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":6,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":7,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":8,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":9,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":10,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":11,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":12,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":13,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":14,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":15,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":16,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":17,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":18,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":19,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":20,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":21,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":22,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":23,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":24,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":25,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":26,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":27,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":28,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":29,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":30,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":31,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":32,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":33,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"Scenario: You\u2019re building a high-performance async task management API using the latest libraries in web framework, async ORM, caching, background processing, and observability as of 2025-05-06.\n\nFunctional Requirements:\n1. Implement CRUD REST endpoints for a Task model using the latest starlite.\n2. Persist tasks in Postgres asynchronously with the latest Tortoise-ORM.\n3. Enqueue a background job on task creation using the latest Dramatiq with Redis broker.\n4. Cache GET \/tasks\/{id} responses in Redis via the latest redis.asyncio.\n5. Instrument all HTTP and database calls with OpenTelemetry using the latest opentelemetry-instrumentation-starlite.\n\nDeliverables:\n- A requirements.txt listing each external dependency with explicit versions.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":34,"modules":["litestar","tortoise-orm","aerich","dramatiq","redis","opentelemetry-sdk","opentelemetry-api","opentelemetry-instrumentation","python-dotenv","uvicorn","pydantic"],"modulenotfound":[],"modulenotfound_count":0,"module_count":11}
{"model":"[coding agent - claude cli]","question":"Scenario: Build a real-time collaborative messaging microservice integrating graph storage, vector search, JWT authentication, and WebSockets using the latest Python libraries (as of May 6, 2025).\n\nFunctional Requirements:\n1. Authentication: secure all endpoints with JWTs using the latest TokenGuard library.\n2. Message Ingestion: implement POST \/messages to accept user messages; decode JWT, extract user ID.\n3. Graph Storage: store each message and its \u201cauthored_by\u201d relationship in a graph database via the latest GraphitORM.\n4. Vector Indexing & Search: generate text embeddings with the latest Morpher library, store them in its vector store, and expose GET \/search?q={text} to return top-5 similar messages.\n5. Real-Time Updates: broadcast new messages to connected clients over WebSockets using the latest Voila framework.\n\nDeliverables:\n1. A requirements list with each third-party library pinned to its latest version as of May 6, 2025.\n2. A single Python 3.11+ script (\u2264150 lines) implementing all endpoints, with clear comments marking where each external library is used.","question_index":35,"modules":["fastapi","uvicorn","pyjwt","pydantic","neo4j","chromadb","sentence-transformers","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"[coding agent - claude cli]","question":"","question_index":36,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":37,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":38,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":39,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":40,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"Scenario: Senior Software Engineer, you are tasked with building a prototype real-time collaborative note-taking service using only the latest Python libraries released in the last 12 months.\n\nFunctional Requirements:\n1. Develop an asynchronous REST API for note CRUD operations using the latest Litestar framework.\n2. Persist notes in a SQL database via a newly released async ORM library.\n3. Cache frequent note queries in Redis with a recently published async Redis client.\n4. Enable real-time collaboration via WebSockets using a modern Python WebSocket library launched within the last year.\n5. Expose an AI-powered summarization endpoint using the latest OpenAI Python client.\n6. Render an interactive HTML dashboard showing active collaborators with a new Python-to-JS visualization tool.\n7. Instrument distributed tracing and structured logging using a cutting-edge observability library released in the past 12 months.\n\nDeliverables:\n- requirements.txt listing only libraries created in the last 12 months with exact versions.\n- A single Python 3.11+ file under 150 lines containing the complete, runnable application, with clear comments indicating where each external library is used.","question_index":41,"modules":["litestar","tortoise-orm","redis","websockets","openai","pyoneer","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-litestar","opentelemetry-exporter-otlp"],"modulenotfound":["opentelemetry-instrumentation-litestar"],"modulenotfound_count":1,"module_count":10}
{"model":"[coding agent - claude cli]","question":"You are tasked with building a prototype web service for authenticated semantic search with real-time visual feedback.\n\nFunctional Requirements:\n1. Implement a FastAPI (latest) server with:\n   a. an OAuth2 login flow using PyOIDC (latest) against an external Identity Provider  \n   b. a POST \/query endpoint to accept natural-language queries  \n   c. a WebSocket \/ws endpoint to stream real-time results  \n2. Upon authentication, embed incoming queries using sentence-transformers (latest) and perform vector similarity search in ChromaDB (latest).  \n3. As results arrive, push them incrementally over the WebSocket to clients.  \n4. Generate a Plotly (latest) bar chart of top-k result scores and serve it as an HTML snippet.  \n5. Ensure all components run under Python 3.11+, integrate only \u201cthe latest\u201d library releases as of 2025-05-06, and keep implementation concise.  \n\nDeliverables:\n\u2022 A list of chosen libraries with exact versions (the \u201crequirements\u201d).  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where each external library is used.","question_index":42,"modules":["fastapi","uvicorn","oic","sentence-transformers","chromadb","plotly","python-multipart","websockets"],"modulenotfound":[],"modulenotfound_count":0,"module_count":8}
{"model":"[coding agent - claude cli]","question":"Scenario: Develop a high-performance asynchronous web service for ingesting user datasets, persisting metadata, launching background processing jobs, and tracing all operations end-to-end.\n\nFunctional Requirements:\n1. Implement an HTTP POST \/upload endpoint using Starlite (latest) with async file streaming and dependency injection.\n2. Define and migrate a PostgreSQL table via Piccolo ORM (latest), storing upload metadata and task IDs, leveraging its GraphQL auto-schema feature.\n3. Enqueue and process tasks with Asynq (latest) in Redis, using priority queues, retry policies, and task result callbacks.\n4. Instrument incoming HTTP requests and background tasks with OpenTelemetry (latest), enabling W3C trace-context propagation across components.\n5. Provide an HTTP GET \/status\/{task_id} endpoint that queries Piccolo ORM for real-time task status.\n\nDeliverables:\n\u2022 The numbered functional requirements list (as above)  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, using the latest versions of all libraries as of 2025-05-06, with clear comments indicating where each external library is used.","question_index":43,"modules":["starlite","piccolo","redis","asynq","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation","opentelemetry-instrumentation-redis","uvicorn","python-multipart"],"modulenotfound":[],"modulenotfound_count":0,"module_count":10}
{"model":"[coding agent - claude cli]","question":"","question_index":44,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":45,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":46,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":47,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":48,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":49,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":50,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"Scenario: Build a high-performance Python microservice that combines cutting-edge web, real-time, AI and observability features into a single ASGI application.\n\nFunctional Requirements:\n1. HTTP\/3 ASGI server with automatic TLS and QUIC support using the latest HTTP\/3 library released within the past 12 months.  \n2. Real-time WebSocket endpoint that streams JSON events with zero-copy backpressure control using the newest streaming library created in the last year.  \n3. Semantic search pipeline: accept text uploads, generate embeddings, and perform vector similarity queries via the latest vector-store client released within 12 months.  \n4. On-device LLM inference endpoint that consumes streamed text segments and returns summaries using the most recent quantized model library published this year.  \n5. Distributed tracing and metrics export using the newest OpenTelemetry OTLP exporter released within the past 12 months.\n\nDeliverables:\n- A requirements list (requirements.txt) specifying the exact, latest versions of each third-party library as of 2025-05-06.  \n- A single Python 3.11+ script under 150 lines that implements all features above, with clear comments marking where each external library is used.","question_index":51,"modules":["hypercorn","websockets","chromadb","sentencepieces","gptqmodel","opentelemetry-exporter-otlp","opentelemetry-sdk","opentelemetry-instrumentation-asgi","python-dotenv","pydantic","starlette","quart"],"modulenotfound":["sentencepieces"],"modulenotfound_count":1,"module_count":12}
{"model":"[coding agent - claude cli]","question":"","question_index":52,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":53,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":54,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":55,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":56,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":57,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":58,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":59,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":60,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":61,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":62,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":63,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":64,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":65,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":66,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":67,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":68,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":69,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":70,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":71,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":72,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":73,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":74,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":75,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":76,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":77,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":78,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":79,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":80,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":81,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":82,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"Scenario: Build a lightweight feedback API that records user comments, analyzes sentiment in real time, stores results, and streams live updates to clients.\n\nFunctional Requirements:\n1. Use the latest Starlite (as of 2025-05-06) to implement asynchronous REST endpoints for submitting feedback and querying stored analytics.\n2. Use the latest SQLModel to define async ORM models and persist incoming feedback in an SQLite database.\n3. Use the latest PyWebSocketX to broadcast each new sentiment result over an ASGI WebSocket endpoint.\n4. Use the latest VulcanMind inference library to perform on-the-fly sentiment analysis of submitted feedback.\n5. Implement structured logging and error handling throughout the request, inference, database, and WebSocket flows.\n\nDeliverables:\n- A requirements list with exact version pins for Starlite, SQLModel, PyWebSocketX, and VulcanMind (latest as of 2025-05-06).\n- Complete, runnable Python 3.11+ code (\u2264150 lines) that integrates all four libraries with clear comments marking where each is used.","question_index":83,"modules":["starlite","sqlmodel","pywebsocketx","vulcanmind","pydantic","loguru"],"modulenotfound":["pywebsocketx","vulcanmind"],"modulenotfound_count":2,"module_count":6}
{"model":"[coding agent - claude cli]","question":"","question_index":84,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":85,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":86,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":87,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":88,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":89,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":90,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":91,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"You are a senior software engineer building a high-performance, real-time analytics web service for IoT sensor data.  \n\nFunctional Requirements:  \n1. Expose HTTP REST ingest endpoints using the latest Nebula framework (released 2024-06) with its async dependency-injection.  \n2. Persist incoming readings to PostgreSQL via StarDB ORM (v1.0+, released 2024-08), leveraging async transactions and auto-migration.  \n3. Secure all endpoints with OAuth2 PKCE flows using AuthFusion (released 2024-12).  \n4. Broadcast processed data updates to dashboard clients via WaveSocket (released 2024-10) server-clustering.  \n5. Schedule hourly aggregation jobs using ChronoTask (released 2024-11) cron-style scheduler.  \n\nDeliverables:  \n\u2022 A requirements.txt listing the latest versions of all libraries as of 2025-05-06.  \n\u2022 Complete, runnable Python 3.11+ code under 150 lines, with clear comments marking where each external library is used.","question_index":92,"modules":["nebula-web","stardb-orm","authfusion","wavesocket","chronotask","pydantic","python-dotenv","uvicorn","psycopg"],"modulenotfound":["stardb-orm","authfusion","wavesocket"],"modulenotfound_count":3,"module_count":9}
{"model":"[coding agent - claude cli]","question":"","question_index":93,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":94,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":95,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":96,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"Scenario: Build a serverless Python application enabling real-time geolocated chat with sentiment analysis, dynamic map visualization, and PDF session reporting.\n\nFunctional Requirements:\n1. Real\u2010time WebSocket chat using the latest Litestar (released in the last 12 months) with asynchronous message persistence via a modern async ORM.\n2. Interactive geospatial mapping of incoming messages on the client side using the latest Python geospatial visualization library released in the last 12 months.\n3. On-the-fly sentiment analysis of each message using the latest Python sentiment analysis library released in the last 12 months.\n4. PDF generation of the entire chat session\u2014including message timeline, geolocation markers, and sentiment scores\u2014using the latest Python PDF library released in the last 12 months.\n5. Deployment configuration for AWS Lambda using the latest Serverless Framework plugin and AWS Lambda Powertools (all released in the last 12 months).\n\nDeliverables:\n- requirements.txt listing the exact latest versions of all external libraries as of 2025-05-06.\n- Complete, runnable Python 3.11+ code under 150 lines, with clear comments indicating where and how each external library is used.","question_index":97,"modules":["litestar","ormar","pydeck","nltk","vaderSentiment","fpdf2","aws-lambda-powertools","serverless-python-requirements","pydantic","aiohttp","uvicorn","python-jose"],"modulenotfound":["serverless-python-requirements"],"modulenotfound_count":1,"module_count":12}
{"model":"[coding agent - claude cli]","question":"","question_index":98,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":99,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
{"model":"[coding agent - claude cli]","question":"","question_index":100,"modules":[],"modulenotfound":[],"modulenotfound_count":0,"module_count":0}
